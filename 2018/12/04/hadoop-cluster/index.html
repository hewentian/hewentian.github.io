<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>hadoop 集群的搭建 | Tim Ho&#39;s Technology Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本篇将说说hadoop集群的搭建，这里说的集群是真集群，不是伪集群。不过，这里的真集群是在虚拟机环境中搭建的。 在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装三台服务器：master、slave1、slave2来搭建hadoop集群。安装好VirtualBox后，启动它。依次点File -&amp;gt; Host Network Manager -&amp;gt; Create，来创建一个网">
<meta name="keywords" content="hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop 集群的搭建">
<meta property="og:url" content="https://github.com/hewentian/2018/12/04/hadoop-cluster/index.html">
<meta property="og:site_name" content="Tim Ho&#39;s Technology Blog">
<meta property="og:description" content="本篇将说说hadoop集群的搭建，这里说的集群是真集群，不是伪集群。不过，这里的真集群是在虚拟机环境中搭建的。 在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装三台服务器：master、slave1、slave2来搭建hadoop集群。安装好VirtualBox后，启动它。依次点File -&amp;gt; Host Network Manager -&amp;gt; Create，来创建一个网">
<meta property="og:locale" content="en-US">
<meta property="og:image" content="https://github.com/img/hadoop-1.png">
<meta property="og:image" content="https://github.com/img/hadoop-2.png">
<meta property="og:image" content="https://github.com/img/hadoop-3.png">
<meta property="og:image" content="https://github.com/img/hadoop-4.png">
<meta property="og:image" content="https://github.com/img/hadoop-5.png">
<meta property="og:image" content="https://github.com/img/hadoop-6.png">
<meta property="og:image" content="https://github.com/img/hadoop-7.png">
<meta property="og:image" content="https://github.com/img/hadoop-8.png">
<meta property="og:image" content="https://github.com/img/hadoop-9.png">
<meta property="og:image" content="https://github.com/img/hadoop-10.png">
<meta property="og:image" content="https://github.com/img/hadoop-11.png">
<meta property="og:image" content="https://github.com/img/hadoop-12.png">
<meta property="og:image" content="https://github.com/img/hadoop-13.png">
<meta property="og:image" content="https://github.com/img/hadoop-14.png">
<meta property="og:image" content="https://github.com/img/hadoop-15.png">
<meta property="og:image" content="https://github.com/img/hadoop-16.png">
<meta property="og:updated_time" content="2020-10-07T07:30:03.974Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hadoop 集群的搭建">
<meta name="twitter:description" content="本篇将说说hadoop集群的搭建，这里说的集群是真集群，不是伪集群。不过，这里的真集群是在虚拟机环境中搭建的。 在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装三台服务器：master、slave1、slave2来搭建hadoop集群。安装好VirtualBox后，启动它。依次点File -&amp;gt; Host Network Manager -&amp;gt; Create，来创建一个网">
<meta name="twitter:image" content="https://github.com/img/hadoop-1.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Tim Ho&#39;s Technology Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">心如止水</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/categories/">Categories</a>
        
          <a class="main-nav-link" href="/tags/">Tags</a>
        
          <a class="main-nav-link" href="/about/">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://github.com/hewentian"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-hadoop-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/hadoop-cluster/" class="article-date">
  <time datetime="2018-12-04T01:12:43.000Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      hadoop 集群的搭建
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说hadoop集群的搭建，这里说的集群是真集群，不是伪集群。不过，这里的真集群是在虚拟机环境中搭建的。</p>
<p>在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装三台服务器：master、slave1、slave2来搭建hadoop集群。安装好VirtualBox后，启动它。依次点<code>File -&gt; Host Network Manager -&gt; Create</code>，来创建一个网络和虚拟机中的机器通讯，这个地址是：<code>192.168.56.1</code>，也就是我们外面实体机的地址（仅和虚拟机中的机器通讯使用）。如下图：</p>
<p><img src="/img/hadoop-1.png" alt="" title="虚拟机网络配置"></p>
<p>我们使用<code>ubuntu 18.04</code>来作为我们的服务器，先在虚拟机中安装好一台服务器master，将Jdk、hadoop在上面安装好，然后将master克隆出slave1、slave2。以master为namenode节点，slave1、slave2作为datanode节点。相关配置如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
</code></pre><h3 id="下面开始master的安装"><a href="#下面开始master的安装" class="headerlink" title="下面开始master的安装"></a>下面开始master的安装</h3><p>在虚拟机中安装<code>master</code>的过程中我们会设置一个用户用于登录，我们将用户名、密码都设为<code>hadoop</code>，当然也可以为其他名字，其他安装过程略。安装好之后，使用默认的网关配置NAT，NAT可以访问外网，我们将<code>jdk-8u102-linux-x64.tar.gz</code>和<a href="http://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/" target="_blank" rel="noopener">hadoop-2.7.3.tar.gz</a>从它们的官网下载到用户的<code>/home/hadoop/</code>目录下。或在实体机中通过SCP命令传进去。然后将网关设置为<code>Host-only Adapter</code>，如下图所示。</p>
<p><img src="/img/hadoop-2.png" alt="" title="网络配置"></p>
<p>网关设置好了之后，我们接下来配置IP地址。在<code>master</code>中<code>[Settings] -&gt; [Network] -&gt; [Wired 这里打开] -&gt; [IPv4]</code>按如下设置：</p>
<p><img src="/img/hadoop-3.png" alt="" title="网络配置"></p>
<h3 id="管理集群"><a href="#管理集群" class="headerlink" title="管理集群"></a>管理集群</h3><p>在上面的IP等配置好之后，我们选择关闭master，注意不是直接关闭，而是在关闭的时候选择<code>Save the machine state</code>。然后在虚拟机中选中<code>master -&gt; Start 下拉箭头 -&gt; Headless start</code>，然后在我们实体机中通过ssh直接登录到master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@192.168.56.110</span><br></pre></td></tr></table></figure></p>
<p>我们可以在实体机通过配置<code>/etc/hosts</code>，加上如下配置：</p>
<pre><code>192.168.56.110    hadoop-host-master
</code></pre><p>然后就可以通过如下方式登录了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>在实体机中通过下面的配置，就可以无密码登录了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p><strong> 下面的操作，均是在实体机中通过SSH到虚拟机执行的操作。 </strong></p>
<h3 id="安装ssh-openssh-rsync"><a href="#安装ssh-openssh-rsync" class="headerlink" title="安装ssh openssh rsync"></a>安装ssh openssh rsync</h3><p>如系统已安装，则勿略下面的安装操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh openssh-server rsync</span><br></pre></td></tr></table></figure></p>
<p>如果上述命令无法执行，请先执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>JDK的安装请参考我之前的笔记：<a href="../../../../2017/12/08/jdk-install/">安装 JDK</a>，这里不再赘述。安装到此目录<code>/usr/local/jdk1.8.0_102/</code>下，记住此路径，下面会用到。下在进行hadoop的安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xf hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压后得到hadoop-2.7.3目录，hadoop的程序和相关配置就在此目录中。</p>
<h3 id="建保存数据的目录"><a href="#建保存数据的目录" class="headerlink" title="建保存数据的目录"></a>建保存数据的目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p hdfs/tmp</span><br><span class="line">$ mkdir -p hdfs/name</span><br><span class="line">$ mkdir -p hdfs/data</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 hdfs/</span><br></pre></td></tr></table></figure>
<h3 id="配置文件浏览"><a href="#配置文件浏览" class="headerlink" title="配置文件浏览"></a>配置文件浏览</h3><p>hadoop的配置文件都位于下面的目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop</span><br><span class="line">$ ls </span><br><span class="line">capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh</span><br><span class="line">configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template</span><br><span class="line">container-executor.cfg      httpfs-signature.secret  mapred-site.xml.template</span><br><span class="line">core-site.xml               httpfs-site.xml          slaves</span><br><span class="line">hadoop-env.cmd              kms-acls.xml             ssl-client.xml.example</span><br><span class="line">hadoop-env.sh               kms-env.sh               ssl-server.xml.example</span><br><span class="line">hadoop-metrics2.properties  kms-log4j.properties     yarn-env.cmd</span><br><span class="line">hadoop-metrics.properties   kms-site.xml             yarn-env.sh</span><br><span class="line">hadoop-policy.xml           log4j.properties         yarn-site.xml</span><br><span class="line">hdfs-site.xml               mapred-env.cmd</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hadoop-env-sh，加上JDK绝对路径"><a href="#配置hadoop-env-sh，加上JDK绝对路径" class="headerlink" title="配置hadoop-env.sh，加上JDK绝对路径"></a>配置hadoop-env.sh，加上JDK绝对路径</h3><p>JDK的路径就是上面安装JDK的时候的路径：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_102/</span><br></pre></td></tr></table></figure></p>
<h3 id="配置core-site-xml，在该文件中加入如下内容"><a href="#配置core-site-xml，在该文件中加入如下内容" class="headerlink" title="配置core-site.xml，在该文件中加入如下内容"></a>配置core-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-host-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hdfs-site-xml，在该文件中加入如下内容"><a href="#配置hdfs-site-xml，在该文件中加入如下内容" class="headerlink" title="配置hdfs-site.xml，在该文件中加入如下内容"></a>配置hdfs-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>至此，master中要安装的通用环境配置完成。在虚拟机中将master复制出slave1、slave2。并参考上面配置IP地址的方法将slave1的ip配置为:<code>192.168.56.111</code>，slave2的ip配置为：<code>192.168.56.112</code>。</p>
<h3 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h3><p>配置master的主机名为<code>hadoop-host-master</code>，在master节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>配置slave1的主机名为<code>hadoop-host-slave-1</code>，在slave1节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<p>配置slave2的主机名为<code>hadoop-host-slave-2</code>，在slave2节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p><strong> 注意：各个节点的主机名一定要不同，否则相同主机名的节点，只会有一个连得上namenode节点，并且集群会报错，修改主机名后，要重启才生效。 </strong></p>
<h3 id="配置域名解析"><a href="#配置域名解析" class="headerlink" title="配置域名解析"></a>配置域名解析</h3><p>分别对master、slave1和slave2都执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">127.0.0.1	localhost</span><br><span class="line">192.168.56.110	hadoop-host-master</span><br><span class="line">192.168.56.111	hadoop-host-slave-1</span><br><span class="line">192.168.56.112	hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>至此，集群配置完成，下面将启动集群。</p>
<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><p>首先启动namenode节点，也就是master，首次启动的时候，要格式化namenode。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -format    <span class="comment"># 再次启动的时候不需要执行此操作</span></span><br><span class="line">$ ./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 NameNode</span><br></pre></td></tr></table></figure></p>
<p>接下来启动datanode节点，也就是slave1、slave2，在这两台服务器上都执行如下启动脚本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh start datanode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">2451 Jps</span><br><span class="line">2162 DataNode</span><br></pre></td></tr></table></figure></p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:50070/" target="_blank" rel="noopener">http://hadoop-host-master:50070/</a><br>来查看是否启动成功。</p>
<p><img src="/img/hadoop-4.png" alt="" title="hadoop管理界面：Overview"></p>
<p>切换tab到Datanodes可以看到有2个datanode节点，如下图所示：</p>
<p><img src="/img/hadoop-5.png" alt="" title="hadoop管理界面:Datanodes"></p>
<p>切换到<code>Utilities -&gt; Browse the file system</code>，如下图所示：</p>
<p><img src="/img/hadoop-6.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>从上面的界面可以，目前HDFS中没有任何文件。我们尝试往其中放一个文件，就将我们的hadoop压缩包放进去，在namenode节点中执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/hadoop-2.7.3.tar.gz /</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup  214092195 2018-12-06 12:20 /hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>我们在图形界面中查看，如下图：</p>
<p><img src="/img/hadoop-7.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>我们点击列表中的文件，将会显示它的数据具体分布在哪些节点上，如下图：</p>
<p><img src="/img/hadoop-8.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<h3 id="停掉集群"><a href="#停掉集群" class="headerlink" title="停掉集群"></a>停掉集群</h3><p>先停掉datanode节点，在slave1、slave2上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure></p>
<p>然后停掉namenode节点，在master上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh stop namenode</span><br></pre></td></tr></table></figure></p>
<h3 id="集中式管理集群"><a href="#集中式管理集群" class="headerlink" title="集中式管理集群"></a>集中式管理集群</h3><p>如果我们的集群里面有成千上万台机器，在每一台机器上面都这样来启动，肯定是不行的。下面我们将通过配置，只在一台机器上面执行一个脚本，就将整个集群启动。</p>
<p>配置SSH无密码登陆，分别在master、slave1和slave2上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>在master上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop-host-master</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-1</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>每执行一条命令的时候，都先输入yes，然后再输入目标机器的登录密码。</p>
<p>如果能成功运行如下命令，则配置免密登录其他机器成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop-host-master</span><br><span class="line">$ ssh hadoop-host-slave-1</span><br><span class="line">$ ssh hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>在master上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi slaves    <span class="comment"># 加入如下内容</span></span><br><span class="line">$</span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>当执行<code>start-dfs.sh</code>时，它会去slaves文件中找从节点。</p>
<p>在master上面启动整个集群：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面通过<code>jps</code>命令可以看到整个集群已经成功启动。同样的，停掉整个集群的命令，如下，同样是在master上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>相关操作，如下图所示：</p>
<p><img src="/img/hadoop-9.png" alt="" title="hadoop集中式管理"></p>
<p><strong> 注意：在主节点执行<code>start-dfs.sh</code>，主节点的用户名必须和所有从节点的用户名相同。因为主节点服务器以这个用户名去远程登录到其他从节点的服务器中，所以在所有的生产环境中控制同一类集群的用户一定要相同。 </strong></p>
<h3 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h3><p>分别在master、slave1和slave2上面都建如下目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p yarn/nm</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 yarn/</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面都按如下方式配置mapred-site.xml，刚解压的hadoop是没有mapred-site.xml的，但是有mapred-site.xml.template，我们修改文件名，并作如下配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ mv mapred-site.xml.template mapred-site.xml</span><br><span class="line">$ vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面都按如下方式配置yarn-site.xml<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定ResourceManager的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop-host-master&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定reducer获取数据的方式 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/home/hadoop/hadoop-2.7.3/yarn/nm&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>配置好了，下面开始启动：<br>在master上面启动resourcemanager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure></p>
<p>在slave1、slave2上面分别启动nodemanager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure></p>
<p>我们可以通过浏览器，查看资源管理器：<br><a href="http://hadoop-host-master:8088/" target="_blank" rel="noopener">http://hadoop-host-master:8088/</a></p>
<p><img src="/img/hadoop-10.png" alt="" title="yarn资源管理"></p>
<p>点击图中的<code>Active Nodes</code>可以看到下图的详情，（如果<code>Unhealthy Nodes</code>有节点，则可能是由于虚拟机中主机的磁盘空间不足所致）。</p>
<p><img src="/img/hadoop-11.png" alt="" title="yarn资源管理"></p>
<p>当然相应的停止命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/yarn-daemon.sh stop resourcemanager</span><br><span class="line">$ ./sbin/yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure></p>
<p>如果有配置集中式管理，我们也可以通过在master上面通过一个命令启动、停止YARN<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-yarn.sh    <span class="comment"># 启动yarn</span></span><br><span class="line">$ ./sbin/stop-yarn.sh    <span class="comment"># 停止yarn</span></span><br></pre></td></tr></table></figure></p>
<p>或者在master上面，通过一个命令启动hadoop和yarn<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-all.sh</span><br><span class="line">或者按顺序执行如下两个命令</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br><span class="line">$ ./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hadoop-12.png" alt="" title="yarn集中启动管理"></p>
<h3 id="启动MR作业日志管理器"><a href="#启动MR作业日志管理器" class="headerlink" title="启动MR作业日志管理器"></a>启动MR作业日志管理器</h3><p>在namenode节点，也就是master，启动MR作业日志管理器。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></p>
<p>它同样有自已的图形界面：<br><a href="http://hadoop-host-master:19888/" target="_blank" rel="noopener">http://hadoop-host-master:19888/</a></p>
<p><img src="/img/hadoop-13.png" alt="" title="MR作业日志管理器"></p>
<h3 id="尝试向集群中提交一个mapReduce任务"><a href="#尝试向集群中提交一个mapReduce任务" class="headerlink" title="尝试向集群中提交一个mapReduce任务"></a>尝试向集群中提交一个mapReduce任务</h3><p>我们在namenode节点中向集群提交一个计算圆周率的mapReduce任务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 4 10</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hadoop-14.png" alt="" title="计算圆周率"></p>
<p><img src="/img/hadoop-15.png" alt="" title="计算圆周率"></p>
<p>从上图可以看出，圆周率已经被计算出来：<code>3.40</code>。另外，在yarn中也可以看到任务的执行情况：<br><img src="/img/hadoop-16.png" alt="" title="计算圆周率"></p>
<p>至此， 集群搭建完毕。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/12/04/hadoop-cluster/" data-id="clf9thvl8000l6h3kwj2blvf4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/12/19/hadoop-mapreduce/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          hadoop mapreduce 示例
        
      </div>
    </a>
  
  
    <a href="/2018/11/12/kafka-intro/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">kafka 介绍</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/">bigdata</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/container/">container</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/db/">db</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/other/">other</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web-server/">web server</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/03/07/xxl-job-note/">xxl-job 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/11/28/seata-note/">seata 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/09/25/spring-note/">spring 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/04/01/mybatis-note/">mybatis 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/01/22/canal-note/">canal 学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">link</h3>
    <div class="widget">
      <li><a href="https://github.com/hewentian" title="Tim Ho's Blog">我的github</a></li>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Tim Ho<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/categories/" class="mobile-nav-link">Categories</a>
  
    <a href="/tags/" class="mobile-nav-link">Tags</a>
  
    <a href="/about/" class="mobile-nav-link">About</a>
  
</nav>
    

<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script> -->
<!-- <script src="//code.jquery.com/jquery-2.2.4.min.js"></script> -->
<script src="/js/jquery-3.5.1.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>