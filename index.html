<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tim Ho&#39;s Technology Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="my personal blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Tim Ho&#39;s Technology Blog">
<meta property="og:url" content="https://github.com/hewentian/index.html">
<meta property="og:site_name" content="Tim Ho&#39;s Technology Blog">
<meta property="og:description" content="my personal blog">
<meta property="og:locale" content="en-US">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tim Ho&#39;s Technology Blog">
<meta name="twitter:description" content="my personal blog">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Tim Ho&#39;s Technology Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">心如止水</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/categories/">Categories</a>
        
          <a class="main-nav-link" href="/tags/">Tags</a>
        
          <a class="main-nav-link" href="/about/">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://github.com/hewentian"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-docker-note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/26/docker-note/" class="article-date">
  <time datetime="2019-07-26T02:32:30.000Z" itemprop="datePublished">2019-07-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/container/">container</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/26/docker-note/">docker 学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天开始学习一下docker，因为它越来越流行了。首先，我们来安装一下。因为我的系统为<code>Ubuntu Linux</code>，所以我的安装过程，参照这里：<br><a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" target="_blank" rel="noopener">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a></p>
<p>安装过程如下，在系统上首次安装的时候需要执行<strong>安装前的准备工作</strong>：</p>
<h3 id="安装前的准备工作"><a href="#安装前的准备工作" class="headerlink" title="安装前的准备工作"></a>安装前的准备工作</h3><p>第一步：首先卸载系统自带的旧版docker（如系统有预安装docker）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get remove docker docker-engine docker.io containerd runc</span><br></pre></td></tr></table></figure></p>
<p>第二步：更新<code>apt</code>包索引：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>第三步：安装允许<code>apt</code>通过HTTPS访问仓库的软件包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg-agent \</span><br><span class="line">    software-properties-common</span><br></pre></td></tr></table></figure></p>
<p>第四步： 添加docker官方的GPG key：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure></p>
<p>验证添加KEY的指纹是否是<code>9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88</code>，我们只需搜索最后8个字符即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-key fingerprint 0EBFCD88</span><br><span class="line">pub   rsa4096 2017-02-22 [SCEA]</span><br><span class="line">      9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88</span><br><span class="line">uid           [ unknown] Docker Release (CE deb) &lt;docker@docker.com&gt;</span><br><span class="line">sub   rsa4096 2017-02-22 [S]</span><br></pre></td></tr></table></figure></p>
<p>第五步：将docker的下载路径添加到下载源中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo add-apt-repository \</span><br><span class="line">   <span class="string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">   <span class="variable">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">   stable"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="开始正式安装"><a href="#开始正式安装" class="headerlink" title="开始正式安装"></a>开始正式安装</h3><p>第一步：更新<code>apt</code>包索引：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>第二步：开始安装docker最新的社区版：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure></p>
<p>安装结束后，docker的daemon进程默认自动启动，否则需要手动启动。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service docker start</span><br></pre></td></tr></table></figure></p>
<p>可选命令有：</p>
<pre><code>service docker {start|stop|restart|status}
</code></pre><p>第三步：验证安装是否正确无误，通过运行一个测试用例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run hello-world</span><br><span class="line"></span><br><span class="line">Unable to find image <span class="string">'hello-world:latest'</span> locally</span><br><span class="line">latest: Pulling from library/hello-world</span><br><span class="line">1b930d010525: Pull complete </span><br><span class="line">Digest: sha256:6540fc08ee6e6b7b63468dc3317e3303aae178cb8a45ed3123180328bcc1d20f</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> hello-world:latest</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">To generate this message, Docker took the following steps:</span><br><span class="line"> 1. The Docker client contacted the Docker daemon.</span><br><span class="line"> 2. The Docker daemon pulled the <span class="string">"hello-world"</span> image from the Docker Hub.</span><br><span class="line">    (amd64)</span><br><span class="line"> 3. The Docker daemon created a new container from that image <span class="built_in">which</span> runs the</span><br><span class="line">    executable that produces the output you are currently reading.</span><br><span class="line"> 4. The Docker daemon streamed that output to the Docker client, <span class="built_in">which</span> sent it</span><br><span class="line">    to your terminal.</span><br><span class="line"></span><br><span class="line">To try something more ambitious, you can run an Ubuntu container with:</span><br><span class="line"> $ docker run -it ubuntu bash</span><br><span class="line"></span><br><span class="line">Share images, automate workflows, and more with a free Docker ID:</span><br><span class="line"> https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">For more examples and ideas, visit:</span><br><span class="line"> https://docs.docker.com/get-started/</span><br></pre></td></tr></table></figure></p>
<p>至此，安装完成。</p>
<h3 id="卸载方法"><a href="#卸载方法" class="headerlink" title="卸载方法"></a>卸载方法</h3><p>要卸载软件本身和删除相关的docker文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get purge docker-ce</span><br><span class="line">$ sudo rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure></p>
<h3 id="docker的一些操作命令"><a href="#docker的一些操作命令" class="headerlink" title="docker的一些操作命令"></a>docker的一些操作命令</h3><p>一个简单的示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run ubuntu:18.04 /bin/<span class="built_in">echo</span> <span class="string">"Hello world"</span></span><br></pre></td></tr></table></figure></p>
<p>或者交互式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -i -t ubuntu:18.04 /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>以后台模式启动容器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -d ubuntu:18.04 /bin/sh -c <span class="string">"while true; do echo hello world; sleep 1; done"</span></span><br><span class="line"></span><br><span class="line">d7c1549c2a495499270eb31819ce5e9ea9748ab8126f025f33b06612491fd447</span><br></pre></td></tr></table></figure></p>
<p>输出的那一长长的字符串，是容器ID。</p>
<p>查看容器内的标准输出：</p>
<pre><code>sudo docker logs -f {CONTAINER ID}|{NAMES}
</code></pre><p>示例如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker logs d7c1549c2a495499270eb31819ce5e9ea9748ab8126f025f33b06612491fd447</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure></p>
<h3 id="查看正在运行中的容器"><a href="#查看正在运行中的容器" class="headerlink" title="查看正在运行中的容器"></a>查看正在运行中的容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">d7c1549c2a49        ubuntu:18.04        <span class="string">"/bin/sh -c 'while t…"</span>   4 minutes ago       Up 4 minutes                            friendly_minsky</span><br></pre></td></tr></table></figure>
<h3 id="停止容器，可以使用容器ID或者容器名"><a href="#停止容器，可以使用容器ID或者容器名" class="headerlink" title="停止容器，可以使用容器ID或者容器名"></a>停止容器，可以使用容器ID或者容器名</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker stop &#123;CONTAINER ID&#125;|&#123;NAMES&#125;</span><br></pre></td></tr></table></figure>
<h3 id="docker的帮助命令"><a href="#docker的帮助命令" class="headerlink" title="docker的帮助命令"></a>docker的帮助命令</h3><p>在命令行中直接输入docker即可看到它的提示，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">$ docker</span><br><span class="line"></span><br><span class="line">Usage:	docker [OPTIONS] COMMAND</span><br><span class="line"></span><br><span class="line">A self-sufficient runtime <span class="keyword">for</span> containers</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">      --config string      Location of client config files (default <span class="string">"/home/hewentian/.docker"</span>)</span><br><span class="line">  -c, --context string     Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context <span class="built_in">set</span> with <span class="string">"docker context use"</span>)</span><br><span class="line">  -D, --debug              Enable debug mode</span><br><span class="line">  -H, --host list          Daemon socket(s) to connect to</span><br><span class="line">  -l, --<span class="built_in">log</span>-level string   Set the logging level (<span class="string">"debug"</span>|<span class="string">"info"</span>|<span class="string">"warn"</span>|<span class="string">"error"</span>|<span class="string">"fatal"</span>) (default <span class="string">"info"</span>)</span><br><span class="line">      --tls                Use TLS; implied by --tlsverify</span><br><span class="line">      --tlscacert string   Trust certs signed only by this CA (default <span class="string">"/home/hewentian/.docker/ca.pem"</span>)</span><br><span class="line">      --tlscert string     Path to TLS certificate file (default <span class="string">"/home/hewentian/.docker/cert.pem"</span>)</span><br><span class="line">      --tlskey string      Path to TLS key file (default <span class="string">"/home/hewentian/.docker/key.pem"</span>)</span><br><span class="line">      --tlsverify          Use TLS and verify the remote</span><br><span class="line">  -v, --version            Print version information and quit</span><br><span class="line"></span><br><span class="line">Management Commands:</span><br><span class="line">  builder     Manage builds</span><br><span class="line">  config      Manage Docker configs</span><br><span class="line">  container   Manage containers</span><br><span class="line">  context     Manage contexts</span><br><span class="line">  engine      Manage the docker engine</span><br><span class="line">  image       Manage images</span><br><span class="line">  network     Manage networks</span><br><span class="line">  node        Manage Swarm nodes</span><br><span class="line">  plugin      Manage plugins</span><br><span class="line">  secret      Manage Docker secrets</span><br><span class="line">  service     Manage services</span><br><span class="line">  stack       Manage Docker stacks</span><br><span class="line">  swarm       Manage Swarm</span><br><span class="line">  system      Manage Docker</span><br><span class="line">  trust       Manage trust on Docker images</span><br><span class="line">  volume      Manage volumes</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  attach      Attach <span class="built_in">local</span> standard input, output, and error streams to a running container</span><br><span class="line">  build       Build an image from a Dockerfile</span><br><span class="line">  commit      Create a new image from a container<span class="string">'s changes</span></span><br><span class="line"><span class="string">  cp          Copy files/folders between a container and the local filesystem</span></span><br><span class="line"><span class="string">  create      Create a new container</span></span><br><span class="line"><span class="string">  deploy      Deploy a new stack or update an existing stack</span></span><br><span class="line"><span class="string">  diff        Inspect changes to files or directories on a container'</span>s filesystem</span><br><span class="line">  events      Get real time events from the server</span><br><span class="line">  <span class="built_in">exec</span>        Run a <span class="built_in">command</span> <span class="keyword">in</span> a running container</span><br><span class="line">  <span class="built_in">export</span>      Export a container<span class="string">'s filesystem as a tar archive</span></span><br><span class="line"><span class="string">  history     Show the history of an image</span></span><br><span class="line"><span class="string">  images      List images</span></span><br><span class="line"><span class="string">  import      Import the contents from a tarball to create a filesystem image</span></span><br><span class="line"><span class="string">  info        Display system-wide information</span></span><br><span class="line"><span class="string">  inspect     Return low-level information on Docker objects</span></span><br><span class="line"><span class="string">  kill        Kill one or more running containers</span></span><br><span class="line"><span class="string">  load        Load an image from a tar archive or STDIN</span></span><br><span class="line"><span class="string">  login       Log in to a Docker registry</span></span><br><span class="line"><span class="string">  logout      Log out from a Docker registry</span></span><br><span class="line"><span class="string">  logs        Fetch the logs of a container</span></span><br><span class="line"><span class="string">  pause       Pause all processes within one or more containers</span></span><br><span class="line"><span class="string">  port        List port mappings or a specific mapping for the container</span></span><br><span class="line"><span class="string">  ps          List containers</span></span><br><span class="line"><span class="string">  pull        Pull an image or a repository from a registry</span></span><br><span class="line"><span class="string">  push        Push an image or a repository to a registry</span></span><br><span class="line"><span class="string">  rename      Rename a container</span></span><br><span class="line"><span class="string">  restart     Restart one or more containers</span></span><br><span class="line"><span class="string">  rm          Remove one or more containers</span></span><br><span class="line"><span class="string">  rmi         Remove one or more images</span></span><br><span class="line"><span class="string">  run         Run a command in a new container</span></span><br><span class="line"><span class="string">  save        Save one or more images to a tar archive (streamed to STDOUT by default)</span></span><br><span class="line"><span class="string">  search      Search the Docker Hub for images</span></span><br><span class="line"><span class="string">  start       Start one or more stopped containers</span></span><br><span class="line"><span class="string">  stats       Display a live stream of container(s) resource usage statistics</span></span><br><span class="line"><span class="string">  stop        Stop one or more running containers</span></span><br><span class="line"><span class="string">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</span></span><br><span class="line"><span class="string">  top         Display the running processes of a container</span></span><br><span class="line"><span class="string">  unpause     Unpause all processes within one or more containers</span></span><br><span class="line"><span class="string">  update      Update configuration of one or more containers</span></span><br><span class="line"><span class="string">  version     Show the Docker version information</span></span><br><span class="line"><span class="string">  wait        Block until one or more containers stop, then print their exit codes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Run '</span>docker COMMAND --<span class="built_in">help</span><span class="string">' for more information on a command.</span></span><br></pre></td></tr></table></figure></p>
<h3 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h3><p>默认从 <a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a> 查找我们需要的镜像，例如，搜索<code>httpd</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker search httpd</span><br><span class="line">NAME                                 DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">httpd                                The Apache HTTP Server Project                  2567                [OK]                </span><br><span class="line">centos/httpd                                                                         23                                      [OK]</span><br><span class="line">centos/httpd-24-centos7              Platform <span class="keyword">for</span> running Apache httpd 2.4 or bui…   22                                      </span><br><span class="line">armhf/httpd                          The Apache HTTP Server Project                  8                                       </span><br><span class="line">polinux/httpd-php                    Apache with PHP <span class="keyword">in</span> Docker (Supervisor, CentO…   3                                       [OK]</span><br></pre></td></tr></table></figure></p>
<h3 id="下载并运行容器"><a href="#下载并运行容器" class="headerlink" title="下载并运行容器"></a>下载并运行容器</h3><p>我们可以在 <a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a> 上面查询所有可用的镜像，找到需要的镜像后，可以下载，例如<code>training/webapp</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker pull training/webapp</span><br><span class="line">$ sudo docker run -d -P training/webapp python app.py</span><br><span class="line"></span><br><span class="line">a2d42ce3df7d0dc34b93095fe3cd526de22f75f2f49a7762e395c32cecae82e5</span><br></pre></td></tr></table></figure></p>
<p>我们也可以通过<code>-p</code>参数来设置不一样的端口，（格式为 本机端口:容器端口）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -d -p 5001:5000 training/webapp python app.py</span><br><span class="line"></span><br><span class="line">2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9</span><br><span class="line"></span><br><span class="line">$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                     NAMES</span><br><span class="line">2f8c4e68d8fb        training/webapp     <span class="string">"python app.py"</span>     27 seconds ago      Up 26 seconds       0.0.0.0:5001-&gt;5000/tcp    infallible_greider</span><br><span class="line">a2d42ce3df7d        training/webapp     <span class="string">"python app.py"</span>     41 seconds ago      Up 40 seconds       0.0.0.0:32769-&gt;5000/tcp   epic_pasteur</span><br></pre></td></tr></table></figure></p>
<p>这样在本机的浏览器上面通过如下2种方式，都能访问到应用：<br><a href="http://localhost:32769/" target="_blank" rel="noopener">http://localhost:32769/</a><br><a href="http://localhost:5001/" target="_blank" rel="noopener">http://localhost:5001/</a></p>
<h3 id="查看容器内的进程状况"><a href="#查看容器内的进程状况" class="headerlink" title="查看容器内的进程状况"></a>查看容器内的进程状况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker top infallible_greider</span><br><span class="line">UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD</span><br><span class="line">root                28721               28694               0                   14:49               ?                   00:00:00            python app.py</span><br></pre></td></tr></table></figure>
<h3 id="查看指定容器的配置和状态信息"><a href="#查看指定容器的配置和状态信息" class="headerlink" title="查看指定容器的配置和状态信息"></a>查看指定容器的配置和状态信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker inspect infallible_greider</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"Id"</span>: <span class="string">"2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9"</span>,</span><br><span class="line">        <span class="string">"Created"</span>: <span class="string">"2019-07-29T06:49:41.334728611Z"</span>,</span><br><span class="line">        <span class="string">"Path"</span>: <span class="string">"python"</span>,</span><br><span class="line">        <span class="string">"Args"</span>: [</span><br><span class="line">            <span class="string">"app.py"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"State"</span>: &#123;</span><br><span class="line">            <span class="string">"Status"</span>: <span class="string">"running"</span>,</span><br><span class="line">            <span class="string">"Running"</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">"Paused"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"Restarting"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"OOMKilled"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"Dead"</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">"Pid"</span>: 28721,</span><br><span class="line">            <span class="string">"ExitCode"</span>: 0,</span><br><span class="line">            <span class="string">"Error"</span>: <span class="string">""</span>,</span><br><span class="line">            <span class="string">"StartedAt"</span>: <span class="string">"2019-07-29T06:49:42.075479437Z"</span>,</span><br><span class="line">            <span class="string">"FinishedAt"</span>: <span class="string">"0001-01-01T00:00:00Z"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"Image"</span>: <span class="string">"sha256:6fae60ef344644649a39240b94d73b8ba9c67f898ede85cf8e947a887b3e6557"</span>,</span><br><span class="line">        <span class="string">"ResolvConfPath"</span>: <span class="string">"/var/lib/docker/containers/2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9/resolv.conf"</span>,</span><br><span class="line">        <span class="string">"HostnamePath"</span>: <span class="string">"/var/lib/docker/containers/2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9/hostname"</span>,</span><br><span class="line">        <span class="string">"HostsPath"</span>: <span class="string">"/var/lib/docker/containers/2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9/hosts"</span>,</span><br><span class="line">        <span class="string">"LogPath"</span>: <span class="string">"/var/lib/docker/containers/2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9/2f8c4e68d8fbb3130fb51197218b9024bed5de1c4614cd7cca198a68807b57a9-json.log"</span>,</span><br><span class="line">        <span class="string">"Name"</span>: <span class="string">"/infallible_greider"</span>,</span><br><span class="line">        <span class="string">"RestartCount"</span>: 0,</span><br><span class="line">        <span class="string">"Driver"</span>: <span class="string">"overlay2"</span>,</span><br><span class="line">        <span class="string">"Platform"</span>: <span class="string">"linux"</span>,</span><br><span class="line">        <span class="string">"MountLabel"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"ProcessLabel"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"AppArmorProfile"</span>: <span class="string">"docker-default"</span>,</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="容器可以停止、重新启动和移除"><a href="#容器可以停止、重新启动和移除" class="headerlink" title="容器可以停止、重新启动和移除"></a>容器可以停止、重新启动和移除</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker start infallible_greider</span><br><span class="line">$ sudo docker stop infallible_greider</span><br><span class="line">$ sudo docker rm infallible_greider</span><br></pre></td></tr></table></figure>
<h3 id="列出本机上的所有镜像"><a href="#列出本机上的所有镜像" class="headerlink" title="列出本机上的所有镜像"></a>列出本机上的所有镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker images</span><br><span class="line"></span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu              18.04               3556258649b2        5 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        6 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure>
<h3 id="列出本机上所有已创建的容器"><a href="#列出本机上所有已创建的容器" class="headerlink" title="列出本机上所有已创建的容器"></a>列出本机上所有已创建的容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                        PORTS               NAMES</span><br><span class="line">ee4b72860ab3        ubuntu:18.04        <span class="string">"/bin/sh -c 'while t…"</span>   8 minutes ago       Up 8 minutes                                      brave_edison</span><br><span class="line">3063341debf2        ubuntu:18.04        <span class="string">"/bin/bash"</span>              9 minutes ago       Exited (0) 9 minutes ago                          friendly_poitras</span><br><span class="line">6623f20692a4        ubuntu:18.04        <span class="string">"/bin/echo 'Hello wo…"</span>   9 minutes ago       Exited (0) 9 minutes ago                          intelligent_roentgen</span><br><span class="line">a2d42ce3df7d        training/webapp     <span class="string">"python app.py"</span>          2 hours ago         Exited (137) 48 minutes ago                       epic_pasteur</span><br><span class="line">1058cf5fafad        training/webapp     <span class="string">"python app.py"</span>          2 hours ago         Exited (137) 2 hours ago                          goofy_lewin</span><br><span class="line">2c113cc40c51        training/webapp     <span class="string">"python app.py"</span>          2 hours ago         Exited (137) 2 hours ago                          confident_gagarin</span><br><span class="line">c9102f1d9541        training/webapp     <span class="string">"python app.py"</span>          3 hours ago         Exited (137) 2 hours ago                          crazy_borg</span><br><span class="line">a4e84d331fbd        hello-world         <span class="string">"/hello"</span>                 6 hours ago         Exited (0) 6 hours ago                            epic_fermi</span><br><span class="line">b03a6f03bd39        hello-world         <span class="string">"/hello"</span>                 2 days ago          Exited (0) 2 days ago                             competent_cartwright</span><br><span class="line">b7c3f4699549        hello-world         <span class="string">"/hello"</span>                 2 days ago          Exited (0) 2 days ago                             crazy_brattain</span><br><span class="line">bb7eb9e197b4        hello-world         <span class="string">"/hello"</span>                 2 days ago          Exited (0) 2 days ago                             pensive_lalande</span><br></pre></td></tr></table></figure>
<h3 id="修改镜像"><a href="#修改镜像" class="headerlink" title="修改镜像"></a>修改镜像</h3><p>我们以已存在的ubuntu镜像为原始版本，创建新的镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -t -i ubuntu:18.04 /bin/bash</span><br><span class="line">root@d23dc5d88f11:/<span class="comment"># apt-get update</span></span><br><span class="line">root@d23dc5d88f11:/<span class="comment"># exit</span></span><br></pre></td></tr></table></figure></p>
<p>查看最后创建的容器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker ps -l</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                          PORTS               NAMES</span><br><span class="line">d23dc5d88f11        ubuntu:18.04        <span class="string">"/bin/bash"</span>         2 minutes ago       Exited (0) About a minute ago                       amazing_vaughan</span><br></pre></td></tr></table></figure></p>
<p>可以看到ID为<code>d23dc5d88f11</code>的容器为我们刚才创建的容器，提交这个容器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker commit -m=<span class="string">"exec apt-get update"</span> -a=<span class="string">"hewentian"</span> d23dc5d88f11 hewentian/ubuntu:v2</span><br><span class="line"></span><br><span class="line">sha256:2bdf86d10fbc18204e04fe5a30dee06dfeb30683247c41e85e8cfe6d66d5d9d6</span><br></pre></td></tr></table></figure></p>
<p>查看我们刚刚创建的镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        6 seconds ago       91MB</span><br><span class="line">ubuntu              18.04               3556258649b2        5 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        6 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p>然后，我们就可以使用我们新建的镜像创建容器了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -it hewentian/ubuntu:v2 /bin/bash</span><br><span class="line">root@10adcace776b:/<span class="comment"># cat /proc/version</span></span><br><span class="line">Linux version 4.15.0-47-generic (buildd@lgw01-amd64-001) (gcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)) <span class="comment">#50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019</span></span><br><span class="line">root@10adcace776b:/<span class="comment"># whoami</span></span><br><span class="line">root</span><br><span class="line">root@10adcace776b:/<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<h3 id="创建镜像"><a href="#创建镜像" class="headerlink" title="创建镜像"></a>创建镜像</h3><p>从零开始创建一个镜像，我们需要一个Dockerfile文件，示例如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ cat /home/hewentian/Documents/docker/ubuntu/Dockerfile</span><br><span class="line"></span><br><span class="line">FROM    ubuntu:18.04</span><br><span class="line">MAINTAINER    hewentian <span class="string">"wentian.he@qq.com"</span></span><br><span class="line"></span><br><span class="line">ENV    AUTHOR=<span class="string">"hewentian"</span></span><br><span class="line">WORKDIR    /tmp/</span><br><span class="line">RUN    /usr/bin/touch he.txt</span><br><span class="line">RUN    /bin/<span class="built_in">echo</span> <span class="string">"The author is <span class="variable">$AUTHOR</span>, created at "</span> &gt;&gt; /tmp/he.txt</span><br><span class="line">RUN    /bin/date &gt;&gt; /tmp/he.txt</span><br></pre></td></tr></table></figure></p>
<p>开始创建镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker build -t hewentian/ubuntu:v2.1 -f /home/hewentian/Documents/docker/ubuntu/Dockerfile .</span><br><span class="line"></span><br><span class="line">Sending build context to Docker daemon   2.56kB</span><br><span class="line">Step 1/7 : FROM    ubuntu:18.04</span><br><span class="line"> ---&gt; 3556258649b2</span><br><span class="line">Step 2/7 : MAINTAINER    hewentian <span class="string">"wentian.he@qq.com"</span></span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 9684fd7dab36</span><br><span class="line">Removing intermediate container 9684fd7dab36</span><br><span class="line"> ---&gt; 87f25ba61a99</span><br><span class="line">Step 3/7 : ENV    AUTHOR=<span class="string">"hewentian"</span></span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 22e933129053</span><br><span class="line">Removing intermediate container 22e933129053</span><br><span class="line"> ---&gt; 23c5a574b01c</span><br><span class="line">Step 4/7 : WORKDIR    /tmp/</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> a4341dbc2164</span><br><span class="line">Removing intermediate container a4341dbc2164</span><br><span class="line"> ---&gt; 94663075f2b0</span><br><span class="line">Step 5/7 : RUN    /usr/bin/touch he.txt</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> e54479ffd964</span><br><span class="line">Removing intermediate container e54479ffd964</span><br><span class="line"> ---&gt; a196207c63e9</span><br><span class="line">Step 6/7 : RUN    /bin/<span class="built_in">echo</span> <span class="string">"The author is <span class="variable">$AUTHOR</span>, created at "</span> &gt;&gt; /tmp/he.txt</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 89d010bd1b78</span><br><span class="line">Removing intermediate container 89d010bd1b78</span><br><span class="line"> ---&gt; 11aa9b6d3605</span><br><span class="line">Step 7/7 : RUN    /bin/date &gt;&gt; /tmp/he.txt</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 66627425d24c</span><br><span class="line">Removing intermediate container 66627425d24c</span><br><span class="line"> ---&gt; c6cd98aa1461</span><br><span class="line">Successfully built c6cd98aa1461</span><br><span class="line">Successfully tagged hewentian/ubuntu:v2.1</span><br></pre></td></tr></table></figure></p>
<p>查看生成的镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu    v2.1                c6cd98aa1461        43 seconds ago      64.2MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        18 hours ago        91MB</span><br><span class="line">ubuntu              18.04               3556258649b2        6 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p>用我们新建的镜像创建容器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -it hewentian/ubuntu:v2.1 /bin/bash</span><br><span class="line"></span><br><span class="line">root@335d56425694:/tmp<span class="comment"># ls /tmp/</span></span><br><span class="line">he.txt</span><br><span class="line">root@335d56425694:/tmp<span class="comment"># more /tmp/he.txt </span></span><br><span class="line">The author is hewentian, created at </span><br><span class="line">Tue Jul 30 03:10:50 UTC 2019</span><br><span class="line">root@335d56425694:/tmp<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>可见，镜像包含我们自已创建的文件。</p>
<h3 id="docker安装nginx"><a href="#docker安装nginx" class="headerlink" title="docker安装nginx"></a>docker安装nginx</h3><p>首先拉取镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker search nginx</span><br><span class="line">$ sudo docker pull nginx</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu    v2.1                c6cd98aa1461        4 hours ago         64.2MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        22 hours ago        91MB</span><br><span class="line">nginx               latest              e445ab08b2be        6 days ago          126MB</span><br><span class="line">ubuntu              18.04               3556258649b2        6 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p>使用nginx的默认配置来启动一个容器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --name nginx-test -p 8081:80 -d nginx</span><br><span class="line"></span><br><span class="line">838ebabcc937cf9a8e13946f92d104b2eb153cc61f21cb47e7661d6bbe205253</span><br></pre></td></tr></table></figure></p>
<p>如果启动成功，则可以在浏览器中访问：<br><a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081/</a></p>
<p>然后，开始部署我们想要的nginx，首先在本机上创建nginx相关文件目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/Documents/docker</span><br><span class="line">$ mkdir -p nginx/www nginx/logs nginx/conf</span><br></pre></td></tr></table></figure></p>
<p>将刚才启动的nginx容器内的配置文件，复制到本机中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker cp 838ebabcc937:/etc/nginx/nginx.conf /home/hewentian/Documents/docker/nginx/conf</span><br></pre></td></tr></table></figure></p>
<p><strong>docker cp: 用于本地主机与容器之间的数据复制</strong></p>
<p>创建nginx欢迎页面<code>/home/hewentian/Documents/docker/nginx/www/index.html</code>：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>Welcome to nginx!<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">    body &#123;</span></span><br><span class="line"><span class="undefined">        width: 35em;</span></span><br><span class="line"><span class="undefined">        margin: 0 auto;</span></span><br><span class="line"><span class="undefined">        font-family: Tahoma, Verdana, Arial, sans-serif;</span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Welcome to docker nginx!<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>For online documentation and support please refer to</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://nginx.org/"</span>&gt;</span>nginx.org<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">Commercial support is available at</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://nginx.com/"</span>&gt;</span>nginx.com<span class="tag">&lt;/<span class="name">a</span>&gt;</span>.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">em</span>&gt;</span>Thank you for using nginx.<span class="tag">&lt;/<span class="name">em</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>启动nginx：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --name nginx-test2 -p 8082:80 -d -v /home/hewentian/Documents/docker/nginx/www:/usr/share/nginx/html -v /home/hewentian/Documents/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /home/hewentian/Documents/docker/nginx/logs:/var/<span class="built_in">log</span>/nginx nginx</span><br></pre></td></tr></table></figure></p>
<p>参数说明：</p>
<pre><code>-v /home/hewentian/Documents/docker/nginx/www:/usr/share/nginx/html：将在本机创建的目录，挂载到容器内的/usr/share/nginx/html目录
</code></pre><p>如果启动成功，则可以在浏览器中访问：<br><a href="http://localhost:8082/" target="_blank" rel="noopener">http://localhost:8082/</a></p>
<h3 id="进入指定的容器"><a href="#进入指定的容器" class="headerlink" title="进入指定的容器"></a>进入指定的容器</h3><p>若启动容器的时候不是以交互模式，之后又想进入容器，则可以使用如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">先启动一个之前停止了的容器</span><br><span class="line">$ sudo docker start 3063341debf2</span><br><span class="line">3063341debf2</span><br><span class="line"></span><br><span class="line">直接运行容器内的脚本</span><br><span class="line">$ sudo docker <span class="built_in">exec</span> -it 3063341debf2 /bin/bash /a.sh</span><br><span class="line">Wed Jul 31 01:40:20 UTC 2019</span><br><span class="line"></span><br><span class="line">以交互模式进入容器</span><br><span class="line">$ sudo docker <span class="built_in">exec</span> -it 3063341debf2 /bin/bash</span><br><span class="line">root@3063341debf2:/<span class="comment"># ls</span></span><br><span class="line">a.sh  bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">root@3063341debf2:/<span class="comment"># sh a.sh </span></span><br><span class="line">Wed Jul 31 01:44:34 UTC 2019</span><br><span class="line">root@3063341debf2:/<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">退出后，容器并不会停止</span><br><span class="line">$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">3063341debf2        ubuntu:18.04        <span class="string">"/bin/bash"</span>         41 hours ago        Up 18 seconds                           friendly_poitras</span><br></pre></td></tr></table></figure></p>
<p>另外，使用<code>attach</code>命令也能进入容器，但是当退出后，容器会停止<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">3063341debf2        ubuntu:18.04        <span class="string">"/bin/bash"</span>         41 hours ago        Up 18 seconds                           friendly_poitras</span><br><span class="line"></span><br><span class="line">$ sudo docker attach --sig-proxy=<span class="literal">false</span> 3063341debf2</span><br><span class="line">root@3063341debf2:/<span class="comment"># ls</span></span><br><span class="line">a.sh  bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">root@3063341debf2:/<span class="comment"># sh a.sh </span></span><br><span class="line">Wed Jul 31 02:02:50 UTC 2019</span><br><span class="line">root@3063341debf2:/<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br></pre></td></tr></table></figure></p>
<h3 id="将镜像导出-导入"><a href="#将镜像导出-导入" class="headerlink" title="将镜像导出/导入"></a>将镜像导出/导入</h3><p>导出镜像语法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker save --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">Usage:	docker save [OPTIONS] IMAGE [IMAGE...]</span><br><span class="line"></span><br><span class="line">Save one or more images to a tar archive (streamed to STDOUT by default)</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -o, --output string   Write to a file, instead of STDOUT</span><br></pre></td></tr></table></figure></p>
<p>导入镜像语法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker load --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">Usage:	docker load [OPTIONS]</span><br><span class="line"></span><br><span class="line">Load an image from a tar archive or STDIN</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -i, --input string   Read from tar archive file, instead of STDIN</span><br><span class="line">  -q, --quiet          Suppress the load output</span><br></pre></td></tr></table></figure></p>
<p>示例：先将镜像导出，然后删除镜像，最后再将导出的镜像重新导入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">&lt;none&gt;              &lt;none&gt;              c6cd98aa1461        27 hours ago        64.2MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        45 hours ago        91MB</span><br><span class="line">nginx               latest              e445ab08b2be        7 days ago          126MB</span><br><span class="line">ubuntu              18.04               3556258649b2        7 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br><span class="line"></span><br><span class="line">$ sudo docker save -o hu.tar hewentian/ubuntu:v2</span><br><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">hu.tar</span><br><span class="line"></span><br><span class="line">$ sudo docker rmi 2bdf86d10fbc</span><br><span class="line">Untagged: hewentian/ubuntu:v2</span><br><span class="line">Deleted: sha256:2bdf86d10fbc18204e04fe5a30dee06dfeb30683247c41e85e8cfe6d66d5d9d6</span><br><span class="line">Deleted: sha256:c4a9226f13fa8f48ef07e27e0954c43f38275b6aa1d24e361ed016dfff056069</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">&lt;none&gt;              &lt;none&gt;              c6cd98aa1461        27 hours ago        64.2MB</span><br><span class="line">nginx               latest              e445ab08b2be        7 days ago          126MB</span><br><span class="line">ubuntu              18.04               3556258649b2        7 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br><span class="line"></span><br><span class="line">$ sudo docker load -i hu.tar </span><br><span class="line">ed4797628ae8: Loading layer [==================================================&gt;]  26.85MB/26.85MB</span><br><span class="line">Loaded image: hewentian/ubuntu:v2</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">&lt;none&gt;              &lt;none&gt;              c6cd98aa1461        27 hours ago        64.2MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        45 hours ago        91MB</span><br><span class="line">nginx               latest              e445ab08b2be        7 days ago          126MB</span><br><span class="line">ubuntu              18.04               3556258649b2        7 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p><strong>注意：导出镜像的时候，要使用<code>REPOSITORY:TAG</code>，而不是<code>IMAGE ID</code>，否则在重新导入的时候会没有<code>REPOSITORY:TAG</code>，显示为none</strong></p>
<h3 id="使用import创建镜像"><a href="#使用import创建镜像" class="headerlink" title="使用import创建镜像"></a>使用import创建镜像</h3><p>也可以从导出的tar文件中创建一个新的镜像，语法如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker import --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">Usage:	docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]</span><br><span class="line"></span><br><span class="line">Import the contents from a tarball to create a filesystem image</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -c, --change list      Apply Dockerfile instruction to the created image</span><br><span class="line">  -m, --message string   Set commit message <span class="keyword">for</span> imported image</span><br></pre></td></tr></table></figure></p>
<p>示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker import hu.tar hewentian/ubuntu:v2.1</span><br><span class="line">sha256:c389673d68c576b08ad8e3c2337de4ee3b4ed7e622fa986771323797edd2d595</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu    v2.1                c389673d68c5        5 seconds ago       66.6MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        45 hours ago        91MB</span><br><span class="line">nginx               latest              e445ab08b2be        7 days ago          126MB</span><br><span class="line">ubuntu              18.04               3556258649b2        7 days ago          64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p><strong>我试过使用import进去的镜像来创建容器，但是失败了，留待以后再解决</strong></p>
<h3 id="登录-登出镜像仓库"><a href="#登录-登出镜像仓库" class="headerlink" title="登录/登出镜像仓库"></a>登录/登出镜像仓库</h3><p>默认登录/登出官方仓库 <a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a> ，不过，也可以登录到指定的私有仓库。</p>
<p>登录语法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker login --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">Usage:	docker login [OPTIONS] [SERVER]</span><br><span class="line"></span><br><span class="line">Log <span class="keyword">in</span> to a Docker registry.</span><br><span class="line">If no server is specified, the default is defined by the daemon.</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -p, --password string   Password</span><br><span class="line">      --password-stdin    Take the password from stdin</span><br><span class="line">  -u, --username string   Username</span><br></pre></td></tr></table></figure></p>
<p>登出语法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker <span class="built_in">logout</span> --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">Usage:	docker <span class="built_in">logout</span> [SERVER]</span><br><span class="line"></span><br><span class="line">Log out from a Docker registry.</span><br><span class="line">If no server is specified, the default is defined by the daemon.</span><br></pre></td></tr></table></figure></p>
<p>登录/登出示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker login -u hewentian</span><br><span class="line">Password: </span><br><span class="line">WARNING! Your password will be stored unencrypted <span class="keyword">in</span> /home/hewentian/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/<span class="comment">#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ sudo docker <span class="built_in">logout</span> </span><br><span class="line">Removing login credentials <span class="keyword">for</span> https://index.docker.io/v1/</span><br></pre></td></tr></table></figure></p>
<h3 id="将本地镜像上传到镜像仓库"><a href="#将本地镜像上传到镜像仓库" class="headerlink" title="将本地镜像上传到镜像仓库"></a>将本地镜像上传到镜像仓库</h3><p>默认上传到docker官方仓库docker.io，上传到私有仓库的例子，后面会介绍。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker push hewentian/ubuntu:v2.1</span><br><span class="line">The push refers to repository [docker.io/hewentian/ubuntu]</span><br><span class="line">8c29bfccf50c: Pushed </span><br><span class="line">v2.1: digest: sha256:992cc4e008449d8285387fe80aff3c9b0574360fc3ad21b04bccc5b6a4229923 size: 528</span><br></pre></td></tr></table></figure></p>
<h3 id="harbor的安装"><a href="#harbor的安装" class="headerlink" title="harbor的安装"></a>harbor的安装</h3><p>我们将在机器<code>192.168.56.113</code>上面安装harbor，安装过程参考这里：<br><a href="https://github.com/goharbor/harbor">https://github.com/goharbor/harbor</a></p>
<p>harbor依赖<code>docker 17.06.0-ce+</code>、<code>docker-compose 1.18.0+</code>，其中<code>dock-ce</code>的安装参照上文。</p>
<p>安装<code>docker-compose</code>，参考： <a href="https://docs.docker.com/compose/install/#install-compose" target="_blank" rel="noopener">https://docs.docker.com/compose/install/#install-compose</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo curl -L <span class="string">"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-<span class="variable">$(uname -s)</span>-<span class="variable">$(uname -m)</span>"</span> -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">$ sudo chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line">$ docker-compose -version</span><br><span class="line">docker-compose version 1.24.1, build 4667896b</span><br></pre></td></tr></table></figure></p>
<p>开始安装harbor，安装之前需要启动docker，参考： <a href="https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md">https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md</a><br>下载离线安装包，在 <a href="https://github.com/goharbor/harbor/releases">https://github.com/goharbor/harbor/releases</a> 下载最新版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop</span><br><span class="line">$ wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.2-rc1.tgz</span><br><span class="line">$ tar xf harbor-offline-installer-v1.8.2-rc1.tgz</span><br><span class="line">$ <span class="built_in">cd</span> harbor</span><br><span class="line">$ ls </span><br><span class="line">harbor.v1.8.2.tar.gz  harbor.yml  install.sh  LICENSE  prepare</span><br></pre></td></tr></table></figure></p>
<p>安装前的配置，配置文件<code>harbor.yml</code>中<code>hostname</code>可以配置成IP地址或域名，主要是用于给客户端登录使用：<br>首先查看本机的<code>hostname</code>和IP地址<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ hostname</span><br><span class="line">hadoop-host-slave-3</span><br><span class="line"></span><br><span class="line">$ ifconfig</span><br><span class="line">enp0s3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.56.113  netmask 255.255.255.0  broadcast 192.168.56.255</span><br><span class="line">        inet6 fe80::90f2:2a79:288c:984e  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 08:00:27:9f:8e:7e  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 4726  bytes 448939 (448.9 KB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 7637  bytes 9739346 (9.7 MB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure></p>
<p>可以看到<code>hostname</code>是<code>hadoop-host-slave-3</code>，而IP地址是<code>192.168.56.113</code>。这里我们将<code>hostname</code>配置成<code>hadoop-host-slave-3</code>，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/harbor</span><br><span class="line">$ vi harbor.yml</span><br><span class="line"></span><br><span class="line">hostname: hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>开始安装，执行一个安装脚本即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/harbor</span><br><span class="line">$ sudo ./install.sh</span><br></pre></td></tr></table></figure></p>
<p>如无意外，你会看到如下的安装日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">[sudo] password <span class="keyword">for</span> hadoop: </span><br><span class="line"></span><br><span class="line">[Step 0]: checking installation environment ...</span><br><span class="line"></span><br><span class="line">Note: docker version: 19.03.1</span><br><span class="line"></span><br><span class="line">Note: docker-compose version: 1.24.1</span><br><span class="line"></span><br><span class="line">[Step 1]: loading Harbor images ...</span><br><span class="line">39b2d676308e: Loading layer [==================================================&gt;]  33.47MB/33.47MB</span><br><span class="line">f3583ea30104: Loading layer [==================================================&gt;]  3.552MB/3.552MB</span><br><span class="line">8290f582ffa5: Loading layer [==================================================&gt;]   6.59MB/6.59MB</span><br><span class="line">19913bc5e52b: Loading layer [==================================================&gt;]  161.3kB/161.3kB</span><br><span class="line">ae8b73743d1b: Loading layer [==================================================&gt;]    215kB/215kB</span><br><span class="line">5c811d1fe61a: Loading layer [==================================================&gt;]  3.584kB/3.584kB</span><br><span class="line">Loaded image: goharbor/harbor-portal:v1.8.2</span><br><span class="line">f27812f7a2da: Loading layer [==================================================&gt;]  8.971MB/8.971MB</span><br><span class="line">c74d2b18a2d1: Loading layer [==================================================&gt;]  38.82MB/38.82MB</span><br><span class="line">c416e128ff4c: Loading layer [==================================================&gt;]  38.82MB/38.82MB</span><br><span class="line">Loaded image: goharbor/harbor-jobservice:v1.8.2</span><br><span class="line">e97909585a09: Loading layer [==================================================&gt;]  8.972MB/8.972MB</span><br><span class="line">23b18d08698d: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">9c1d8c03df3e: Loading layer [==================================================&gt;]   20.1MB/20.1MB</span><br><span class="line">9666a22cf141: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">95783fa51b82: Loading layer [==================================================&gt;]  7.465MB/7.465MB</span><br><span class="line">285e05bca91e: Loading layer [==================================================&gt;]  27.56MB/27.56MB</span><br><span class="line">Loaded image: goharbor/harbor-registryctl:v1.8.2</span><br><span class="line">6543a3ba9bd9: Loading layer [==================================================&gt;]    338MB/338MB</span><br><span class="line">43f486f0ed18: Loading layer [==================================================&gt;]    107kB/107kB</span><br><span class="line">Loaded image: goharbor/harbor-migrator:v1.8.2</span><br><span class="line">6710d86773e1: Loading layer [==================================================&gt;]  50.51MB/50.51MB</span><br><span class="line">dba91d68db46: Loading layer [==================================================&gt;]  3.584kB/3.584kB</span><br><span class="line">4b6a61fc3477: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">efd64eeb5c31: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">25d50c6108dd: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">6c22404ddaf0: Loading layer [==================================================&gt;]  3.584kB/3.584kB</span><br><span class="line">135fef0d64a7: Loading layer [==================================================&gt;]  12.29kB/12.29kB</span><br><span class="line">Loaded image: goharbor/harbor-log:v1.8.2</span><br><span class="line">f080cac48a5f: Loading layer [==================================================&gt;]  3.552MB/3.552MB</span><br><span class="line">Loaded image: goharbor/nginx-photon:v1.8.2</span><br><span class="line">9562b05e7bd1: Loading layer [==================================================&gt;]  8.971MB/8.971MB</span><br><span class="line">2ff1ba9952dc: Loading layer [==================================================&gt;]  5.143MB/5.143MB</span><br><span class="line">463651a0baca: Loading layer [==================================================&gt;]  15.13MB/15.13MB</span><br><span class="line">feceecff30a6: Loading layer [==================================================&gt;]  26.47MB/26.47MB</span><br><span class="line">a2d1a1b1eaaa: Loading layer [==================================================&gt;]  22.02kB/22.02kB</span><br><span class="line">2c8463eca215: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">7e91f466c852: Loading layer [==================================================&gt;]  46.74MB/46.74MB</span><br><span class="line">Loaded image: goharbor/notary-server-photon:v0.6.1-v1.8.2</span><br><span class="line">628aac791456: Loading layer [==================================================&gt;]    113MB/113MB</span><br><span class="line">32e13bd19d15: Loading layer [==================================================&gt;]  10.94MB/10.94MB</span><br><span class="line">17d6a3366a31: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">9c3d274d3072: Loading layer [==================================================&gt;]  48.13kB/48.13kB</span><br><span class="line">a3e8bc524efe: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">6edf120ab0a5: Loading layer [==================================================&gt;]  10.99MB/10.99MB</span><br><span class="line">Loaded image: goharbor/clair-photon:v2.0.8-v1.8.2</span><br><span class="line">fa7f8bd666e1: Loading layer [==================================================&gt;]  8.972MB/8.972MB</span><br><span class="line">d23a3ac1da5c: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">25ece37b9b62: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">ceff80c4799d: Loading layer [==================================================&gt;]   20.1MB/20.1MB</span><br><span class="line">4ddaf99a2326: Loading layer [==================================================&gt;]   20.1MB/20.1MB</span><br><span class="line">Loaded image: goharbor/registry-photon:v2.7.1-patch-2819-v1.8.2</span><br><span class="line">86ef8960f9fa: Loading layer [==================================================&gt;]  13.72MB/13.72MB</span><br><span class="line">4be07cab0847: Loading layer [==================================================&gt;]  26.47MB/26.47MB</span><br><span class="line">b3f2bb8db417: Loading layer [==================================================&gt;]  22.02kB/22.02kB</span><br><span class="line">4c68837d983b: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">f2526a5c0965: Loading layer [==================================================&gt;]  45.33MB/45.33MB</span><br><span class="line">Loaded image: goharbor/notary-signer-photon:v0.6.1-v1.8.2</span><br><span class="line">9c6a2b28994d: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">49bb4e719955: Loading layer [==================================================&gt;]  1.536kB/1.536kB</span><br><span class="line">47d1a63f5482: Loading layer [==================================================&gt;]  69.81MB/69.81MB</span><br><span class="line">db449d60801c: Loading layer [==================================================&gt;]  39.75MB/39.75MB</span><br><span class="line">f01c7fa07db7: Loading layer [==================================================&gt;]  144.4kB/144.4kB</span><br><span class="line">5ff7a32e9f2c: Loading layer [==================================================&gt;]  3.005MB/3.005MB</span><br><span class="line">Loaded image: goharbor/prepare:v1.8.2</span><br><span class="line">6602e119ecab: Loading layer [==================================================&gt;]  8.971MB/8.971MB</span><br><span class="line">6b45eae45c58: Loading layer [==================================================&gt;]  46.86MB/46.86MB</span><br><span class="line">e3d9614f88b3: Loading layer [==================================================&gt;]  5.632kB/5.632kB</span><br><span class="line">f0b457c2a1b1: Loading layer [==================================================&gt;]  28.67kB/28.67kB</span><br><span class="line">f4e712369f36: Loading layer [==================================================&gt;]  46.86MB/46.86MB</span><br><span class="line">Loaded image: goharbor/harbor-core:v1.8.2</span><br><span class="line">c39fa71cb1b3: Loading layer [==================================================&gt;]   63.4MB/63.4MB</span><br><span class="line">245ad05b59aa: Loading layer [==================================================&gt;]  50.88MB/50.88MB</span><br><span class="line">6fc4b5ec5705: Loading layer [==================================================&gt;]  6.656kB/6.656kB</span><br><span class="line">8a003956ed73: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">0b4d3b06d5d5: Loading layer [==================================================&gt;]   7.68kB/7.68kB</span><br><span class="line">c045e2109691: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">eef5f9c09eb0: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">75776554d401: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">Loaded image: goharbor/harbor-db:v1.8.2</span><br><span class="line">0130cb61aaba: Loading layer [==================================================&gt;]  74.58MB/74.58MB</span><br><span class="line">9f0973beb46c: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">74bd291b6f8b: Loading layer [==================================================&gt;]   59.9kB/59.9kB</span><br><span class="line">3b11caba8d3e: Loading layer [==================================================&gt;]  61.95kB/61.95kB</span><br><span class="line">Loaded image: goharbor/redis-photon:v1.8.2</span><br><span class="line">5b00b48e6ec3: Loading layer [==================================================&gt;]  8.976MB/8.976MB</span><br><span class="line">7f5008b71ec6: Loading layer [==================================================&gt;]  44.39MB/44.39MB</span><br><span class="line">02f96d3b6e35: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">da8354357ee3: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">1819913851a3: Loading layer [==================================================&gt;]   44.4MB/44.4MB</span><br><span class="line">Loaded image: goharbor/chartmuseum-photon:v0.9.0-v1.8.2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Step 2]: preparing environment ...</span><br><span class="line">prepare base dir is <span class="built_in">set</span> to /home/hadoop/harbor</span><br><span class="line">Generated configuration file: /config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">Generated configuration file: /config/nginx/nginx.conf</span><br><span class="line">Generated configuration file: /config/core/env</span><br><span class="line">Generated configuration file: /config/core/app.conf</span><br><span class="line">Generated configuration file: /config/registry/config.yml</span><br><span class="line">Generated configuration file: /config/registryctl/env</span><br><span class="line">Generated configuration file: /config/db/env</span><br><span class="line">Generated configuration file: /config/jobservice/env</span><br><span class="line">Generated configuration file: /config/jobservice/config.yml</span><br><span class="line">Generated and saved secret to file: /secret/keys/secretkey</span><br><span class="line">Generated certificate, key file: /secret/core/private_key.pem, cert file: /secret/registry/root.crt</span><br><span class="line">Generated configuration file: /compose_location/docker-compose.yml</span><br><span class="line">Clean up the input dir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Step 3]: starting Harbor ...</span><br><span class="line">Creating network <span class="string">"harbor_harbor"</span> with the default driver</span><br><span class="line">Creating harbor-log ... <span class="keyword">done</span></span><br><span class="line">Creating redis       ... <span class="keyword">done</span></span><br><span class="line">Creating registryctl ... <span class="keyword">done</span></span><br><span class="line">Creating registry    ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-db   ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-core ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-portal     ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-jobservice ... <span class="keyword">done</span></span><br><span class="line">Creating nginx             ... <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">✔ ----Harbor has been installed and started successfully.----</span><br><span class="line"></span><br><span class="line">Now you should be able to visit the admin portal at http://hadoop-host-slave-3. </span><br><span class="line">For more details, please visit https://github.com/goharbor/harbor .</span><br></pre></td></tr></table></figure></p>
<p>按照上面的提示，我们在浏览器中访问<br><a href="http://hadoop-host-slave-3" target="_blank" rel="noopener">http://hadoop-host-slave-3</a><br><a href="http://harbor.hewentian.com" target="_blank" rel="noopener">http://harbor.hewentian.com</a></p>
<p>在访问之前，我们需要在本地机器中配置一下hosts，添加如下两行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ more /etc/hosts</span><br><span class="line"></span><br><span class="line">192.168.56.113	hadoop-host-slave-3</span><br><span class="line">192.168.56.113	harbor.hewentian.com</span><br></pre></td></tr></table></figure></p>
<p>为方便docker将镜像上传到私有harbor，这里多配置一个域名。所以访问上面所列的两个站点，结果是一样的。后面的操作，我们使用<code>harbor.hewentian.com</code>这个域名。</p>
<p><img src="/img/harbor-admin-login.png" alt="" title="harbor 管理员登录"></p>
<p>输入管理员的初始用户名/密码：<code>admin/Harbor12345</code>，登录之后，在页面上可以修改密码。登录之后，如下图：</p>
<p><img src="/img/harbor-logined.png" alt="" title="harbor logined"></p>
<p>我们创建一个用户，用户名/密码：<code>hewentian/Harbor12345</code>，用于上传下载镜像：</p>
<p><img src="/img/harbor-new-user.png" alt="" title="harbor 创建用户"></p>
<p>然后我们退出管理员帐号，用新创建的用户登录：</p>
<p><img src="/img/harbor-user-login.png" alt="" title="harbor 普通用户登录"></p>
<p>创建一个project，名为hp，可见性为public：</p>
<p><img src="/img/harbor-create-project.png" alt="" title="harbor 普通用户登录"></p>
<p>docker登录到harbor<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker login harbor.hewentian.com -u hewentian</span><br><span class="line">Password: </span><br><span class="line">Error response from daemon: Get https://harbor.hewentian.com/v2/: dial tcp 192.168.56.113:443: connect: connection refused</span><br></pre></td></tr></table></figure></p>
<p>有可能会报上面的错误，原因是docker与registry交互默认使用的是HTTPS，但是我们搭建的harbor默认使用的是HTTP服务。解决方法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/docker/daemon.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"insecure-registries"</span>: [<span class="string">"harbor.hewentian.com"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>文件<code>/etc/docker/daemon.json</code>原先可能并不存在，它所有可能的配置，可以参考这里：<br><a href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file</a></p>
<p>重启docker，并尝试登录到harbor<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service docker restart</span><br><span class="line"></span><br><span class="line">$ sudo docker login harbor.hewentian.com -u hewentian</span><br><span class="line">Password: </span><br><span class="line">WARNING! Your password will be stored unencrypted <span class="keyword">in</span> /home/hewentian/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/<span class="comment">#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure></p>
<h3 id="将本地镜像上传到私有镜像仓库harbor"><a href="#将本地镜像上传到私有镜像仓库harbor" class="headerlink" title="将本地镜像上传到私有镜像仓库harbor"></a>将本地镜像上传到私有镜像仓库harbor</h3><p>上传到私库的命令，和上传到官方仓库的命令差不多，命令如下：</p>
<pre><code>docker push reg.yourdomain.com/myproject/myrepo:mytag
</code></pre><p>先查看本地的所有镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu    v2.1                3712fd008024        8 days ago          64.2MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        10 days ago         91MB</span><br><span class="line">nginx               latest              e445ab08b2be        2 weeks ago         126MB</span><br><span class="line">ubuntu              18.04               3556258649b2        2 weeks ago         64.2MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p>例如我们要将<code>ubuntu:18.04</code>上传到harbor<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker push harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line">[sudo] password <span class="keyword">for</span> hewentian: </span><br><span class="line">The push refers to repository [harbor.hewentian.com/hp/ubuntu]</span><br><span class="line">An image does not exist locally with the tag: harbor.hewentian.com/hp/ubuntu</span><br></pre></td></tr></table></figure></p>
<p>可以看到，不能直接上传，要先为待上传的镜像打tag<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker tag ubuntu:18.04 harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu                 v2.1                3712fd008024        8 days ago          64.2MB</span><br><span class="line">hewentian/ubuntu                 v2                  2bdf86d10fbc        10 days ago         91MB</span><br><span class="line">nginx                            latest              e445ab08b2be        2 weeks ago         126MB</span><br><span class="line">harbor.hewentian.com/hp/ubuntu   18.04               3556258649b2        2 weeks ago         64.2MB</span><br><span class="line">ubuntu                           18.04               3556258649b2        2 weeks ago         64.2MB</span><br><span class="line">hello-world                      latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp                  latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p>从上面可以看到，打tag后的镜像只是原镜像的一个引用，它们的<code>IMAGE ID</code>是一样的。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker push harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line">The push refers to repository [harbor.hewentian.com/hp/ubuntu]</span><br><span class="line">b079b3fa8d1b: Pushed </span><br><span class="line">a31dbd3063d7: Pushed </span><br><span class="line">c56e09e1bd18: Pushed </span><br><span class="line">543791078bdb: Pushed </span><br><span class="line">18.04: digest: sha256:d91842ef309155b85a9e5c59566719308fab816b40d376809c39cf1cf4de3c6a size: 1152</span><br></pre></td></tr></table></figure></p>
<p>上传成功。同样在浏览器上面，也可以看到，已经成功上传了。</p>
<p><img src="/img/harbor-push-image-1.png" alt="" title="harbor 查看镜像"></p>
<p><img src="/img/harbor-push-image-2.png" alt="" title="harbor 查看镜像"></p>
<h3 id="从harbor下载镜像"><a href="#从harbor下载镜像" class="headerlink" title="从harbor下载镜像"></a>从harbor下载镜像</h3><p>先将本地的镜像删掉<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker rmi harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line">Untagged: harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line">Untagged: harbor.hewentian.com/hp/ubuntu@sha256:d91842ef309155b85a9e5c59566719308fab816b40d376809c39cf1cf4de3c6a</span><br><span class="line"></span><br><span class="line">$ sudo docker rmi ubuntu:18.04</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu    v2.1                3712fd008024        8 days ago          64.2MB</span><br><span class="line">hewentian/ubuntu    v2                  2bdf86d10fbc        10 days ago         91MB</span><br><span class="line">nginx               latest              e445ab08b2be        2 weeks ago         126MB</span><br><span class="line">hello-world         latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp     latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<p>然后从harbor下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker pull harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line">18.04: Pulling from hp/ubuntu</span><br><span class="line">7413c47ba209: Pull complete </span><br><span class="line">0fe7e7cbb2e8: Pull complete </span><br><span class="line">1d425c982345: Pull complete </span><br><span class="line">344da5c95cec: Pull complete </span><br><span class="line">Digest: sha256:d91842ef309155b85a9e5c59566719308fab816b40d376809c39cf1cf4de3c6a</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line">harbor.hewentian.com/hp/ubuntu:18.04</span><br><span class="line"></span><br><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hewentian/ubuntu                 v2.1                3712fd008024        8 days ago          64.2MB</span><br><span class="line">hewentian/ubuntu                 v2                  2bdf86d10fbc        10 days ago         91MB</span><br><span class="line">nginx                            latest              e445ab08b2be        2 weeks ago         126MB</span><br><span class="line">harbor.hewentian.com/hp/ubuntu   18.04               3556258649b2        2 weeks ago         64.2MB</span><br><span class="line">hello-world                      latest              fce289e99eb9        7 months ago        1.84kB</span><br><span class="line">training/webapp                  latest              6fae60ef3446        4 years ago         349MB</span><br></pre></td></tr></table></figure></p>
<h3 id="查看harbor进程状态"><a href="#查看harbor进程状态" class="headerlink" title="查看harbor进程状态"></a>查看harbor进程状态</h3><p>harbor的日志默认存放在<code>/var/log/harbor</code>，如果有下列哪个服务不是<code>Up</code>状态，可以查看相关日志<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker-compose ps</span><br><span class="line">[sudo] password <span class="keyword">for</span> hadoop: </span><br><span class="line">      Name                     Command                  State                 Ports          </span><br><span class="line">---------------------------------------------------------------------------------------------</span><br><span class="line">harbor-core         /harbor/start.sh                 Up (healthy)                            </span><br><span class="line">harbor-db           /entrypoint.sh postgres          Up (healthy)   5432/tcp                 </span><br><span class="line">harbor-jobservice   /harbor/start.sh                 Up                                      </span><br><span class="line">harbor-log          /bin/sh -c /usr/<span class="built_in">local</span>/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcp</span><br><span class="line">harbor-portal       nginx -g daemon off;             Up (healthy)   80/tcp                   </span><br><span class="line">nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80-&gt;80/tcp       </span><br><span class="line">redis               docker-entrypoint.sh redis ...   Up             6379/tcp                 </span><br><span class="line">registry            /entrypoint.sh /etc/regist ...   Up (healthy)   5000/tcp                 </span><br><span class="line">registryctl         /harbor/start.sh                 Up (healthy)</span><br></pre></td></tr></table></figure></p>
<h3 id="harbor生命周期管理"><a href="#harbor生命周期管理" class="headerlink" title="harbor生命周期管理"></a>harbor生命周期管理</h3><p>可以使用<code>docker-compose</code>命令来启动、停止harbor<br>停止harbor：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker-compose stop</span><br><span class="line"></span><br><span class="line">Stopping nginx             ... <span class="keyword">done</span></span><br><span class="line">Stopping harbor-jobservice ... <span class="keyword">done</span></span><br><span class="line">Stopping harbor-portal     ... <span class="keyword">done</span></span><br><span class="line">Stopping harbor-core       ... <span class="keyword">done</span></span><br><span class="line">Stopping registry          ... <span class="keyword">done</span></span><br><span class="line">Stopping harbor-db         ... <span class="keyword">done</span></span><br><span class="line">Stopping registryctl       ... <span class="keyword">done</span></span><br><span class="line">Stopping redis             ... <span class="keyword">done</span></span><br><span class="line">Stopping harbor-log        ... <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>启动harbor：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker-compose start</span><br><span class="line"></span><br><span class="line">Starting <span class="built_in">log</span>         ... <span class="keyword">done</span></span><br><span class="line">Starting registry    ... <span class="keyword">done</span></span><br><span class="line">Starting registryctl ... <span class="keyword">done</span></span><br><span class="line">Starting postgresql  ... <span class="keyword">done</span></span><br><span class="line">Starting core        ... <span class="keyword">done</span></span><br><span class="line">Starting portal      ... <span class="keyword">done</span></span><br><span class="line">Starting redis       ... <span class="keyword">done</span></span><br><span class="line">Starting jobservice  ... <span class="keyword">done</span></span><br><span class="line">Starting proxy       ... <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>参考文献：<br><a href="https://docs.docker.com/" target="_blank" rel="noopener">https://docs.docker.com/</a><br><a href="https://blog.docker.com/" target="_blank" rel="noopener">https://blog.docker.com/</a><br><a href="https://github.com/goharbor/harbor">https://github.com/goharbor/harbor</a><br><a href="https://www.runoob.com/docker/docker-image-usage.html" target="_blank" rel="noopener">https://www.runoob.com/docker/docker-image-usage.html</a></p>
<p>未完待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/07/26/docker-note/" data-id="cjz3s88q9000dqc3k2aemjust" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/docker/">docker</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-nginx-note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/10/nginx-note/" class="article-date">
  <time datetime="2019-06-10T11:20:16.000Z" itemprop="datePublished">2019-06-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/web-server/">web server</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/10/nginx-note/">nginx 学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天一看，天哪，原来已经有4个月没有更新博客了，我在想，这4个月我都干嘛去了，内心立马慌起来了（你知道的，程序员是不能停止学习的）。但是细想，虽然没更新博客，但还是看了几本书：《从0到1》、《毛泽东选集》卷一、《MongoDB in Action》、《SpringBoot in Action》、《白帽子讲Web安全》，内心立马淡定了不少。好了，题外话不多说了，马上进入主题。</p>
<p><code>nginx</code>是一个<code>HTTP</code>和反向代理服务器、邮件代理服务器和通用的<code>TCP/UDP</code>代理服务器，最初由俄罗斯程序员<code>Igor Sysoev</code>所开发。</p>
<p>详细介绍可以参见：<br><a href="http://nginx.org/en/" target="_blank" rel="noopener">http://nginx.org/en/</a><br><a href="http://nginx.org/en/docs/" target="_blank" rel="noopener">http://nginx.org/en/docs/</a></p>
<p>本文将说下<code>nginx</code>的简单安装使用，因为在项目中要使用到的。我在虚拟机VirtualBox中的机器中安装，系统为<code>Ubuntu Linux</code>，机器节点相关配置如下（之前安装过Hadoop集群的机器）：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
</code></pre><p>首先，我们要将<code>nginx</code>的安装包下载回来，截止本文写时，它的最新稳定版本为<code>1.16.0</code>，可以在它的<a href="http://nginx.org/download/nginx-1.16.0.tar.gz" target="_blank" rel="noopener">官网</a>下载。我先在我的物理机器下载回来。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/Downloads/</span><br><span class="line">$ wget http://nginx.org/download/nginx-1.16.0.tar.gz</span><br><span class="line">$ wget http://nginx.org/download/nginx-1.16.0.tar.gz.asc</span><br><span class="line"></span><br><span class="line">验证下载文件的完整性，这里略</span><br></pre></td></tr></table></figure>
<p>在物理机上将它传到要安装的机器<code>hadoop-host-master</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scp nginx-1.16.0.tar.gz hadoop@hadoop-host-master:~/</span><br></pre></td></tr></table></figure></p>
<p>接下来，我们进入<code>hadoop-host-master</code>中操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br><span class="line"></span><br><span class="line">$ tar xf nginx-1.16.0.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> nginx-1.16.0/</span><br><span class="line">$ ls</span><br><span class="line">auto  CHANGES  CHANGES.ru  conf  configure  contrib  html  LICENSE  man  README  src</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ sudo ./configure </span><br><span class="line">checking <span class="keyword">for</span> OS</span><br><span class="line"> + Linux 4.15.0-39-generic x86_64</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">中间省略部分</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Configuration summary</span><br><span class="line">  + using system PCRE library</span><br><span class="line">  + OpenSSL library is not used</span><br><span class="line">  + using system zlib library</span><br><span class="line"></span><br><span class="line">  nginx path prefix: <span class="string">"/usr/local/nginx"</span></span><br><span class="line">  nginx binary file: <span class="string">"/usr/local/nginx/sbin/nginx"</span></span><br><span class="line">  nginx modules path: <span class="string">"/usr/local/nginx/modules"</span></span><br><span class="line">  nginx configuration prefix: <span class="string">"/usr/local/nginx/conf"</span></span><br><span class="line">  nginx configuration file: <span class="string">"/usr/local/nginx/conf/nginx.conf"</span></span><br><span class="line">  nginx pid file: <span class="string">"/usr/local/nginx/logs/nginx.pid"</span></span><br><span class="line">  nginx error <span class="built_in">log</span> file: <span class="string">"/usr/local/nginx/logs/error.log"</span></span><br><span class="line">  nginx http access <span class="built_in">log</span> file: <span class="string">"/usr/local/nginx/logs/access.log"</span></span><br><span class="line">  nginx http client request body temporary files: <span class="string">"client_body_temp"</span></span><br><span class="line">  nginx http proxy temporary files: <span class="string">"proxy_temp"</span></span><br><span class="line">  nginx http fastcgi temporary files: <span class="string">"fastcgi_temp"</span></span><br><span class="line">  nginx http uwsgi temporary files: <span class="string">"uwsgi_temp"</span></span><br><span class="line">  nginx http scgi temporary files: <span class="string">"scgi_temp"</span></span><br></pre></td></tr></table></figure></p>
<p>在<code>./configure</code>过程中会检查依赖的缺失情况，如果有缺失，则会在这里提示。我们根据提示安装即可。</p>
<p>一般来说，nginx编译会依赖：zlib、zlib-devel、openssl、openssl-devel、pcre、pcre-devel、gcc、g++</p>
<p>在CentOS上可以通过以下方式进行安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel</span><br></pre></td></tr></table></figure></p>
<p>而在Ubuntu上面则可以通过下面的方式进行安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install build-essential    这会同时安装 gcc、g++</span><br><span class="line">$ sudo apt-get install zlib1g</span><br><span class="line">$ sudo apt-get install openssl libssl-dev</span><br><span class="line">$ sudo apt-get install libpcre3 libpcre3-dev</span><br><span class="line">$ sudo apt-get install zlib1g-dev</span><br></pre></td></tr></table></figure></p>
<p>接下来进行编译安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">objs/ngx_modules.o \</span><br><span class="line">-ldl -lpthread -lcrypt -lpcre -lz \</span><br><span class="line">-Wl,-E</span><br><span class="line">sed -e <span class="string">"s|%%PREFIX%%|/usr/local/nginx|"</span> \</span><br><span class="line">	-e <span class="string">"s|%%PID_PATH%%|/usr/local/nginx/logs/nginx.pid|"</span> \</span><br><span class="line">	-e <span class="string">"s|%%CONF_PATH%%|/usr/local/nginx/conf/nginx.conf|"</span> \</span><br><span class="line">	-e <span class="string">"s|%%ERROR_LOG_PATH%%|/usr/local/nginx/logs/error.log|"</span> \</span><br><span class="line">	&lt; man/nginx.8 &gt; objs/nginx.8</span><br><span class="line">make[1]: Leaving directory <span class="string">'/home/hadoop/nginx-1.16.0'</span></span><br></pre></td></tr></table></figure></p>
<p>它默认会安装到<code>/usr/local/nginx/</code>目录下。</p>
<p>启动命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /usr/<span class="built_in">local</span>/nginx/sbin/nginx -c /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br></pre></td></tr></table></figure></p>
<p><strong>注意：使用<code>-c</code>参数指定配置文件一定要使用绝对路径，否则可能会报错。</strong></p>
<p>修改配置文件后，检查配置文件是否正确的命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /usr/<span class="built_in">local</span>/nginx/sbin/nginx -t -c /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line">nginx: the configuration file /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf syntax is ok</span><br><span class="line">nginx: configuration file /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf <span class="built_in">test</span> is successful</span><br></pre></td></tr></table></figure></p>
<p>可以打开 <a href="http://192.168.56.110/" target="_blank" rel="noopener">http://192.168.56.110/</a> 来看看是否已启动。</p>
<p><img src="/img/nginx-1.png" alt="" title="nginx 初始启动界面"></p>
<p>另外，使用 <a href="http://hadoop-host-master/" target="_blank" rel="noopener">http://hadoop-host-master/</a> 也是一样的。</p>
<p><img src="/img/nginx-2.png" alt="" title="nginx 初始启动界面"></p>
<p>当你看到上面的界面后，nginx的安装到这里就成功了。</p>
<h3 id="一些常用操作命令"><a href="#一些常用操作命令" class="headerlink" title="一些常用操作命令"></a>一些常用操作命令</h3><p>进入nginx的安装目录，这里进入默认安装目录，然后查看帮助信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/nginx/sbin</span><br><span class="line">$ ./nginx -h</span><br><span class="line"></span><br><span class="line">nginx version: nginx/1.16.0</span><br><span class="line">Usage: nginx [-?hvVtTq] [-s signal] [-c filename] [-p prefix] [-g directives]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -?,-h         : this <span class="built_in">help</span></span><br><span class="line">  -v            : show version and <span class="built_in">exit</span></span><br><span class="line">  -V            : show version and configure options <span class="keyword">then</span> <span class="built_in">exit</span></span><br><span class="line">  -t            : <span class="built_in">test</span> configuration and <span class="built_in">exit</span></span><br><span class="line">  -T            : <span class="built_in">test</span> configuration, dump it and <span class="built_in">exit</span></span><br><span class="line">  -q            : suppress non-error messages during configuration testing</span><br><span class="line">  -s signal     : send signal to a master process: stop, quit, reopen, reload</span><br><span class="line">  -p prefix     : <span class="built_in">set</span> prefix path (default: /usr/<span class="built_in">local</span>/nginx/)</span><br><span class="line">  -c filename   : <span class="built_in">set</span> configuration file (default: conf/nginx.conf)</span><br><span class="line">  -g directives : <span class="built_in">set</span> global directives out of configuration file</span><br></pre></td></tr></table></figure></p>
<p>说明如下：</p>
<ol>
<li>启动命令： <code>./nginx</code>；</li>
<li>关闭命令： <code>./nginx -s stop</code>，快速停止nginx，可能并不保存相关信息；</li>
<li>退出命令： <code>./nginx -s quit</code>，完整有序的停止nginx，会保存相关信息，建议使用此命令；</li>
<li>动态加载配置文件： <code>./nginx -s reload</code>可以不关闭nginx的情况下更新配置文件；</li>
<li>重新打开日志文件：<code>./nginx -s reopen</code>；</li>
<li>查看Nginx版本： <code>./nginx -v</code>；</li>
<li>检查配置文件是否正确： <code>./nginx -t</code>或者检查指定配置文件<code>./nginx -t -c /usr/local/nginx/conf/nginx.conf</code>。</li>
</ol>
<p>接下来，我们作一些简单的配置示例。但是在开始之前，在执行访问的机器上面（在这里是我的物理机器），配置一下HOST，<strong>增加</strong>下面这3行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">192.168.56.110 www.hewentian.com</span><br><span class="line">192.168.56.110 admin.hewentian.com</span><br><span class="line">192.168.56.110 img.hewentian.com</span><br><span class="line">192.168.56.110 api.hewentian.com</span><br><span class="line">192.168.56.110 so.hewentian.com</span><br></pre></td></tr></table></figure></p>
<h3 id="示例一：将某目录下的图片，让其他机器可以通过WEB访问"><a href="#示例一：将某目录下的图片，让其他机器可以通过WEB访问" class="headerlink" title="示例一：将某目录下的图片，让其他机器可以通过WEB访问"></a>示例一：将某目录下的图片，让其他机器可以通过WEB访问</h3><p>在安装了nginx的机器<code>hadoop-host-master</code>上有个目录<code>/home/hadoop/Pictures</code>，这个目录下放有图片（我的机器里有一张：my_computer.png）。现在想通过web，让其他机器的用户访问这个目录下的图片。</p>
<p>修改<code>nginx.conf</code>配置，添加如下代码即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  img.hewentian.com;                 <span class="comment"># 修改 1/3： WEB访问的位置</span></span><br><span class="line">        access_log  logs/img.access.log  main;          <span class="comment"># 修改 2/3： 日志存放位置。记得将此配置文件中的 log_format  main 前面的注释打开</span></span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line">            root   /home/hadoop/Pictures/;              <span class="comment"># 修改 3/3： 图片存放位置</span></span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>保存文件后，退出。<br>检查配置文件是否正确：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /usr/<span class="built_in">local</span>/nginx/sbin/nginx -t -c /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br></pre></td></tr></table></figure></p>
<p>然后重启nginx，首先找出nginx进程号：</p>
<pre><code>ps -ef | grep nginx
</code></pre><p>然后杀死nginx的主进程：</p>
<pre><code>sudo kill -9 [nginx进程号]
</code></pre><p>重启nginx</p>
<pre><code>sudo /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
</code></pre><p>在物理机器上面打开浏览器，试着访问： <a href="http://img.hewentian.com/my_computer.png" target="_blank" rel="noopener">http://img.hewentian.com/my_computer.png</a></p>
<p><img src="/img/nginx-3.png" alt="" title="nginx 作为图片服务器"></p>
<p>查看nginx访问日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ tail /usr/<span class="built_in">local</span>/nginx/logs/img.access.log</span><br><span class="line"></span><br><span class="line">192.168.56.1 - - [11/Jun/2019:22:46:33 +0800] <span class="string">"GET / HTTP/1.1"</span> 403 555 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [11/Jun/2019:22:46:50 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 200 59332 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [11/Jun/2019:22:47:21 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:00:51:04 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:00:51:04 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:00:51:04 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:00:51:05 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:01:02:06 +0800] <span class="string">"GET /my_computer.png HTTP/1.1"</span> 304 0 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="示例二：实现简单的负载匀衡"><a href="#示例二：实现简单的负载匀衡" class="headerlink" title="示例二：实现简单的负载匀衡"></a>示例二：实现简单的负载匀衡</h3><p>有个应用，它有个接口<code>/hello</code>，是返回当前服务器的IP地址。我是使用SpringBoot来简单开发的，代码只有几行：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.web.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.InetAddress;</span><br><span class="line"><span class="keyword">import</span> java.net.UnknownHostException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;HelloController&lt;/b&gt; 是 返回当前服务器IP地址的Controller</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019-06-12 14:52:49</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloController</span> </span>&#123;</span><br><span class="line">     <span class="keyword">private</span> <span class="keyword">static</span> Logger log = Logger.getLogger(HelloController.class);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/hello"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">index</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String address = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            InetAddress inetAddress = InetAddress.getLocalHost();</span><br><span class="line">            address = inetAddress.getHostAddress();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnknownHostException e) &#123;</span><br><span class="line">            log.error(e.getMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> address + <span class="string">" is serving for you: Hello World"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>部署在下面的2台服务器上，都是监听8080端口。</p>
<pre><code>slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
</code></pre><p>现在想在网页上访问：<a href="http://192.168.56.110" target="_blank" rel="noopener">http://192.168.56.110</a> 或 <a href="http://api.hewentian.com" target="_blank" rel="noopener">http://api.hewentian.com</a> 的时候，就会访问到上面这2台机器，实现负载均衡。</p>
<p>修改<code>nginx.conf</code>配置，添加如下代码即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line">upstream api_worker &#123;</span><br><span class="line">    server 192.168.56.111:8080 weight=3000;</span><br><span class="line">    server 192.168.56.112:8080 weight=3000;</span><br><span class="line">    keepalive 2000;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen      80;</span><br><span class="line">    server_name 192.168.56.110  api.hewentian.com;</span><br><span class="line">    access_log  logs/api.access.log  main;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass http://api_worker/;</span><br><span class="line">        proxy_redirect off;</span><br><span class="line">        proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">        proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        proxy_http_version 1.1;</span><br><span class="line">        proxy_set_header Connection <span class="string">""</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>重启nginx。正常情况下，流量会匀衡地分到2台服务器，如下图所示。<br><img src="/img/nginx-4.png" alt="" title="nginx 作为负载匀衡服务器"><br><img src="/img/nginx-5.png" alt="" title="nginx 作为负载匀衡服务器"></p>
<p>如果两台机器中的某一台挂掉了，流量会自动分到另外一台，这样也实现了简单的高可用。</p>
<p>查看nginx访问日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ tail /usr/<span class="built_in">local</span>/nginx/logs/api.access.log </span><br><span class="line"></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /hello HTTP/1.1"</span> 200 46 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 200 946 <span class="string">"http://api.hewentian.com/hello"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /hello HTTP/1.1"</span> 200 46 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 200 946 <span class="string">"http://api.hewentian.com/hello"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /hello HTTP/1.1"</span> 200 46 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 200 946 <span class="string">"http://api.hewentian.com/hello"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /hello HTTP/1.1"</span> 200 46 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 200 946 <span class="string">"http://api.hewentian.com/hello"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /hello HTTP/1.1"</span> 200 46 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line">192.168.56.1 - - [12/Jun/2019:02:46:52 +0800] <span class="string">"GET /favicon.ico HTTP/1.1"</span> 200 946 <span class="string">"http://api.hewentian.com/hello"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="示例三：实现同一个域名下的两个服务"><a href="#示例三：实现同一个域名下的两个服务" class="headerlink" title="示例三：实现同一个域名下的两个服务"></a>示例三：实现同一个域名下的两个服务</h3><p>有两个服务：<br>一个是一个WEB服务，是指向另一台机器的，其中<code>/hello</code>是一个接口：<br><a href="http://www.hewentian.com/api/hello" target="_blank" rel="noopener">http://www.hewentian.com/api/hello</a></p>
<p>另一个是用来看某个目录下的图片的：<br><a href="http://www.hewentian.com/img/my_computer.png" target="_blank" rel="noopener">http://www.hewentian.com/img/my_computer.png</a></p>
<p>修改<code>nginx.conf</code>配置，添加如下代码即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line">upstream api_worker &#123;</span><br><span class="line">    server 192.168.56.111:8080 weight=3000;</span><br><span class="line">    keepalive 2000;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen      80;</span><br><span class="line">    server_name www.hewentian.com;</span><br><span class="line">  </span><br><span class="line">    location / &#123;</span><br><span class="line">        root   html;</span><br><span class="line">        index  index.html;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    location /api/ &#123;</span><br><span class="line">        proxy_pass http://api_worker/;</span><br><span class="line">        <span class="comment">#proxy_redirect off;</span></span><br><span class="line">        proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">        proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        <span class="comment">#proxy_http_version 1.1;</span></span><br><span class="line">        <span class="comment">#proxy_set_header Connection "";</span></span><br><span class="line">        access_log  logs/api.access.log  main;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /img/ &#123;</span><br><span class="line">        <span class="built_in">alias</span> /home/hadoop/Pictures/;</span><br><span class="line">        access_log  logs/img.access.log  main;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>重启nginx。访问上述地址，将得到如下图效果：</p>
<p><img src="/img/nginx-6.png" alt="" title="nginx 实现同一个域名下的两个服务"></p>
<p><img src="/img/nginx-7.png" alt="" title="nginx 实现同一个域名下的两个服务"></p>
<p>查看nginx访问日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tail -n1 /usr/<span class="built_in">local</span>/nginx/logs/api.access.log /usr/<span class="built_in">local</span>/nginx/logs/img.access.log </span><br><span class="line"></span><br><span class="line">==&gt; /usr/<span class="built_in">local</span>/nginx/logs/api.access.log &lt;==</span><br><span class="line">192.168.56.1 - - [12/Jun/2019:04:09:55 +0800] <span class="string">"GET /api/hello HTTP/1.1"</span> 200 46 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br><span class="line"></span><br><span class="line">==&gt; /usr/<span class="built_in">local</span>/nginx/logs/img.access.log &lt;==</span><br><span class="line">192.168.56.1 - - [12/Jun/2019:04:08:04 +0800] <span class="string">"GET /img/my_computer.png HTTP/1.1"</span> 200 59332 <span class="string">"-"</span> <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span> <span class="string">"-"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="示例四：静态HTML项目、WEB接口项目并存和Tomcat项目"><a href="#示例四：静态HTML项目、WEB接口项目并存和Tomcat项目" class="headerlink" title="示例四：静态HTML项目、WEB接口项目并存和Tomcat项目"></a>示例四：静态HTML项目、WEB接口项目并存和Tomcat项目</h3><p>有三个服务：</p>
<ol>
<li><p>第一个是一个WEB服务，是指向另一台机器的，其中<code>/hello</code>是一个接口，另外此WEB项目包含静态HTML代码：<br><a href="http://www.hewentian.com/index.html" target="_blank" rel="noopener">http://www.hewentian.com/index.html</a>  <strong>最后的 /index.html不能少</strong><br><a href="http://www.hewentian.com/hello" target="_blank" rel="noopener">http://www.hewentian.com/hello</a></p>
</li>
<li><p>第二个也是一个WEB服务，是指向另一台机器的，其中<code>/hello</code>是一个接口，另外此WEB项目包含静态HTML代码：<br><a href="http://admin.hewentian.com/index.html" target="_blank" rel="noopener">http://admin.hewentian.com/index.html</a>  <strong>最后的 /index.html不能少</strong><br><a href="http://admin.hewentian.com/hello" target="_blank" rel="noopener">http://admin.hewentian.com/hello</a></p>
</li>
<li><p>第三个是Tomcat中的一个WEB项目：<br><a href="http://so.hewentian.com" target="_blank" rel="noopener">http://so.hewentian.com</a></p>
</li>
</ol>
<p>修改<code>nginx.conf</code>配置，添加如下代码即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name  www.hewentian.com;</span><br><span class="line">    access_log  logs/www.access.log  main;</span><br><span class="line"></span><br><span class="line">    location ~* (.html|.js|.css|.png|.jpg|.gif|.ico|.woff|.ttf|.woff2)$ &#123;</span><br><span class="line">        root	/home/hadoop/www-hewentian;</span><br><span class="line">        index	main.html index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass   http://192.168.56.111:8080;</span><br><span class="line">        proxy_redirect off;</span><br><span class="line">        proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">        proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  admin.hewentian.com;</span><br><span class="line">    access_log  logs/admin.access.log  main;</span><br><span class="line"></span><br><span class="line">    location ~* (.html|.js|.css|.png|.jpg|.gif|.ico|.woff|.ttf|.woff2)$ &#123;</span><br><span class="line">        root 	/home/hadoop/admin-hewentian;</span><br><span class="line">        index  	main.html index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass   http://192.168.56.112:8080;</span><br><span class="line">        proxy_redirect off;</span><br><span class="line">        proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">        proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name  so.hewentian.com;</span><br><span class="line">    access_log  logs/so.access.log  main;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass   http://192.168.56.111:8082/so/; <span class="comment"># 注意：最后的/不能少，具体位置： /home/hadoop/apache-tomcat-8.0.47/webapps/so</span></span><br><span class="line">        proxy_redirect off;</span><br><span class="line">        proxy_set_header Host <span class="variable">$host</span>;</span><br><span class="line">        proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="示例五：同一个Tomcat下的两个项目"><a href="#示例五：同一个Tomcat下的两个项目" class="headerlink" title="示例五：同一个Tomcat下的两个项目"></a>示例五：同一个Tomcat下的两个项目</h3><p>有三个服务：</p>
<ol>
<li><p>第一个是一个WEB服务：<br><a href="http://www.hewentian.com/so/" target="_blank" rel="noopener">http://www.hewentian.com/so/</a></p>
</li>
<li><p>第二个也是一个WEB服务，和第一个项目在同一个Tomcat中：<br><a href="http://www.hewentian.com/smswzl/" target="_blank" rel="noopener">http://www.hewentian.com/smswzl/</a></p>
</li>
<li><p>第三个是统计nginx状态的功能，需要安装<code>./configure --with-http_stub_status_module</code>模块，并重新编译安装nginx：<br><a href="http://www.hewentian.com/nginxstatus" target="_blank" rel="noopener">http://www.hewentian.com/nginxstatus</a></p>
</li>
</ol>
<p>修改<code>nginx.conf</code>配置，添加如下代码即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /usr/<span class="built_in">local</span>/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line">gzip  on;</span><br><span class="line">gzip_min_length  1k;</span><br><span class="line">gzip_buffers     4 16k;</span><br><span class="line">gzip_http_version 1.0;</span><br><span class="line">gzip_comp_level 2;</span><br><span class="line">gzip_types       text/plain application/x-javascript text/css application/xml;</span><br><span class="line">gzip_vary on;</span><br><span class="line"></span><br><span class="line">upstream tomcatServer &#123;</span><br><span class="line">    server 192.168.56.111:8082;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  www.hewentian.com;</span><br><span class="line">    charset utf-8;</span><br><span class="line"></span><br><span class="line">    root	/home/hadoop/www-hewentian;</span><br><span class="line">    index	main.html index.html index.htm;</span><br><span class="line"></span><br><span class="line">    access_log logs/www.access.log  combined;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#expires</span></span><br><span class="line">    location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$ &#123;</span><br><span class="line">        expires 30d;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ .*\.(js|css)?$ &#123;</span><br><span class="line">        expires 24h;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /nginxstatus &#123;</span><br><span class="line">        stub_status on;</span><br><span class="line">        access_log off;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /so &#123;</span><br><span class="line">        index index.html;</span><br><span class="line">        proxy_pass http://tomcatServer/so;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /smswzl &#123;</span><br><span class="line">        index index.html;</span><br><span class="line">        proxy_pass http://tomcatServer/smswzl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/06/10/nginx-note/" data-id="cjz3s88r3002rqc3kf5lwamne" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nginx/">nginx</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hbase-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/20/hbase-cluster/" class="article-date">
  <time datetime="2019-01-20T04:31:29.000Z" itemprop="datePublished">2019-01-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/20/hbase-cluster/">hbase 集群的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Hbase-介绍"><a href="#Hbase-介绍" class="headerlink" title="Hbase 介绍"></a>Hbase 介绍</h3><p>Hbase的<a href="http://hbase.apache.org/book.html" target="_blank" rel="noopener">官方文档</a>中有对Hbase的详细介绍，这里不再赘述。这里用一句话描述如下：</p>
<p>Apache HBase™ is the Hadoop database, a distributed, scalable, big data store.</p>
<p>Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project’s goal is the hosting of very large tables – billions of rows X millions of columns – atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google’s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.</p>
<h3 id="Hbase-的安装"><a href="#Hbase-的安装" class="headerlink" title="Hbase 的安装"></a>Hbase 的安装</h3><p>安装过程参考这里：<br><a href="http://hbase.apache.org/book.html#quickstart_fully_distributed" target="_blank" rel="noopener">http://hbase.apache.org/book.html#quickstart_fully_distributed</a></p>
<p>Hbase依赖于HADOOP，我们在上一篇<a href="../../../../2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>的基础上安装Hbase。</p>
<p>节点分布如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
slave3:
    ip: 192.168.56.113
    hostname: hadoop-host-slave-3
</code></pre><p>如下图（绿色代表在这些节点上面安装这些程序，与<code>hadoop 集群的搭建HA</code>安装中的图类似，这里多了后面两列）：</p>
<p><img src="/img/hbase-1.png" alt="" title="HBase集群 节点分布"></p>
<h3 id="安装NTP"><a href="#安装NTP" class="headerlink" title="安装NTP"></a>安装NTP</h3><p>可能还要在各节点服务器上面安装NTP服务，实现服务器节点间时间的一致。如果服务器节点间的时间不一致，可能会引发HBase的异常，这一点在HBase官网上有特别强调。在这里，设置我的笔记本电脑为NTP的服务端节点，即是我的电脑从国家授时中心同步时间，然后其它节点（master、slave1、slave2、slave3）作为客户端从我的笔记本同步时间。此篇的安装过程将省略这个步骤，在后续篇章中再介绍，本篇将手动将各节点的时间调成一致。</p>
<h3 id="修改ulimit"><a href="#修改ulimit" class="headerlink" title="修改ulimit"></a>修改ulimit</h3><p>Configuring the maximum number of file descriptors and processes for the user who is running the HBase process is an operating system configuration, rather than an HBase configuration. It is also important to be sure that the settings are changed for the user that actually runs HBase. To see which user started HBase, and that user’s ulimit configuration, look at the first line of the HBase log for that instance.</p>
<p>修改ulimit，以增加linux系统能同时打开文件的数量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line">hadoop  -       nofile  32768</span><br><span class="line">hadoop  -       nproc   32000</span><br></pre></td></tr></table></figure>
<p>修改后需重启系统才能生效。</p>
<h3 id="下载Hbase"><a href="#下载Hbase" class="headerlink" title="下载Hbase"></a>下载Hbase</h3><p>首先下载Hbase，我们下载的时候，要选择适合我们HADOOP版本的Hbase，我们下载的稳定版为<a href="http://archive.apache.org/dist/hbase/1.2.6/" target="_blank" rel="noopener">hbase-1.2.6-bin.tar.gz</a>，将压缩包首先传到<code>master</code>节点的<code>/home/hadoop/</code>目录下，先在<code>master</code>节点配置好，然后同步到其他3个节点。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xzvf hbase-1.2.6-bin.tar.gz</span><br><span class="line">$ </span><br><span class="line">$ <span class="built_in">cd</span> hbase-1.2.6/</span><br><span class="line">$ ls</span><br><span class="line">bin  CHANGES.txt  conf  docs  hbase-webapps  LEGAL  lib  LICENSE.txt  NOTICE.txt  README.txt</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hbase-env-sh，加上JDK绝对路径"><a href="#配置hbase-env-sh，加上JDK绝对路径" class="headerlink" title="配置hbase-env.sh，加上JDK绝对路径"></a>配置hbase-env.sh，加上JDK绝对路径</h3><ol>
<li>JDK的路径就是安装JDK的时候的路径；</li>
<li>Hbase内置有zookeeper，但是为了方便管理，我们单独部署zookeeper，即使用HADOOP中用作ZKFC的zookeeper；</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi hbase-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_102/</span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hbase-site-xml"><a href="#配置hbase-site-xml" class="headerlink" title="配置hbase-site.xml"></a>配置hbase-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop-host-master:8020/hbase&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Do not create the dir hbase, the system will create it automatically, and the value is dfs.namenode.rpc-address.hadoop-cluster-ha.nn1&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Property from ZooKeeper<span class="string">'s config zoo.cfg. The port at which the clients will connect.&lt;/description&gt;</span></span><br><span class="line"><span class="string">    &lt;/property&gt;</span></span><br><span class="line"><span class="string">    &lt;property&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span></span><br><span class="line"><span class="string">        &lt;value&gt;hadoop-host-master,hadoop-host-slave-1,hadoop-host-slave-2&lt;/value&gt;</span></span><br><span class="line"><span class="string">    &lt;/property&gt;</span></span><br><span class="line"><span class="string">    &lt;property&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span></span><br><span class="line"><span class="string">        &lt;value&gt;/home/hadoop/zookeeper-3.4.6/data&lt;/value&gt;</span></span><br><span class="line"><span class="string">        &lt;description&gt;Property from ZooKeeper'</span>s config zoo.cfg. The directory <span class="built_in">where</span> the snapshot is stored.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h3 id="配置regionservers"><a href="#配置regionservers" class="headerlink" title="配置regionservers"></a>配置regionservers</h3><p>在将要运行regionservers的节点加入此文件中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi regionservers</span><br><span class="line"></span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br><span class="line">hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<h3 id="配置backup-masters"><a href="#配置backup-masters" class="headerlink" title="配置backup-masters"></a>配置backup-masters</h3><p>我们将<code>hadoop-host-master</code>作为Hbase集群的master，并配置HBase使用<code>hadoop-host-slave-1</code>作为backup master，在conf目录下创建一个文件backup-masters, 并在其中添加作为backup master的主机名<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi backup-masters</span><br><span class="line"></span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<h3 id="复制hdfs-site-xml配置文件"><a href="#复制hdfs-site-xml配置文件" class="headerlink" title="复制hdfs-site.xml配置文件"></a>复制hdfs-site.xml配置文件</h3><p>复制<code>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</code>到<code>$HBASE_HOME/conf</code>目录下，这样以保证HDFS与Hbase两边配置一致，这也是官网所推荐的方式。例子，如果HDFS中配置的副本数量为5（默认为3），如果没有将hadoop的<code>hdfs-site.xml</code>复制到<code>$HBASE_HOME/conf</code>目录下，则Hbase将会按3份备份，从而两边不一致，导致出现异常。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf/</span><br><span class="line">$ cp /home/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml .</span><br></pre></td></tr></table></figure>
<p>至此，配置完毕，将这些配置同步到其他三个节点，在<code>hadoop-host-master</code>上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ scp -r hbase-1.2.6 hadoop@hadoop-host-slave-1:/home/hadoop/</span><br><span class="line">$ scp -r hbase-1.2.6 hadoop@hadoop-host-slave-2:/home/hadoop/</span><br><span class="line">$ scp -r hbase-1.2.6 hadoop@hadoop-host-slave-3:/home/hadoop/</span><br></pre></td></tr></table></figure></p>
<h3 id="启动Hbase"><a href="#启动Hbase" class="headerlink" title="启动Hbase"></a>启动Hbase</h3><p>可使用<code>$HBASE_HOME/bin/start-hbase.sh</code>指令启动整个集群，如果要使用该命令，则集群的节点间必须实现ssh的免密码登录，这样才能到不同的节点启动服务。</p>
<p>按我们前面的规划，<code>hadoop-host-master</code>将作为Hbase集群的master，其实在哪台机器上面运行<code>start-hbase.sh</code>指令，那么这台机器将成为master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/bin</span><br><span class="line">$ ./start-hbase.sh</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hbase-2.png" alt="" title="HBase集群启动"></p>
<p>当执行<code>jps</code>指令后，可以看到<code>hadoop-host-master</code>上面多了一个<code>HMaster</code>进程，在<code>hadoop-host-slave-1</code>中会同时存在<code>HMaster</code>、<code>HRegionServer</code>进程，而在其他两个节点则只存在<code>HRegionServer</code>进程。</p>
<p>另外，我们可以在其他任何机器通过以下命令启动一个master<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/bin</span><br><span class="line">$ ./hbase-daemon.sh start master</span><br><span class="line"></span><br><span class="line">或者启动作为backup master</span><br><span class="line">$ ./hbase-daemon.sh start master --backup</span><br></pre></td></tr></table></figure></p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:16010/" target="_blank" rel="noopener">http://hadoop-host-master:16010/</a><br><a href="http://hadoop-host-slave-1:16010/" target="_blank" rel="noopener">http://hadoop-host-slave-1:16010/</a></p>
<p>来查看是否启动成功，如无意外的话，你会看到如下结果页面。其中一个是Master，另一个是Back Master：</p>
<p><img src="/img/hbase-3.png" alt="" title="HBase集群管理界面 Master"></p>
<p><img src="/img/hbase-4.png" alt="" title="HBase集群管理界面 Back Master"></p>
<p>同样的它在HDFS中也自动创建了保存数据的目录：</p>
<p><img src="/img/hbase-5.png" alt="" title="HBase集群在HDFS中的目录"></p>
<p>至此，集群搭建成功。</p>
<h3 id="Hbase初体验"><a href="#Hbase初体验" class="headerlink" title="Hbase初体验"></a>Hbase初体验</h3><p>首先我们通过SHELL的方式简单体验一下Hbase：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/</span><br><span class="line">$ ./bin/hbase shell</span><br><span class="line">2019-01-23 19:16:36,118 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">HBase Shell; enter <span class="string">'help&lt;RETURN&gt;'</span> <span class="keyword">for</span> list of supported commands.</span><br><span class="line">Type <span class="string">"exit&lt;RETURN&gt;"</span> to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.6030 seconds</span><br><span class="line"></span><br><span class="line">=&gt; []</span><br></pre></td></tr></table></figure></p>
<p>由上述结果可知，Hbase中现在没有一张表。我们尝试创建一张表<code>t_student</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create <span class="string">'t_student'</span>, <span class="string">'cf1'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 2.4700 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - t_student</span><br><span class="line">hbase(main):003:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">t_student</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0250 seconds</span><br><span class="line"></span><br><span class="line">=&gt; [<span class="string">"t_student"</span>]</span><br><span class="line">hbase(main):004:0&gt; desc <span class="string">'t_student'</span></span><br><span class="line">Table t_student is ENABLED</span><br><span class="line">t_student</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf1'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, VERSIONS =&gt; <span class="string">'1'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, TTL =&gt; <span class="string">'FOREVER'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, BLO</span><br><span class="line">CKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>&#125;</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.2010 seconds</span><br></pre></td></tr></table></figure></p>
<p>往表中插入2条数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):005:0&gt; put <span class="string">'t_student'</span>, <span class="string">'01'</span>, <span class="string">'cf1:name'</span>, <span class="string">'tim'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.1840 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):006:0&gt; put <span class="string">'t_student'</span>, <span class="string">'02'</span>, <span class="string">'cf1:name'</span>, <span class="string">'timho'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.3630 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):007:0&gt; scan <span class="string">'t_student'</span></span><br><span class="line">ROW                                                  COLUMN+CELL</span><br><span class="line"> 01                                                  column=cf1:name, timestamp=1548242390794, value=tim</span><br><span class="line"> 02                                                  column=cf1:name, timestamp=1548246522887, value=timho</span><br><span class="line">2 row(s) <span class="keyword">in</span> 0.1240 seconds</span><br></pre></td></tr></table></figure></p>
<p>插入数据之后，可能在HDFS中还不能立刻看到，因为数据还在内存中，但可以通过以下命令将数据立刻写到HDFS中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):008:0&gt; flush <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.7290 seconds</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以在HDFS、Hbase的管理界面分别看到表信息：</p>
<p><img src="/img/hbase-6.png" alt="" title="HBase的数据在HDFS中"></p>
<p><img src="/img/hbase-7.png" alt="" title="HBase集群管理界面 用户表信息"></p>
<p>当我们在HDFS中看到表中的某个块的数据，如下：</p>
<p><img src="/img/hbase-8.png" alt="" title="HBase的数据在HDFS中"></p>
<p>我们可以通过Hbase中的命令来查看数据的真实内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6</span><br><span class="line">$ ./bin/hbase hfile -p -f /hbase/data/default/t_student/b76cccf6c6a7926bf8f40b4eafc6991e/cf1/2ed0a233411447778982edce04e96fe3</span><br><span class="line">2019-01-23 19:45:33,200 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">2019-01-23 19:45:34,046 INFO  [main] hfile.CacheConfig: Created cacheConfig: CacheConfig:disabled</span><br><span class="line">K: 01/cf1:name/1548242390794/Put/vlen=3/seqid=4 V: tim</span><br><span class="line">Scanned kv count -&gt; 1</span><br></pre></td></tr></table></figure></p>
<p>查看集群状态和节点数量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):009:0&gt; status</span><br><span class="line">1 active master, 1 backup masters, 3 servers, 0 dead, 1.0000 average load</span><br></pre></td></tr></table></figure></p>
<p>根据条件查询数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):010:0&gt; get <span class="string">'t_student'</span>, <span class="string">'01'</span></span><br><span class="line">COLUMN                                               CELL</span><br><span class="line"> cf1:name                                            timestamp=1548242390794, value=tim</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0590 seconds</span><br></pre></td></tr></table></figure></p>
<p>表失效、表生效、删除表：</p>
<ol>
<li>使用<code>disable</code>命令可将某张表失效，失效后该表将不能使用；</li>
<li>使用<code>enable</code>命令可使表重新生效，表生效后，即可对表进行操作；</li>
<li>使用<code>drop</code>命令可对表进行删除，但只有表在失效的情况下，才能进行删除。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; desc <span class="string">'t_student'</span></span><br><span class="line">Table t_student is ENABLED</span><br><span class="line">t_student</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf1'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, VERSIONS =&gt; <span class="string">'1'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, TTL =&gt; <span class="string">'FOREVER'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, BLO</span><br><span class="line">CKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>&#125;</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0480 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):012:0&gt; <span class="built_in">disable</span> <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 2.4070 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):013:0&gt; desc <span class="string">'t_student'</span></span><br><span class="line">Table t_student is DISABLED</span><br><span class="line">t_student</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf1'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, VERSIONS =&gt; <span class="string">'1'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, TTL =&gt; <span class="string">'FOREVER'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, BLO</span><br><span class="line">CKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>&#125;</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0320 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):014:0&gt; <span class="built_in">enable</span> <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 1.3260 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):015:0&gt; <span class="built_in">disable</span> <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 2.2550 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):016:0&gt; drop <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 1.3540 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):017:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.0060 seconds</span><br><span class="line"></span><br><span class="line">=&gt; []</span><br></pre></td></tr></table></figure>
<p>退出 hbase shell<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; quit</span><br></pre></td></tr></table></figure></p>
<h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><ol>
<li><p>有时候我们重启了hadoop集群后，发现hbase无法使用，有可能是我们在hbase-site.xml中配置的hadoop master节点已经不是active了，解决办法是在hadoop中手动将其设为active状态；</p>
</li>
<li><p>主从节点时间没有同步时，极有可能出现如下错误，同步时间后可以正常启动：</p>
<pre><code>master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 855041 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.

[HBase] ERROR:org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
</code></pre></li>
</ol>
<h3 id="java操作Hbase"><a href="#java操作Hbase" class="headerlink" title="java操作Hbase"></a>java操作Hbase</h3><p>java操作Hbase的例子见这里：<a href="https://github.com/hewentian/hadoop-demo/blob/master/src/main/java/com/hewentian/hadoop/utils/HbaseUtil.java">HbaseUtil.java</a>、<a href="https://github.com/hewentian/hadoop-demo/blob/master/src/main/java/com/hewentian/hadoop/hbase/HbaseDemo.java">HbaseDemo.java</a></p>
<p>未完，待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/01/20/hbase-cluster/" data-id="cjz3s88qh000xqc3kohjvrjnx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hive-note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/10/hive-note/" class="article-date">
  <time datetime="2019-01-10T14:30:12.000Z" itemprop="datePublished">2019-01-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/10/hive-note/">hive 学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Hive-介绍"><a href="#Hive-介绍" class="headerlink" title="Hive 介绍"></a>Hive 介绍</h3><p>hive的<a href="https://hive.apache.org/" target="_blank" rel="noopener">官方文档</a>中有对hive的详细介绍，这里不再赘述。我们用一句话描述如下：<br>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p>
<h3 id="hive-的安装"><a href="#hive-的安装" class="headerlink" title="hive 的安装"></a>hive 的安装</h3><p>安装过程参考这里：<br><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
<p>hive依赖于HADOOP，我们在上一篇<a href="../../../../2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>的基础上安装hive。</p>
<p>首先下载hive，我们下载的时候，要选择适合我们HADOOP版本的hive，我们下载的稳定版为<a href="http://archive.apache.org/dist/hive/hive-1.2.2/" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a>，我们将在HADOOP集群的namenode上面安装，即在master机器上面安装。将压缩包传到<code>/home/hadoop/</code>目录下。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xzvf apache-hive-1.2.2-bin.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>解压后得到目录<code>apache-hive-1.2.2-bin</code>，我们看下压缩包中的内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin</span><br><span class="line">$ ls</span><br><span class="line">bin  conf  examples  hcatalog  lib  LICENSE  NOTICE  README.txt  RELEASE_NOTES.txt  scripts</span><br><span class="line">$</span><br><span class="line">$ ls conf/</span><br><span class="line">beeline-log4j.properties.template  hive-env.sh.template                 hive-log4j.properties.template</span><br><span class="line">hive-default.xml.template          hive-exec-log4j.properties.template  ivysettings.xml</span><br></pre></td></tr></table></figure></p>
<p>配置HADOOP_HOME：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/conf/</span><br><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br><span class="line">$ cp hive-env.sh.template hive-env.sh</span><br><span class="line">$ vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/home/hadoop/hadoop-2.7.3</span><br></pre></td></tr></table></figure></p>
<p>到这里，hive就配置好了，可以运行了。但，不妨看下下面的<code>配置hive元数据的存储位置</code>，因为生产环境可能会这样用。我们这次的安装将不会执行这个步骤。</p>
<p>配置hive元数据的存储位置（可选配置）<br>hive默认将元数据存储在<code>derby</code>数据库中（hive安装包自带），当然我们也可以选择存储在其他数据库，如mysql中。下面演示一下：<br>首先在MYSQL数据库中创建一个数据库，用于存储hive的元数据，我们就将库名创建为hive：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database hive;</span><br></pre></td></tr></table></figure></p>
<p>然后配置hive使用mysql存储元数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/conf/</span><br><span class="line">$ vi hive-site.xml</span><br></pre></td></tr></table></figure></p>
<p>修改下面部分，假定我们的数据库地址、用户名和密码如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.1.188:3306/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      JDBC connect string for a JDBC metastore.</span><br><span class="line">      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class="line">      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>最后，将mysql连接JDBC的jar包<a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.42/mysql-connector-java-5.1.42.jar" target="_blank" rel="noopener">mysql-connector-java-5.1.42.jar</a>放到<code>apache-hive-1.2.2-bin/lib</code>目录下</p>
<p>好了，以上这部分是<strong>可选配置</strong>部分。</p>
<h3 id="启动hive"><a href="#启动hive" class="headerlink" title="启动hive"></a>启动hive</h3><p>初次启动hive，需在HDFS中创建几个目录，用于存储hive的数据，我们在安装hive的master节点执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -mkdir /tmp</span><br><span class="line">$ ./bin/hdfs dfs -mkdir -p /user/hive/warehouse</span><br><span class="line">$</span><br><span class="line">$ ./bin/hdfs dfs -chmod g+w /tmp</span><br><span class="line">$ ./bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></p>
<p>正式启动hive<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./hive</span><br></pre></td></tr></table></figure></p>
<p>启动的时候可能会报如下错误：</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
    at org.apache.hadoop.fs.Path.initialize(Path.java:205)
    at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:171)
    at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:659)
    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:582)
    at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:549)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:750)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
    at java.net.URI.checkPath(URI.java:1823)
    at java.net.URI.&lt;init&gt;(URI.java:745)
    at org.apache.hadoop.fs.Path.initialize(Path.java:202)
    ... 12 more
</code></pre><p>解决方法如下，先建目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/</span><br><span class="line">$ mkdir iotmp</span><br></pre></td></tr></table></figure></p>
<p>将<code>hive-site.xml</code>中</p>
<ol>
<li>包含<code>${system:java.io.tmpdir}</code>的配置项替换为上面的路径<code>/home/hadoop/apache-hive-1.2.2-bin/iotmp</code>，一共有4处；</li>
<li>包含<code>${system:user.name}</code>的配置项替换为<code>hadoop</code>。<br>修改项如下：<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Location of Hive run time structured log file<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/hadoop/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>重新启动hive：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./hive</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/home/hadoop/apache-hive-1.2.2-bin/lib/hive-common-1.2.2.jar!/hive-log4j.properties</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.821 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; use default;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.043 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; show tables;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.094 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p>
<p>至此，hive安装成功。从上面可知，hive有一个默认的数据库<code>default</code>，并且里面一张表也没有。</p>
<h3 id="hive初体验"><a href="#hive初体验" class="headerlink" title="hive初体验"></a>hive初体验</h3><ul>
<li><p>A longer tutorial that covers more features of HiveQL:<br><a href="https://cwiki.apache.org/confluence/display/Hive/Tutorial" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Tutorial</a></p>
</li>
<li><p>The HiveQL Language Manual:<br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual</a></p>
</li>
</ul>
<h3 id="创建数据库："><a href="#创建数据库：" class="headerlink" title="创建数据库："></a>创建数据库：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE DATABASE IF NOT EXISTS tim;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.323 seconds</span><br><span class="line">hive&gt;</span><br><span class="line">    &gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">tim</span><br><span class="line">Time taken: 0.025 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>
<p>同样，我们可以在HDFS中查看到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -ls /user/hive/warehouse</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwxr-x   - hadoop supergroup          0 2019-01-01 19:32 /user/hive/warehouse/tim.db</span><br></pre></td></tr></table></figure></p>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use tim;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.042 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; CREATE TABLE IF NOT EXISTS t_user (</span><br><span class="line">    &gt; id INT,</span><br><span class="line">    &gt; name STRING COMMENT <span class="string">'user name'</span>,</span><br><span class="line">    &gt; age INT COMMENT <span class="string">'user age'</span>,</span><br><span class="line">    &gt; sex STRING COMMENT <span class="string">'user sex'</span>,</span><br><span class="line">    &gt; birthday DATE COMMENT <span class="string">'user birthday'</span>,</span><br><span class="line">    &gt; address STRING COMMENT <span class="string">'user address'</span></span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; COMMENT <span class="string">'This is the use info table'</span></span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt;  FIELDS TERMINATED BY <span class="string">'\t'</span></span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.521 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; show tables;</span><br><span class="line">OK</span><br><span class="line">t_user</span><br><span class="line">Time taken: 0.035 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<h3 id="查看表结构"><a href="#查看表结构" class="headerlink" title="查看表结构"></a>查看表结构</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc t_user;</span><br><span class="line">OK</span><br><span class="line">id                  	int</span><br><span class="line">name                	string              	user name</span><br><span class="line">age                 	int                 	user age</span><br><span class="line">sex                 	string              	user sex</span><br><span class="line">birthday            	date                	user birthday</span><br><span class="line">address             	string              	user address</span><br><span class="line">Time taken: 0.074 seconds, Fetched: 6 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; INSERT INTO TABLE t_user(id, name, age, sex, birthday, address) VALUES(1, <span class="string">'Tim Ho'</span>, 23, <span class="string">'M'</span>, <span class="string">'1989-05-01'</span>, <span class="string">'Higher Education Mega Center South, Guangzhou city, Guangdong Province'</span>);</span><br><span class="line">Query ID = hadoop_20190102160558_640a90a7-9122-4650-af78-acb436e2643b</span><br><span class="line">Total <span class="built_in">jobs</span> = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is <span class="built_in">set</span> to 0 since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1546186928725_0015, Tracking URL = http://hadoop-host-master:8088/proxy/application_1546186928725_0015/</span></span><br><span class="line"><span class="string">Kill Command = /home/hadoop/hadoop-2.7.3/bin/hadoop job  -kill job_1546186928725_0015</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-01-02 16:06:08,341 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-01-02 16:06:14,565 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 1 seconds 390 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1546186928725_0015</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://hadoop-cluster-ha/user/hive/warehouse/tim.db/t_user/.hive-staging_hive_2019-01-02_16-05-58_785_7094384272339204067-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table tim.t_user</span></span><br><span class="line"><span class="string">Table tim.t_user stats: [numFiles=1, numRows=1, totalSize=96, rawDataSize=95]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 1.39 sec   HDFS Read: 4763 HDFS Write: 162 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 1 seconds 390 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 17.079 seconds</span></span><br><span class="line"><span class="string">hive&gt;</span></span><br></pre></td></tr></table></figure>
<p>执行插入操作它会产生一个mapReduce任务。</p>
<h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from t_user;</span><br><span class="line">OK</span><br><span class="line">1	Tim Ho	23	M	1989-05-01	Higher Education Mega Center South, Guangzhou city, Guangdong Province</span><br><span class="line">Time taken: 0.196 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; select * from t_user <span class="built_in">where</span> name=<span class="string">'Tim Ho'</span>;</span><br><span class="line">OK</span><br><span class="line">1	Tim Ho	23	M	1989-05-01	Higher Education Mega Center South, Guangzhou city, Guangdong Province</span><br><span class="line">Time taken: 0.258 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; select count(*) from t_user;</span><br><span class="line">Query ID = hadoop_20190102161100_d60df721-539d-4e5b-a3db-a4951ac884b4</span><br><span class="line">Total <span class="built_in">jobs</span> = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</span><br><span class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</span><br><span class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to <span class="built_in">set</span> a constant number of reducers:</span><br><span class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1546186928725_0016, Tracking URL = http://hadoop-host-master:8088/proxy/application_1546186928725_0016/</span><br><span class="line">Kill Command = /home/hadoop/hadoop-2.7.3/bin/hadoop job  -<span class="built_in">kill</span> job_1546186928725_0016</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2019-01-02 16:11:10,739 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-01-02 16:11:16,997 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.05 sec</span><br><span class="line">2019-01-02 16:11:23,280 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.37 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 2 seconds 370 msec</span><br><span class="line">Ended Job = job_1546186928725_0016</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.37 sec   HDFS Read: 7285 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 2 seconds 370 msec</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">Time taken: 24.444 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<p>由上面可知，执行简单的查询操作不会启动mapReduce，但执行像COUNT这样的统计操作将会产生一个mapReduce。</p>
<h3 id="从文件中导入数据"><a href="#从文件中导入数据" class="headerlink" title="从文件中导入数据"></a>从文件中导入数据</h3><p>语法：</p>
<pre><code>LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
</code></pre><p>我们可以按定义表结构时的使用的字段分隔符(\t)，将数据存放在文本文件里，然后使用LOAD命令来导入。例如我们将数据存放在<code>/home/hadoop/user.txt</code>中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2	scott	25	M	1977-10-21	USA</span><br><span class="line">3	tiger	21	F	1977-08-12	UK</span><br></pre></td></tr></table></figure></p>
<p>然后在hive中执行LOAD命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH <span class="string">'/home/hadoop/user.txt'</span> INTO TABLE t_user;</span><br><span class="line">Loading data to table tim.t_user</span><br><span class="line">Table tim.t_user stats: [numFiles=2, numRows=0, totalSize=151, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.214 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; select * from t_user;</span><br><span class="line">OK</span><br><span class="line">1	Tim Ho	23	M	1989-05-01	Higher Education Mega Center South, Guangzhou city, Guangdong Province</span><br><span class="line">2	scott	25	M	1977-10-21	USA</span><br><span class="line">3	tiger	21	F	1977-08-12	UK</span><br><span class="line">Time taken: 0.085 seconds, Fetched: 3 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="通过JAVA代码操作hive"><a href="#通过JAVA代码操作hive" class="headerlink" title="通过JAVA代码操作hive"></a>通过JAVA代码操作hive</h3><p>HQL脚本通常有以下几种方式执行：</p>
<ol>
<li>hive -e “hql”; </li>
<li>hive -f “hql.file”;</li>
<li>hive jdbc code.</li>
</ol>
<p>本节主要讲讲如何通过java来操作hive，首先启动HiveServer2，首次启动的时候需执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./schematool -dbType derby -initSchema</span><br></pre></td></tr></table></figure></p>
<p>启动hiveserver2，hiveserver2命令未来可用于替代hive命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./hiveserver2</span><br></pre></td></tr></table></figure></p>
<p>启动后，你可能会发现，啥也没输出。这时我们在另一个SHELL窗口中启动beelie<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./beeline -u jdbc:hive2://hadoop-host-master:10000 -n hadoop -p hadoop</span><br><span class="line"></span><br><span class="line">Connecting to jdbc:hive2://hadoop-host-master:10000</span><br><span class="line">Connected to: Apache Hive (version 1.2.2)</span><br><span class="line">Driver: Hive JDBC (version 1.2.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; </span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| tim            |</span><br><span class="line">+----------------+--+</span><br><span class="line">2 rows selected (0.217 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; use tim;</span><br><span class="line">No rows affected (0.08 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; show tables;</span><br><span class="line">+-----------+--+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+--+</span><br><span class="line">| t_user    |</span><br><span class="line">+-----------+--+</span><br><span class="line">1 row selected (0.071 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; select * from t_user;</span><br><span class="line">+------------+--------------+-------------+-------------+------------------+-------------------------------------------------------------------------+--+</span><br><span class="line">| t_user.id  | t_user.name  | t_user.age  | t_user.sex  | t_user.birthday  |                             t_user.address                              |</span><br><span class="line">+------------+--------------+-------------+-------------+------------------+-------------------------------------------------------------------------+--+</span><br><span class="line">| 1          | Tim Ho       | 23          | M           | 1989-05-01       | Higher Education Mega Center South, Guangzhou city, Guangdong Province  |</span><br><span class="line">| 2          | scott        | 25          | M           | 1977-10-21       | USA                                                                     |</span><br><span class="line">| 3          | tiger        | 21          | F           | 1977-08-12       | UK                                                                      |</span><br><span class="line">+------------+--------------+-------------+-------------+------------------+-------------------------------------------------------------------------+--+</span><br><span class="line">3 rows selected (0.219 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt;</span><br></pre></td></tr></table></figure></p>
<p>由上面可知，和在hive命令下的操作是一样的。上面的命令也可以没有<code>-p hadoop</code>这个参数，这个可以在<code>hive-site.xml</code>中配置。</p>
<p>java代码操作hive的例子在这里：<a href="https://github.com/hewentian/hadoop-demo/blob/master/src/main/java/com/hewentian/hadoop/utils/HiveUtil.java">HiveUtil.java</a>、<a href="https://github.com/hewentian/hadoop-demo/blob/master/src/main/java/com/hewentian/hadoop/hive/HiveDemo.java">HiveDemo.java</a></p>
<p>未完待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/01/10/hive-note/" data-id="cjz3s88qj0014qc3kb62ema2a" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/">hive</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop-cluster-ha" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/01/hadoop-cluster-ha/" class="article-date">
  <time datetime="2019-01-01T06:13:31.000Z" itemprop="datePublished">2019-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说<code>hadoop</code>集群HA的搭建，如果不想搭建HA，可以参考我之前的笔记：<a href="../../../../2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>，下面HA的搭建很多步骤与此文相同。</p>
<p>为了解决<code>hadoop 1.0.0</code>之前版本的单点故障问题，在<code>hadoop 2.0.0</code>中通过在同一个集群上运行两个<code>NameNode</code>的<code>主动/被动</code>配置热备份，这样集群允许在一个NameNode出现故障时，请求转移到另外一个NameNode来保证集群的正常运行。两个NameNode有相同的职能。在任何时刻，只有一个是<code>active</code>状态的，另一个是<code>standby</code>状态的。当集群运行时，只有<code>active</code>状态的NameNode是正常工作的，<code>standby</code>状态的NameNode是处于待命状态的，时刻同步<code>active</code>状态NameNode的数据。一旦<code>active</code>状态的NameNode不能工作，通过手工或者自动切换，<code>standby</code>状态的NameNode就可以转变为<code>active</code>状态的，就可以继续工作了，这就是高可靠。</p>
<p>安装过程参考官方文档：<br><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></p>
<p>hadoop集群的搭建，我们将搭建如下图所示的集群，HADOOP集群中所有节点的配置文件可以一模一样的。</p>
<p><img src="/img/hadoop-ha-1.png" alt="" title="HADOOP HA集群结构图"></p>
<p>对上图的节点分布，如下图（绿色代表在这些节点上面安装这些程序，一般运行namenode的节点都同时运行ZKFC）：</p>
<p><img src="/img/hadoop-ha-2.png" alt="" title="HADOOP HA集群 节点分布"></p>
<p>在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装四台服务器：master、slave1、slave2、slave3来搭建hadoop集群HA。安装好VirtualBox后，启动它。依次点<code>File -&gt; Host Network Manager -&gt; Create</code>，来创建一个网络和虚拟机中的机器通讯，这个地址是：<code>192.168.56.1</code>，也就是我们外面实体机的地址（仅和虚拟机中的机器通讯使用）。如下图：</p>
<p><img src="/img/hadoop-1.png" alt="" title="虚拟机网络配置"></p>
<p>我们使用<code>ubuntu 18.04</code>来作为我们的服务器，先在虚拟机中安装好一台服务器master，将Jdk、hadoop在上面安装好，然后将master克隆出slave1、slave2、slave3。以master为namenode节点，slave1、slave2、slave3作为datanode节点。slave1同时也作为namenode节点。相关配置如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
slave3:
    ip: 192.168.56.113
    hostname: hadoop-host-slave-3
</code></pre><h3 id="下面开始master的安装"><a href="#下面开始master的安装" class="headerlink" title="下面开始master的安装"></a>下面开始master的安装</h3><p>在虚拟机中安装<code>master</code>的过程中我们会设置一个用户用于登录，我们将用户名、密码都设为<code>hadoop</code>，当然也可以为其他名字，其他安装过程略。安装好之后，使用默认的网关配置NAT，NAT可以访问外网，我们将<code>jdk-8u102-linux-x64.tar.gz</code>和<code>hadoop-2.7.3.tar.gz</code>从它们的官网下载到用户的<code>/home/hadoop/</code>目录下。或在实体机中通过SCP命令传进去。然后将网关设置为<code>Host-only Adapter</code>，如下图所示。</p>
<p><img src="/img/hadoop-2.png" alt="" title="网络配置"></p>
<p>网关设置好了之后，我们接下来配置IP地址。在<code>master</code>中<code>[Settings] -&gt; [Network] -&gt; [Wired 这里打开] -&gt; [IPv4]</code>按如下设置：</p>
<p><img src="/img/hadoop-3.png" alt="" title="网络配置"></p>
<h3 id="管理集群"><a href="#管理集群" class="headerlink" title="管理集群"></a>管理集群</h3><p>在上面的IP等配置好之后，我们选择关闭master，注意不是直接关闭，而是在关闭的时候选择<code>Save the machine state</code>。然后在虚拟机中选中<code>master -&gt; Start 下拉箭头 -&gt; Headless start</code>，然后在我们实体机中通过ssh直接登录到master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@192.168.56.110</span><br></pre></td></tr></table></figure></p>
<p>我们可以在实体机通过配置<code>/etc/hosts</code>，加上如下配置：</p>
<pre><code>192.168.56.110    hadoop-host-master
</code></pre><p>然后就可以通过如下方式登录了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>在实体机中通过下面的配置，就可以无密码登录了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p><strong> 下面的操作，均是在实体机中通过SSH到虚拟机执行的操作。 </strong></p>
<h3 id="安装ssh-openssh-rsync"><a href="#安装ssh-openssh-rsync" class="headerlink" title="安装ssh openssh rsync"></a>安装ssh openssh rsync</h3><p>如系统已安装，则勿略下面的安装操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh openssh-server rsync</span><br></pre></td></tr></table></figure></p>
<p>如果上述命令无法执行，请先执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>JDK的安装请参考我之前的笔记：<a href="../../../../2017/12/08/install-jdk/">安装 JDK</a>，这里不再赘述。安装到此目录<code>/usr/local/jdk1.8.0_102/</code>下，记住此路径，下面会用到。下在进行hadoop的安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xf hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压后得到hadoop-2.7.3目录，hadoop的程序和相关配置就在此目录中。</p>
<h3 id="建保存数据的目录"><a href="#建保存数据的目录" class="headerlink" title="建保存数据的目录"></a>建保存数据的目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p hdfs/tmp</span><br><span class="line">$ mkdir -p hdfs/name</span><br><span class="line">$ mkdir -p hdfs/data</span><br><span class="line">$ mkdir -p journal/data</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 hdfs/</span><br><span class="line">$ chmod -R 777 journal/</span><br></pre></td></tr></table></figure>
<h3 id="配置文件浏览"><a href="#配置文件浏览" class="headerlink" title="配置文件浏览"></a>配置文件浏览</h3><p>hadoop的配置文件都位于下面的目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop</span><br><span class="line">$ ls </span><br><span class="line">capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh</span><br><span class="line">configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template</span><br><span class="line">container-executor.cfg      httpfs-signature.secret  mapred-site.xml.template</span><br><span class="line">core-site.xml               httpfs-site.xml          slaves</span><br><span class="line">hadoop-env.cmd              kms-acls.xml             ssl-client.xml.example</span><br><span class="line">hadoop-env.sh               kms-env.sh               ssl-server.xml.example</span><br><span class="line">hadoop-metrics2.properties  kms-log4j.properties     yarn-env.cmd</span><br><span class="line">hadoop-metrics.properties   kms-site.xml             yarn-env.sh</span><br><span class="line">hadoop-policy.xml           log4j.properties         yarn-site.xml</span><br><span class="line">hdfs-site.xml               mapred-env.cmd</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hadoop-env-sh，加上JDK绝对路径"><a href="#配置hadoop-env-sh，加上JDK绝对路径" class="headerlink" title="配置hadoop-env.sh，加上JDK绝对路径"></a>配置hadoop-env.sh，加上JDK绝对路径</h3><p>JDK的路径就是上面安装JDK的时候的路径：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_102/</span><br></pre></td></tr></table></figure></p>
<h3 id="配置core-site-xml，在该文件中加入如下内容"><a href="#配置core-site-xml，在该文件中加入如下内容" class="headerlink" title="配置core-site.xml，在该文件中加入如下内容"></a>配置core-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-cluster-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-master:2181,hadoop-host-slave-1:2181,hadoop-host-slave-2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hdfs-site-xml，在该文件中加入如下内容"><a href="#配置hdfs-site-xml，在该文件中加入如下内容" class="headerlink" title="配置hdfs-site.xml，在该文件中加入如下内容"></a>配置hdfs-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-cluster-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.hadoop-cluster-ha<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.hadoop-cluster-ha.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.hadoop-cluster-ha.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-slave-1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.hadoop-cluster-ha.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.hadoop-cluster-ha.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-slave-1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop-host-slave-1:8485;hadoop-host-slave-2:8485;hadoop-host-slave-3:8485/hadoop-cluster-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.hadoop-cluster-ha<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop-2.7.3/journal/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>至此，master中要安装的通用环境配置完成。在虚拟机中将master复制出slave1、slave2、slave3。并参考上面配置IP地址的方法将slave1的ip配置为:<code>192.168.56.111</code>，slave2的ip配置为：<code>192.168.56.112</code>，slave3的ip配置为：<code>192.168.56.113</code>。</p>
<h3 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h3><p>配置master的主机名为<code>hadoop-host-master</code>，在master节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>配置slave1的主机名为<code>hadoop-host-slave-1</code>，在slave1节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<p>配置slave2的主机名为<code>hadoop-host-slave-2</code>，在slave2节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>配置slave3的主机名为<code>hadoop-host-slave-3</code>，在slave3节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p><strong> 注意：各个节点的主机名一定要不同，否则相同主机名的节点，只会有一个连得上namenode节点，并且集群会报错，修改主机名后，要重启才生效。 </strong></p>
<h3 id="配置域名解析"><a href="#配置域名解析" class="headerlink" title="配置域名解析"></a>配置域名解析</h3><p>分别对master、slave1、slave2、slave3都执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">127.0.0.1	localhost</span><br><span class="line">192.168.56.110	hadoop-host-master</span><br><span class="line">192.168.56.111	hadoop-host-slave-1</span><br><span class="line">192.168.56.112	hadoop-host-slave-2</span><br><span class="line">192.168.56.113	hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<h3 id="集中式管理集群"><a href="#集中式管理集群" class="headerlink" title="集中式管理集群"></a>集中式管理集群</h3><p>配置SSH无密码登陆，分别在master、slave1、slave2和slave3上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>在master、slave1上面执行如下脚本（master和slave1都作为namenode）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop-host-master</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-1</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-2</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>每执行一条命令的时候，都先输入yes，然后再输入目标机器的登录密码。</p>
<p>如果能成功运行如下命令，则配置免密登录其他机器成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop-host-master</span><br><span class="line">$ ssh hadoop-host-slave-1</span><br><span class="line">$ ssh hadoop-host-slave-2</span><br><span class="line">$ ssh hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>在master、slave1上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi slaves    <span class="comment"># 加入如下内容</span></span><br><span class="line">$</span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br><span class="line">hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>当执行<code>start-dfs.sh</code>时，它会去slaves文件中找从节点。</p>
<h3 id="安装zookeeper"><a href="#安装zookeeper" class="headerlink" title="安装zookeeper"></a>安装zookeeper</h3><p>我们在master、slave1和slave2上面安装zookeeper集群，安装过程可以参考：<a href="../../../../2017/12/06/zookeeper-install-cluster/">zookeeper 集群版安装方法</a>，这里不再赘述。</p>
<p>至此，集群配置完成，下面将启动集群。</p>
<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><p>首次启动的时候，先启动<code>journalnode</code>，分别在三台<code>journalnode</code>机器上面启动，因为接下来格式化<code>namenode</code>的时候，数据会写到这些节点中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh start journalnode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 JournalNode</span><br></pre></td></tr></table></figure></p>
<p>接下来在任意一台namenode执行如下命令，我们在master中执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -format    <span class="comment"># 再次启动的时候不需要执行此操作</span></span><br><span class="line">$ ./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 NameNode</span><br></pre></td></tr></table></figure></p>
<p>然后在另一台未格式化的namenode节点，即slave1执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure></p>
<p>然后停掉所有服务，在master下执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">Stopping namenodes on [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: no namenode to stop</span><br><span class="line">hadoop-host-master: stopping namenode</span><br><span class="line">hadoop-host-slave-1: no datanode to stop</span><br><span class="line">hadoop-host-slave-2: no datanode to stop</span><br><span class="line">hadoop-host-slave-3: no datanode to stop</span><br><span class="line">Stopping journal nodes [hadoop-host-slave-1 hadoop-host-slave-2 hadoop-host-slave-3]</span><br><span class="line">hadoop-host-slave-2: stopping journalnode</span><br><span class="line">hadoop-host-slave-1: stopping journalnode</span><br><span class="line">hadoop-host-slave-3: stopping journalnode</span><br><span class="line">Stopping ZK Failover Controllers on NN hosts [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: no zkfc to stop</span><br><span class="line">hadoop-host-master: no zkfc to stop</span><br></pre></td></tr></table></figure></p>
<p>在其中一个namenode上执行格式化ZKFC，我们在master中执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs zkfc -formatZK</span><br><span class="line">$</span><br><span class="line"></span><br><span class="line">18/12/30 12:54:52 INFO ha.ActiveStandbyElector: Session connected.</span><br><span class="line">18/12/30 12:54:52 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/hadoop-cluster-ha <span class="keyword">in</span> ZK.</span><br><span class="line">18/12/30 12:54:52 INFO zookeeper.ClientCnxn: EventThread shut down</span><br><span class="line">18/12/30 12:54:52 INFO zookeeper.ZooKeeper: Session: 0x167fd5512250000 closed</span><br></pre></td></tr></table></figure></p>
<p>再次启动集群的时候，不需执行上面的操作，直接执行如下命令即可，我们在master上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br><span class="line">$</span><br><span class="line"></span><br><span class="line">Starting namenodes on [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: starting namenode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-namenode-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-master: starting namenode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-namenode-hadoop-host-master.out</span><br><span class="line">hadoop-host-slave-2: starting datanode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-datanode-hadoop-host-slave-2.out</span><br><span class="line">hadoop-host-slave-1: starting datanode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-datanode-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-slave-3: starting datanode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-datanode-hadoop-host-slave-3.out</span><br><span class="line">Starting journal nodes [hadoop-host-slave-1 hadoop-host-slave-2 hadoop-host-slave-3]</span><br><span class="line">hadoop-host-slave-2: starting journalnode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-journalnode-hadoop-host-slave-2.out</span><br><span class="line">hadoop-host-slave-1: starting journalnode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-journalnode-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-slave-3: starting journalnode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-journalnode-hadoop-host-slave-3.out</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: starting zkfc, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-zkfc-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-master: starting zkfc, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-zkfc-hadoop-host-master.out</span><br></pre></td></tr></table></figure></p>
<p>它会自动启动namenode、datanode、journalnode和zkfc，在启动的过程中观看日志，是个好习惯。</p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:50070/" target="_blank" rel="noopener">http://hadoop-host-master:50070/</a><br><a href="http://hadoop-host-slave-1:50070/" target="_blank" rel="noopener">http://hadoop-host-slave-1:50070/</a><br>来查看是否启动成功，如无意外的话，你会看到如下结果页面。其中一个是active，另一个是standby：</p>
<p><img src="/img/hadoop-ha-3.png" alt="" title="hadoop管理界面standby：Overview"></p>
<p><img src="/img/hadoop-ha-4.png" alt="" title="hadoop管理界面active：Overview"></p>
<p>我们在active节点的页面上切换tab到Datanodes可以看到有3个datanode节点，如下图所示：</p>
<p><img src="/img/hadoop-ha-5.png" alt="" title="hadoop管理界面:Datanodes"></p>
<p>切换到<code>Utilities -&gt; Browse the file system</code>，如下图所示（只能在active节点的页面中查看，standby节点对HDFS没有READ权限）：</p>
<p><img src="/img/hadoop-ha-6.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>从上面的界面可以看到，目前HDFS中没有任何文件。我们尝试往其中放一个文件，就将我们的hadoop的压缩包放进去，在<code>active</code>的namenode节点中执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/Downloads/hadoop-2.7.3.tar.gz /</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hadoop supergroup  214092195 2018-12-29 22:07 /hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>我们在图形界面中查看，如下图：</p>
<p><img src="/img/hadoop-ha-7.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>我们点击列表中的文件，将会显示它的数据具体分布在哪些节点上，如下图：</p>
<p><img src="/img/hadoop-ha-8.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p><strong> 注意：在主节点执行<code>start-dfs.sh</code>，主节点的用户名必须和所有从节点的用户名相同。因为主节点服务器以这个用户名去远程登录到其他从节点的服务器中，所以在所有的生产环境中控制同一类集群的用户一定要相同。 </strong></p>
<h3 id="验证failover，即验证两个namenode是否可以自动切换"><a href="#验证failover，即验证两个namenode是否可以自动切换" class="headerlink" title="验证failover，即验证两个namenode是否可以自动切换"></a>验证failover，即验证两个namenode是否可以自动切换</h3><p>我们将<code>active</code>的namenode kill掉，在<code>active</code>的namenode节点上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">2593 QuorumPeerMain</span><br><span class="line">31444 Jps</span><br><span class="line">30613 NameNode</span><br><span class="line">30965 DFSZKFailoverController</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">kill</span> -9 30613</span><br></pre></td></tr></table></figure></p>
<p>我们kill掉之后发现standby无法自动切换到active。我们查看日志，发现：<br>/home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-zkfc-hadoop-host-slave-1.log<br>有如下内容：</p>
<p><img src="/img/hadoop-ha-9.png" alt=""></p>
<p>结论：两个namenode节点无法自动切换，的原因是操作系统安装的<code>openssh</code>版本和<code>hadoop</code>内部使用的版本不匹配造成的。</p>
<p>解决方案：将<code>$HADOOP_HOME/share</code>目录下的<code>jsch-0.1.42.jar</code>升级到<code>jsch-0.1.54.jar</code>，重启集群，问题解决。</p>
<p>我们首先到maven中央仓库下载<code>jsch-0.1.54.jar</code>：</p>
<pre><code>https://mvnrepository.com/artifact/com.jcraft/jsch/0.1.54
</code></pre><p>我们只需将两个namenode中的<code>jsch-0.1.42.jar</code>升级到<code>jsch-0.1.54.jar</code>即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ find ./ -name <span class="string">"*jsch*"</span></span><br><span class="line">$ </span><br><span class="line">./share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/jsch-0.1.42.jar</span><br><span class="line">./share/hadoop/common/lib/jsch-0.1.42.jar</span><br><span class="line">./share/hadoop/tools/lib/jsch-0.1.42.jar</span><br><span class="line">./share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/jsch-0.1.42.jar</span><br></pre></td></tr></table></figure></p>
<p>从查询结果看，只有4个JAR包需要升级，我们只要将两个namenode节点中的JAR包替换即可。重启集群，再次验证<code>failover</code>，我们可以看到两个namenode已经可以自动切换。大功告成。</p>
<h3 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h3><p>YARN的启动步骤和<a href="../../../../2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>一样，这里不再赘述。</p>
<h3 id="active和standby之间的手动切换"><a href="#active和standby之间的手动切换" class="headerlink" title="active和standby之间的手动切换"></a>active和standby之间的手动切换</h3><p>有时候，我们需要手动将某个namenode设置为active，可以通过<code>haadmin</code>命令，相关用法如下（<strong>我一般的做法是将原来active的namenode断网，从而让standby的节点成为active，然后再将之前断网的机器连回网络</strong>）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ ./bin/hdfs haadmin --<span class="built_in">help</span></span><br><span class="line">-<span class="built_in">help</span>: Unknown <span class="built_in">command</span></span><br><span class="line">Usage: haadmin</span><br><span class="line">    [-transitionToActive [--forceactive] &lt;serviceId&gt;]</span><br><span class="line">    [-transitionToStandby &lt;serviceId&gt;]</span><br><span class="line">    [-failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;]</span><br><span class="line">    [-getServiceState &lt;serviceId&gt;]</span><br><span class="line">    [-checkHealth &lt;serviceId&gt;]</span><br><span class="line">    [-<span class="built_in">help</span> &lt;<span class="built_in">command</span>&gt;]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs haadmin -getServiceState nn1</span><br><span class="line">standby</span><br><span class="line">$ ./bin/hdfs haadmin -getServiceState nn2</span><br><span class="line">active</span><br><span class="line">$ ./bin/hdfs haadmin -transitionToActive --forcemanual nn1</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/01/01/hadoop-cluster-ha/" data-id="cjz3s88qd000lqc3kx6sppa2i" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop-mapreduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/19/hadoop-mapreduce/" class="article-date">
  <time datetime="2018-12-19T12:06:47.000Z" itemprop="datePublished">2018-12-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/19/hadoop-mapreduce/">hadoop mapreduce示例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>关于hadoop集群的搭建，请参考我的上一篇 <a href="../../../../2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>，这里将说说如何写一个简单的统计单词个数的<code>mapReduce</code>示例程序，并部署在<code>YARN</code>上面运行。</p>
<p>代码托管在：<a href="https://github.com/hewentian/hadoop-demo">https://github.com/hewentian/hadoop-demo</a></p>
<p>下面详细说明。</p>
<h3 id="第一步：将要统计单词个数的文件放到HDFS中"><a href="#第一步：将要统计单词个数的文件放到HDFS中" class="headerlink" title="第一步：将要统计单词个数的文件放到HDFS中"></a>第一步：将要统计单词个数的文件放到HDFS中</h3><p>例如我们将hadoop安装目录下的<code>README.txt</code>文件放到HDFS中的<code>/</code>目录下，在<code>master</code>节点上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/hadoop-2.7.3/README.txt /</span><br></pre></td></tr></table></figure></p>
<p><code>README.txt</code>文件的内容如下：</p>
<pre><code>For the latest information about Hadoop, please visit our website at:

   http://hadoop.apache.org/core/

and our wiki, at:

   http://wiki.apache.org/hadoop/

This distribution includes cryptographic software.  The country in 
which you currently reside may have restrictions on the import, 
possession, use, and/or re-export to another country, of 
encryption software.  BEFORE using any encryption software, please 
check your country&apos;s laws, regulations and policies concerning the
import, possession, or use, and re-export of encryption software, to 
see if this is permitted.  See &lt;http://www.wassenaar.org/&gt; for more
information.

The U.S. Government Department of Commerce, Bureau of Industry and
Security (BIS), has classified this software as Export Commodity 
Control Number (ECCN) 5D002.C.1, which includes information security
software using or performing cryptographic functions with asymmetric
algorithms.  The form and manner of this Apache Software Foundation
distribution makes it eligible for export under the License Exception
ENC Technology Software Unrestricted (TSU) exception (see the BIS 
Export Administration Regulations, Section 740.13) for both object 
code and source code.

The following provides more details on the included cryptographic
software:
  Hadoop Core uses the SSL libraries from the Jetty project written 
by mortbay.org.
</code></pre><h3 id="第二步：建立一个maven工程"><a href="#第二步：建立一个maven工程" class="headerlink" title="第二步：建立一个maven工程"></a>第二步：建立一个maven工程</h3><p>新建一个maven工程，目录结构如下：</p>
<p><img src="/img/hadoop-mapreduce-1.png" alt="" title="mapreduce工程项目结构"></p>
<p>其中，pom.xml内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hewentian<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop/<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.apache.org<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="第三步：编写mapper程序"><a href="#第三步：编写mapper程序" class="headerlink" title="第三步：编写mapper程序"></a>第三步：编写mapper程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.hadoop.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.MapReduceBase;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Reporter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;WordCountMapper&lt;/b&gt; 是</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018-12-18 23:06:02</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每次调用map方法会传入split中的一行数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key             该行数据在文件中的位置下标</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value           这行数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> outputCollector</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> reporter</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; outputCollector, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="keyword">if</span> (StringUtils.isNotBlank(line)) &#123;</span><br><span class="line">            StringTokenizer st = <span class="keyword">new</span> StringTokenizer(line);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (st.hasMoreTokens()) &#123;</span><br><span class="line">                String word = st.nextToken();</span><br><span class="line">                outputCollector.collect(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>)); <span class="comment">// map 的输出</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="第四步：编写reducer程序"><a href="#第四步：编写reducer程序" class="headerlink" title="第四步：编写reducer程序"></a>第四步：编写reducer程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.hadoop.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.MapReduceBase;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Reporter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;WordCountReducer&lt;/b&gt; 是</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018-12-18 23:47:12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; outputCollector, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (values.hasNext()) &#123;</span><br><span class="line">            sum += values.next().get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outputCollector.collect(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="第五步：编写job程序"><a href="#第五步：编写job程序" class="headerlink" title="第五步：编写job程序"></a>第五步：编写job程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.hadoop.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.JobClient;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.JobConf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;WordCountJob&lt;/b&gt; 是</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018-12-19 09:05:18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountJob</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"need: input file and output dir."</span>);</span><br><span class="line">            System.out.println(<span class="string">"eg: &#123;HADOOP_HOME&#125;/bin/hadoop jar /home/hadoop/wordCount.jar /README.txt /output/wc/"</span>);</span><br><span class="line">            System.exit(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        JobConf jobConf = <span class="keyword">new</span> JobConf(WordCountJob.class);</span><br><span class="line">        jobConf.setJobName(<span class="string">"word count mapreduce demo"</span>);</span><br><span class="line"></span><br><span class="line">        jobConf.setMapperClass(WordCountMapper.class);</span><br><span class="line">        jobConf.setReducerClass(WordCountReducer.class);</span><br><span class="line">        jobConf.setOutputKeyClass(Text.class);</span><br><span class="line">        jobConf.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// mapreduce 输入数据所在的目录或文件</span></span><br><span class="line">        FileInputFormat.addInputPath(jobConf, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">// mr执行之后的输出数据的目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(jobConf, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        JobClient.runJob(jobConf);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="第六步：将程序打包成JAR文件"><a href="#第六步：将程序打包成JAR文件" class="headerlink" title="第六步：将程序打包成JAR文件"></a>第六步：将程序打包成JAR文件</h3><p>将上述工程打包成JAR文件，并设置默认运行的类为<code>WordCountJob</code>，打包后得文件<code>wordCount.jar</code>，我们将它上传到<code>master</code>节点的<code>home</code>目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scp wordCount.jar hadoop@hadoop-host-master:~/</span><br></pre></td></tr></table></figure></p>
<h3 id="第七步：登录master节点执行JAR文件"><a href="#第七步：登录master节点执行JAR文件" class="headerlink" title="第七步：登录master节点执行JAR文件"></a>第七步：登录master节点执行JAR文件</h3><p>登录master节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>执行JAR文件，若指定的输出目录不存在，HDFS会自动创建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hadoop jar /home/hadoop/wordCount.jar /README.txt /output/wc/</span><br></pre></td></tr></table></figure></p>
<p>执行过程中部分输出如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">18/12/07 02:48:57 INFO client.RMProxy: Connecting to ResourceManager at hadoop-host-master/192.168.56.110:8032</span><br><span class="line">18/12/07 02:48:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop-host-master/192.168.56.110:8032</span><br><span class="line">18/12/07 02:48:58 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.</span><br><span class="line">18/12/07 02:49:00 INFO mapred.FileInputFormat: Total input paths to process : 1</span><br><span class="line"></span><br><span class="line">18/12/07 02:49:00 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">18/12/07 02:49:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544066791145_0005</span><br><span class="line">18/12/07 02:49:00 INFO impl.YarnClientImpl: Submitted application application_1544066791145_0005</span><br><span class="line">18/12/07 02:49:01 INFO mapreduce.Job: The url to track the job: http://hadoop-host-master:8088/proxy/application_1544066791145_0005/</span><br><span class="line">18/12/07 02:49:01 INFO mapreduce.Job: Running job: job_1544066791145_0005</span><br><span class="line">18/12/07 02:49:10 INFO mapreduce.Job: Job job_1544066791145_0005 running in uber mode : false</span><br><span class="line">18/12/07 02:49:10 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/12/07 02:49:20 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/12/07 02:49:27 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">18/12/07 02:49:28 INFO mapreduce.Job: Job job_1544066791145_0005 completed successfully</span><br><span class="line">18/12/07 02:49:28 INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=2419</span><br><span class="line">		FILE: Number of bytes written=360364</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=2235</span><br><span class="line">		HDFS: Number of bytes written=1306</span><br><span class="line">		HDFS: Number of read operations=9</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=2</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=2</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=16581</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=4407</span><br><span class="line">		Total time spent by all map tasks (ms)=16581</span><br><span class="line">		Total time spent by all reduce tasks (ms)=4407</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=16581</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=4407</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=16978944</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=4512768</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=31</span><br><span class="line">		Map output records=179</span><br><span class="line">		Map output bytes=2055</span><br><span class="line">		Map output materialized bytes=2425</span><br><span class="line">		Input split bytes=186</span><br><span class="line">		Combine input records=0</span><br><span class="line">		Combine output records=0</span><br><span class="line">		Reduce input groups=131</span><br><span class="line">		Reduce shuffle bytes=2425</span><br><span class="line">		Reduce input records=179</span><br><span class="line">		Reduce output records=131</span><br><span class="line">		Spilled Records=358</span><br><span class="line">		Shuffled Maps =2</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=2</span><br><span class="line">		GC time elapsed (ms)=364</span><br><span class="line">		CPU time spent (ms)=1510</span><br><span class="line">		Physical memory (bytes) snapshot=480575488</span><br><span class="line">		Virtual memory (bytes) snapshot=5843423232</span><br><span class="line">		Total committed heap usage (bytes)=262725632</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=2049</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=1306</span><br></pre></td></tr></table></figure></p>
<p>等执行成功后，在<code>master</code>节点上查看结果（部分）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -cat /output/wc/*</span><br><span class="line">(BIS),	1</span><br><span class="line">(ECCN)	1</span><br><span class="line">(TSU)	1</span><br><span class="line">(see	1</span><br><span class="line">5D002.C.1,	1</span><br><span class="line">740.13)	1</span><br><span class="line">&lt;http://www.wassenaar.org/&gt;	1</span><br><span class="line">Administration	1</span><br><span class="line">Apache	1</span><br><span class="line">BEFORE	1</span><br><span class="line">BIS	1</span><br><span class="line">Bureau	1</span><br><span class="line">Commerce,	1</span><br><span class="line">Commodity	1</span><br><span class="line">Control	1</span><br><span class="line">Core	1</span><br><span class="line">Department	1</span><br><span class="line">ENC	1</span><br><span class="line">Exception	1</span><br><span class="line">Export	2</span><br><span class="line">For	1</span><br><span class="line">Foundation	1</span><br></pre></td></tr></table></figure></p>
<h3 id="我们在浏览器中查看HDFS和YARN中的数据"><a href="#我们在浏览器中查看HDFS和YARN中的数据" class="headerlink" title="我们在浏览器中查看HDFS和YARN中的数据"></a>我们在浏览器中查看HDFS和YARN中的数据</h3><p>在HDFS管理器中查看：</p>
<p><img src="/img/hadoop-mapreduce-2.png" alt="" title="mapreduce的结果在HDFS中"></p>
<p>在YARN管理器中查看：</p>
<p><img src="/img/hadoop-mapreduce-3.png" alt="" title="mapreduce在YARN中的记录"></p>
<p>大功告成！！！ <strong> （hadoop集群中的时间与我本机的时间不一致，毕竟，很久没启动集群了） </strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/12/19/hadoop-mapreduce/" data-id="cjz3s88qg000tqc3kgrou5fap" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/hadoop-cluster/" class="article-date">
  <time datetime="2018-12-04T01:12:43.000Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说hadoop集群的搭建，这里说的集群是真集群，不是伪集群。不过，这里的真集群是在虚拟机环境中搭建的。</p>
<p>在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装三台服务器：master、slave1、slave2来搭建hadoop集群。安装好VirtualBox后，启动它。依次点<code>File -&gt; Host Network Manager -&gt; Create</code>，来创建一个网络和虚拟机中的机器通讯，这个地址是：<code>192.168.56.1</code>，也就是我们外面实体机的地址（仅和虚拟机中的机器通讯使用）。如下图：</p>
<p><img src="/img/hadoop-1.png" alt="" title="虚拟机网络配置"></p>
<p>我们使用<code>ubuntu 18.04</code>来作为我们的服务器，先在虚拟机中安装好一台服务器master，将Jdk、hadoop在上面安装好，然后将master克隆出slave1、slave2。以master为namenode节点，slave1、slave2作为datanode节点。相关配置如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
</code></pre><h3 id="下面开始master的安装"><a href="#下面开始master的安装" class="headerlink" title="下面开始master的安装"></a>下面开始master的安装</h3><p>在虚拟机中安装<code>master</code>的过程中我们会设置一个用户用于登录，我们将用户名、密码都设为<code>hadoop</code>，当然也可以为其他名字，其他安装过程略。安装好之后，使用默认的网关配置NAT，NAT可以访问外网，我们将<code>jdk-8u102-linux-x64.tar.gz</code>和<a href="http://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/" target="_blank" rel="noopener">hadoop-2.7.3.tar.gz</a>从它们的官网下载到用户的<code>/home/hadoop/</code>目录下。或在实体机中通过SCP命令传进去。然后将网关设置为<code>Host-only Adapter</code>，如下图所示。</p>
<p><img src="/img/hadoop-2.png" alt="" title="网络配置"></p>
<p>网关设置好了之后，我们接下来配置IP地址。在<code>master</code>中<code>[Settings] -&gt; [Network] -&gt; [Wired 这里打开] -&gt; [IPv4]</code>按如下设置：</p>
<p><img src="/img/hadoop-3.png" alt="" title="网络配置"></p>
<h3 id="管理集群"><a href="#管理集群" class="headerlink" title="管理集群"></a>管理集群</h3><p>在上面的IP等配置好之后，我们选择关闭master，注意不是直接关闭，而是在关闭的时候选择<code>Save the machine state</code>。然后在虚拟机中选中<code>master -&gt; Start 下拉箭头 -&gt; Headless start</code>，然后在我们实体机中通过ssh直接登录到master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@192.168.56.110</span><br></pre></td></tr></table></figure></p>
<p>我们可以在实体机通过配置<code>/etc/hosts</code>，加上如下配置：</p>
<pre><code>192.168.56.110    hadoop-host-master
</code></pre><p>然后就可以通过如下方式登录了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>在实体机中通过下面的配置，就可以无密码登录了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p><strong> 下面的操作，均是在实体机中通过SSH到虚拟机执行的操作。 </strong></p>
<h3 id="安装ssh-openssh-rsync"><a href="#安装ssh-openssh-rsync" class="headerlink" title="安装ssh openssh rsync"></a>安装ssh openssh rsync</h3><p>如系统已安装，则勿略下面的安装操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh openssh-server rsync</span><br></pre></td></tr></table></figure></p>
<p>如果上述命令无法执行，请先执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>JDK的安装请参考我之前的笔记：<a href="../../../../2017/12/08/install-jdk/">安装 JDK</a>，这里不再赘述。安装到此目录<code>/usr/local/jdk1.8.0_102/</code>下，记住此路径，下面会用到。下在进行hadoop的安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xf hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压后得到hadoop-2.7.3目录，hadoop的程序和相关配置就在此目录中。</p>
<h3 id="建保存数据的目录"><a href="#建保存数据的目录" class="headerlink" title="建保存数据的目录"></a>建保存数据的目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p hdfs/tmp</span><br><span class="line">$ mkdir -p hdfs/name</span><br><span class="line">$ mkdir -p hdfs/data</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 hdfs/</span><br></pre></td></tr></table></figure>
<h3 id="配置文件浏览"><a href="#配置文件浏览" class="headerlink" title="配置文件浏览"></a>配置文件浏览</h3><p>hadoop的配置文件都位于下面的目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop</span><br><span class="line">$ ls </span><br><span class="line">capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh</span><br><span class="line">configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template</span><br><span class="line">container-executor.cfg      httpfs-signature.secret  mapred-site.xml.template</span><br><span class="line">core-site.xml               httpfs-site.xml          slaves</span><br><span class="line">hadoop-env.cmd              kms-acls.xml             ssl-client.xml.example</span><br><span class="line">hadoop-env.sh               kms-env.sh               ssl-server.xml.example</span><br><span class="line">hadoop-metrics2.properties  kms-log4j.properties     yarn-env.cmd</span><br><span class="line">hadoop-metrics.properties   kms-site.xml             yarn-env.sh</span><br><span class="line">hadoop-policy.xml           log4j.properties         yarn-site.xml</span><br><span class="line">hdfs-site.xml               mapred-env.cmd</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hadoop-env-sh，加上JDK绝对路径"><a href="#配置hadoop-env-sh，加上JDK绝对路径" class="headerlink" title="配置hadoop-env.sh，加上JDK绝对路径"></a>配置hadoop-env.sh，加上JDK绝对路径</h3><p>JDK的路径就是上面安装JDK的时候的路径：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_102/</span><br></pre></td></tr></table></figure></p>
<h3 id="配置core-site-xml，在该文件中加入如下内容"><a href="#配置core-site-xml，在该文件中加入如下内容" class="headerlink" title="配置core-site.xml，在该文件中加入如下内容"></a>配置core-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-host-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hdfs-site-xml，在该文件中加入如下内容"><a href="#配置hdfs-site-xml，在该文件中加入如下内容" class="headerlink" title="配置hdfs-site.xml，在该文件中加入如下内容"></a>配置hdfs-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>至此，master中要安装的通用环境配置完成。在虚拟机中将master复制出slave1、slave2。并参考上面配置IP地址的方法将slave1的ip配置为:<code>192.168.56.111</code>，slave2的ip配置为：<code>192.168.56.112</code>。</p>
<h3 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h3><p>配置master的主机名为<code>hadoop-host-master</code>，在master节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>配置slave1的主机名为<code>hadoop-host-slave-1</code>，在slave1节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<p>配置slave2的主机名为<code>hadoop-host-slave-2</code>，在slave2节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p><strong> 注意：各个节点的主机名一定要不同，否则相同主机名的节点，只会有一个连得上namenode节点，并且集群会报错，修改主机名后，要重启才生效。 </strong></p>
<h3 id="配置域名解析"><a href="#配置域名解析" class="headerlink" title="配置域名解析"></a>配置域名解析</h3><p>分别对master、slave1和slave2都执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">127.0.0.1	localhost</span><br><span class="line">192.168.56.110	hadoop-host-master</span><br><span class="line">192.168.56.111	hadoop-host-slave-1</span><br><span class="line">192.168.56.112	hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>至此，集群配置完成，下面将启动集群。</p>
<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><p>首先启动namenode节点，也就是master，首次启动的时候，要格式化namenode。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -format    <span class="comment"># 再次启动的时候不需要执行此操作</span></span><br><span class="line">$ ./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 NameNode</span><br></pre></td></tr></table></figure></p>
<p>接下来启动datanode节点，也就是slave1、slave2，在这两台服务器上都执行如下启动脚本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh start datanode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">2451 Jps</span><br><span class="line">2162 DataNode</span><br></pre></td></tr></table></figure></p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:50070/" target="_blank" rel="noopener">http://hadoop-host-master:50070/</a><br>来查看是否启动成功。</p>
<p><img src="/img/hadoop-4.png" alt="" title="hadoop管理界面：Overview"></p>
<p>切换tab到Datanodes可以看到有2个datanode节点，如下图所示：</p>
<p><img src="/img/hadoop-5.png" alt="" title="hadoop管理界面:Datanodes"></p>
<p>切换到<code>Utilities -&gt; Browse the file system</code>，如下图所示：</p>
<p><img src="/img/hadoop-6.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>从上面的界面可以，目前HDFS中没有任何文件。我们尝试往其中放一个文件，就将我们的hadoop压缩包放进去，在namenode节点中执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/hadoop-2.7.3.tar.gz /</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup  214092195 2018-12-06 12:20 /hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>我们在图形界面中查看，如下图：</p>
<p><img src="/img/hadoop-7.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>我们点击列表中的文件，将会显示它的数据具体分布在哪些节点上，如下图：</p>
<p><img src="/img/hadoop-8.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<h3 id="停掉集群"><a href="#停掉集群" class="headerlink" title="停掉集群"></a>停掉集群</h3><p>先停掉datanode节点，在slave1、slave2上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure></p>
<p>然后停掉namenode节点，在master上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh stop namenode</span><br></pre></td></tr></table></figure></p>
<h3 id="集中式管理集群"><a href="#集中式管理集群" class="headerlink" title="集中式管理集群"></a>集中式管理集群</h3><p>如果我们的集群里面有成千上万台机器，在每一台机器上面都这样来启动，肯定是不行的。下面我们将通过配置，只在一台机器上面执行一个脚本，就将整个集群启动。</p>
<p>配置SSH无密码登陆，分别在master、slave1和slave2上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>在master上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop-host-master</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-1</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>每执行一条命令的时候，都先输入yes，然后再输入目标机器的登录密码。</p>
<p>如果能成功运行如下命令，则配置免密登录其他机器成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop-host-master</span><br><span class="line">$ ssh hadoop-host-slave-1</span><br><span class="line">$ ssh hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>在master上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi slaves    <span class="comment"># 加入如下内容</span></span><br><span class="line">$</span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>当执行<code>start-dfs.sh</code>时，它会去slaves文件中找从节点。</p>
<p>在master上面启动整个集群：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面通过<code>jps</code>命令可以看到整个集群已经成功启动。同样的，停掉整个集群的命令，如下，同样是在master上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>相关操作，如下图所示：</p>
<p><img src="/img/hadoop-9.png" alt="" title="hadoop集中式管理"></p>
<p><strong> 注意：在主节点执行<code>start-dfs.sh</code>，主节点的用户名必须和所有从节点的用户名相同。因为主节点服务器以这个用户名去远程登录到其他从节点的服务器中，所以在所有的生产环境中控制同一类集群的用户一定要相同。 </strong></p>
<h3 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h3><p>分别在master、slave1和slave2上面都建如下目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p yarn/nm</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 yarn/</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面都按如下方式配置mapred-site.xml，刚解压的hadoop是没有mapred-site.xml的，但是有mapred-site.xml.template，我们修改文件名，并作如下配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ mv mapred-site.xml.template mapred-site.xml</span><br><span class="line">$ vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面都按如下方式配置yarn-site.xml<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定ResourceManager的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop-host-master&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定reducer获取数据的方式 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/home/hadoop/hadoop-2.7.3/yarn/nm&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>配置好了，下面开始启动：<br>在master上面启动resourcemanager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure></p>
<p>在slave1、slave2上面分别启动nodemanager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure></p>
<p>我们可以通过浏览器，查看资源管理器：<br><a href="http://hadoop-host-master:8088/" target="_blank" rel="noopener">http://hadoop-host-master:8088/</a></p>
<p><img src="/img/hadoop-10.png" alt="" title="yarn资源管理"></p>
<p>点击图中的<code>Active Nodes</code>可以看到下图的详情，（如果<code>Unhealthy Nodes</code>有节点，则可能是由于虚拟机中主机的磁盘空间不足所致）。</p>
<p><img src="/img/hadoop-11.png" alt="" title="yarn资源管理"></p>
<p>当然相应的停止命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/yarn-daemon.sh stop resourcemanager</span><br><span class="line">$ ./sbin/yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure></p>
<p>如果有配置集中式管理，我们也可以通过在master上面通过一个命令启动、停止YARN<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-yarn.sh    <span class="comment"># 启动yarn</span></span><br><span class="line">$ ./sbin/stop-yarn.sh    <span class="comment"># 停止yarn</span></span><br></pre></td></tr></table></figure></p>
<p>或者在master上面，通过一个命令启动hadoop和yarn<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-all.sh</span><br><span class="line">或者按顺序执行如下两个命令</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br><span class="line">$ ./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hadoop-12.png" alt="" title="yarn集中启动管理"></p>
<h3 id="启动MR作业日志管理器"><a href="#启动MR作业日志管理器" class="headerlink" title="启动MR作业日志管理器"></a>启动MR作业日志管理器</h3><p>在namenode节点，也就是master，启动MR作业日志管理器。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></p>
<p>它同样有自已的图形界面：<br><a href="http://hadoop-host-master:19888/" target="_blank" rel="noopener">http://hadoop-host-master:19888/</a></p>
<p><img src="/img/hadoop-13.png" alt="" title="MR作业日志管理器"></p>
<h3 id="尝试向集群中提交一个mapReduce任务"><a href="#尝试向集群中提交一个mapReduce任务" class="headerlink" title="尝试向集群中提交一个mapReduce任务"></a>尝试向集群中提交一个mapReduce任务</h3><p>我们在namenode节点中向集群提交一个计算圆周率的mapReduce任务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 4 10</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hadoop-14.png" alt="" title="计算圆周率"></p>
<p><img src="/img/hadoop-15.png" alt="" title="计算圆周率"></p>
<p>从上图可以看出，圆周率已经被计算出来：<code>3.40</code>。另外，在yarn中也可以看到任务的执行情况：<br><img src="/img/hadoop-16.png" alt="" title="计算圆周率"></p>
<p>至此， 集群搭建完毕。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/12/04/hadoop-cluster/" data-id="cjz3s88qf000qqc3k3e3xjlsi" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-kafka-intro" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/12/kafka-intro/" class="article-date">
  <time datetime="2018-11-12T01:04:49.000Z" itemprop="datePublished">2018-11-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/12/kafka-intro/">kafka 介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这是一篇译文，因英文水平有限，翻译未免有不足之处。如果想看原文，请访问这里：<br><a href="http://kafka.apache.org/intro" target="_blank" rel="noopener">http://kafka.apache.org/intro</a></p>
<p><strong> 如果想简单体验一下kafka，可以阅读我上两篇介绍的 <a href="../../../../2018/10/24/kafka-standalone/">kafka 单节点安装</a>、<a href="../../../../2018/10/27/kafka-cluster/">kafka 集群的搭建</a> </strong></p>
<h3 id="Apache-Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？"><a href="#Apache-Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？" class="headerlink" title="Apache Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？"></a>Apache Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？</h3><p>流媒体平台有3个主要的性能指标：</p>
<ol>
<li>发布和订阅消息流，类似于消息队列或者企业消息系统；</li>
<li>以容错方式、持久化存储流数据；</li>
<li>实时处理流数据。</li>
</ol>
<p>kafka通常应用于两种广泛的场景：</p>
<ol>
<li>在系统或应用程序之间构建可靠的用于传输实时数据的管道; </li>
<li>构建实时的流数据处理程序，来转换或处理数据流。</li>
</ol>
<h3 id="为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能"><a href="#为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能" class="headerlink" title="为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能"></a>为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能</h3><p>首先，了解一下几个概念：</p>
<ol>
<li>kafka可以以集群方式运行于一台或者多台服务器，这些服务器可以分布在不同的数据中心；</li>
<li>kafka集群将流式数据分类存储，这种类别通常被称为主题；</li>
<li>每一条消息由键、值和时间戳组成。</li>
</ol>
<p><img src="/img/kafka-1.png" alt="" title="来源：http://kafka.apache.org/20/images/kafka-apis.png"></p>
<p>kafka有4个核心API：</p>
<ol>
<li><a href="http://kafka.apache.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a>允许应用程序将一条消息发布到一个或者多个kafka主题中；</li>
<li><a href="http://kafka.apache.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>允许应用程序订阅一个或者多个主题，并且处理被发往其中的数据流；</li>
<li><a href="http://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>允许一个应用充当流式处理器：从作为输入流的一个或多个主题中消费消息，然后将处理过的消息输出到另一个或多个主题中。高效地将输入流的数据转换、传输到输出流中；</li>
<li><a href="http://kafka.apache.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>允许建立和重用已有的生产者或消费者，它们连接着某个kafka主题，而这些主题是和已存在的应用和数据系统连接着的。例如，关系数据库的连接器将会捕获对表的每一个修改。</li>
</ol>
<p>在kafka中，客户端和服务器端的通讯是通过一个简单、高效、语言无关的<a href="https://kafka.apache.org/protocol.html" target="_blank" rel="noopener">TCP协议</a>完成的。此协议是有版本代差的，但新版本向后兼容旧版本。我们提供一个JAVA客户端连接kafka，但是其他语言的客户端也提供。消费者和服务端建立的是长连接。</p>
<h3 id="主题和日志（存储策略）"><a href="#主题和日志（存储策略）" class="headerlink" title="主题和日志（存储策略）"></a>主题和日志（存储策略）</h3><p>我们首先钻研一下kafka中为处理流记录而提供的核心抽象概念–主题。</p>
<p>主题就是一个分类，或者说是专为发布消息而命名的。在kafka中主题通常有多个订阅者，也就是说一个主题可以有零个、一个或者多个消费者，这些消费者都订阅写往其中的消息。</p>
<p>对每一个主题，kafka集群都使用分区存储，像下面这样：<br><img src="/img/kafka-2.png" alt="" title="来源：http://kafka.apache.org/20/images/log_anatomy.png"></p>
<p>每一个分区中的消息都是按顺序存储的，持续往该分区中存放的数据的顺序都是不可改变的，结构化存储。分区中的每一条消息都会被分配一个有序的ID号，被称为偏移量，用于唯一标示该分区中的每一条消息。</p>
<p>kafka集群会持久化发布到它的每一条消息，无论它们是否已经被消费过，可以通过配置文件配置该消息存放多久。例如，如果保存策略被设置为2天，那么当一条消息发布2天之内，它都是可以被消费的，只是一旦被消费之后，它就会被删掉以释放空间。kafka是持续高性能的，这与存储于它的数据大小关系不大，因此长期保存数据，都是没问题的。</p>
<p><img src="/img/kafka-3.png" alt="" title="来源：http://kafka.apache.org/20/images/log_consumer.png"></p>
<p>实际上，唯一存储于每一个消费者中的元数据是偏移量或者该消费者在这个分区中访问存储数据的位置。偏移量由消费者控制：通常，消费者中保存的偏移量随着它消费消息，将呈线性增长。但是，实际上，由于这个偏移量是由消费者控制的，所以它可以指定它消费的任何位置上的消息。例如：一个消费者可以重置到一个旧的偏移量来处理旧的数据或者跳过大部分记录，然后从当前位置开始消费消息。每个消费者的偏移量在kafka服务器中也是有存储的。</p>
<p>这个组合功能意味着kafka消费者是轻量级的，它们的连接和断开对集群和其他消费者影响极小。例如，你可以使用我们的命令行工具去不停地显示最新添加到某个主题的内容，而这，不会对任何订阅这个主题的消费者产生影响。</p>
<p>分区在存储中扮演着不同的目的：首先，它允许存储的数据量超过单台服务器允许的规模。每个单独的分区能储存的数据量取决于它所在的服务器磁盘的大小等因素，但是一个主题可以有多个分区，因此它能储存任意数量的数据。其次，分区充当并行处理的单元–同时能处理的并发数。</p>
<h3 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h3><p>一个主题的分区分布于kafka集群中的多台服务器中，每一台服务器都可以处理数据和向共享分区发送请求。为了容错，每一个分区都可以配置一定的副本数。</p>
<p>每一个分区都有一台服务器担当主服务器，可以有零个或者多个从服务器。主服务器处理对分区的所有读和写请求，从服务器由主服务器调度。如果主服务器挂掉了，从服务器中会自动产生一个新的主服务器。集群中的每一台服务器既充当某个分区的主服务器，又充当其他分区的从服务器，所以整个集群是负载均衡的。</p>
<h3 id="地域复制"><a href="#地域复制" class="headerlink" title="地域复制"></a>地域复制</h3><p>kafka的MirrorMaker为你的集群提供跨地域复制支持。使用MirrorMaker，消息可以跨越多个数据中心或者不同的云区进行同步。你可以在主从模式下用于备份和恢复，或者在主主模式下使数据更靠近你的用户，或者支持数据本地请求。</p>
<h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>生产者往它们选定的主题中发送消息的时候，应该为每一条消息指定它要发送到的分区。我们可以使用环形策略，简单地使数据平均分配于所有分区中，也可以根据消息中的语义来自动地选择分区。接下来将会说下分区的使用。</p>
<h3 id="消息者"><a href="#消息者" class="headerlink" title="消息者"></a>消息者</h3><p>我们可以对消费者分组，每个组有一个组名。每一条发送到指定主题中的消息，都会被订阅了这个主题的同一个组中的一个消费者消费。同一个组中的消费者可以在同一台机器或者多台机器中。</p>
<p>如果所有消费者实例都在同一个组中，那么所有消息都会高效地平均发送到所有消费者实例。<br>如果所有消费者实例分布在不同的组中，那么每条消息都会被广播到所有组中的一个消费者。</p>
<p><img src="/img/kafka-4.png" alt="" title="来源：http://kafka.apache.org/20/images/consumer-groups.png"></p>
<p>上图所示：该kafka集群有2台服务器，4个分区（P0-P3），有2个消费者组。消费者组A有2个消费者实例，而消费者组B有4个。</p>
<p>通常，主题都会有少量的消费者组，在逻辑上看，一个消费者组就是一个订阅者。每个组包含多个消费者，这能很好的实现扩展和容错。在订阅的语义上：订阅者只不过是一群消费者，而不是一个。</p>
<p>消费的方式在kafka中的实现是通过将分区分配给所有消费者实例，因此在任何时刻，每一个实例都是一个”公平共享“分区的唯一消费者。维护组中成员关系的方式在kafak中是通过kafka协议自动实现的：如果新的实例加进组，那么它将从其他组员中获取一个分区（如果这个组员处理两个以上分区）；如果一个实例挂掉了，那么它所处理的分区将被分配给组中剩下的成员们。</p>
<p>kafka对每一个分区中的消息都只提供一个总的顺序，同一个主题中不同分区中的顺序各不相同。每个分区排序组织该分区中数据的能力能满足大部分应用的需求。但是，如果你想要一个所有消息的总顺序，可以通过为这个主题设置一个分区来实现，不过，这意味着一个消费者组中只能有一个消费者来处理该主题的消息。</p>
<h3 id="多租户架构"><a href="#多租户架构" class="headerlink" title="多租户架构"></a>多租户架构</h3><p>你可以以多租户架构方式部署kafka。多租户架构可以通过配置，来指定哪个主题可以生产和消费数据，并且支持设置操作指标。管理员可以定义和限制所有请求的指标，以控制客户端能使用的服务器资源数。</p>
<p>更多相关信息，请访问<a href="https://kafka.apache.org/documentation/#security" target="_blank" rel="noopener">这里</a>。</p>
<h3 id="保证"><a href="#保证" class="headerlink" title="保证"></a>保证</h3><p>在高层次看kafka提供以下保证：</p>
<ol>
<li>一个生产者发往指定主题中指定分区中的消息，将会按照它们发送的顺序出现。例如：如果一个生产者发送消息M1、M2，如果M1先发送，那么M1将会首先出现在那个分区中，并且M1的顺序号要比M2的小；</li>
<li>消费者按顺序读取存储在主题中的消息；</li>
<li>如果一个主题有 N 个副本，那么我们能承受高达 N-1 台服务器同时挂掉，而不会丢失任何消息。</li>
</ol>
<p>更多关于这些保证的描述将在相关章节中详细说明。</p>
<h3 id="kafka作为一个消息系统"><a href="#kafka作为一个消息系统" class="headerlink" title="kafka作为一个消息系统"></a>kafka作为一个消息系统</h3><p>kafka的流媒体概念与传统的企业消息系统相比有什么不同？</p>
<p>消息传输在传统上有2种模型：队列和发布-订阅。在队列模型中，一组消费者从一台服务器读取消息，每个消息只会发送到其中一个消费者；在发布-订阅模型中，每条消息都会广播到所有消费者。这两种模型各自都有优势和不足。队列的优势是允许你将消息平均分配给所有消费者处理，这能扩展系统的处理能力。不幸的是，队列不能有多个订阅者，消息一旦被其中一个消费者读取就会被删掉。发布-订阅模型允许你将消息广播到所有消费者，这种方式不能扩展处理能力，因为每条消息都会被发送到所有订阅者。</p>
<p>消费者组的概念在kafka中通常包含上述两种概念。对队列来说，消费者组允许你将数据平均分配给所有消费者组来处理；对发布-订阅来说，kafka允许你将消息广播到所有消费者组。</p>
<p>kafka模型的优势是每个主题都有这两种属性：它能扩展处理能力，同时也支持多个订阅者。我们不需要选择其中一个，或者另外一个。</p>
<p>kafka相比于传统的消息系统，它更能保证消息的顺序。</p>
<p>传统队列会将消息按顺序保存在服务器上，如果多个消费者同时消费这个队列，那么服务器将按消息的存储顺序来分发给消费者。然而，尽管服务器按顺序分发消息，但是消息是异步的发送到每个消费者的，所以不同的消费者接收到消息的顺序可能不同。这意味着，在并行处理的情况下，消息的顺序将不能保证。在消息系统中通常有一个概念：唯一消费者，它允许只有一个消费者消费一个队列，当然这也意味着这种情况下不存在并行处理。</p>
<p>kafka在这方面做得比较好。在主题中，它有一个并行的概念–分区。kafka既能保证消息的顺序，又能在多个消费者之间保持负载均衡。通过将主题的分区分配给指定的消费者组，每个分区只能被消费者组中的一个消费者消费，来实现的。这样，我们能确保这个消费者是这个分区的唯一消费者和按顺序消费这个分区中的消息。尽管主题有很多个分区，我们仍能在多个消费者实例之间保持负载均衡。值得注意的是，消费者组中消费者的数量不能多于分区数。</p>
<h3 id="kafka作为一个存储系统"><a href="#kafka作为一个存储系统" class="headerlink" title="kafka作为一个存储系统"></a>kafka作为一个存储系统</h3><p>任何允许发布与消费消息分离的消息队列，实际上充当了目前使用的消息存储系统。kafka的不同之处在于它还是一个非常优秀的存储系统。</p>
<p>写入kafka的数据将会写入磁盘，并且进行副本复制以实现容错。kafka允许生产者等待确认，在收到回复之后才会认为写成功，并且即使写入的服务器失败了，也能保证这条消息是存在的。</p>
<p>kafka能很好地使用磁盘结构来扩容：无论服务器上有 50KB 还是 50TB 的持久化数据，kafka的性能都是一样的，不会随着数据的增多而出现性能下降。</p>
<p>由于kafka可以大规模的存储数据，并且允许客户控制其读取位置，您可以将kafka作为一种专用于高性能、低延迟提交日志存储，并且能复制和传播的分布式文件系统。</p>
<p>更多关于kafka的提交日志存储和副本复制的设计，请访问<a href="https://kafka.apache.org/documentation/#design" target="_blank" rel="noopener">这里</a>。</p>
<h3 id="kafka作为一个流媒体处理系统"><a href="#kafka作为一个流媒体处理系统" class="headerlink" title="kafka作为一个流媒体处理系统"></a>kafka作为一个流媒体处理系统</h3><p>kafka仅仅提供读取、写入和存储数据流是不够的，最终目的是实现流的实时处理。</p>
<p>在kafka中，流处理器是指持续地从输入主题获取数据流，对获取到的数据流执行某种处理，并将处理过的数据，持续地输出到输出主题中。</p>
<p>例如，零售店的应用程序可能会将销售额和货物作为输入流，通过相关计算，然后输出重新排序和根据此数据计算的价格调整的流。</p>
<p>可以直接使用生产者和消费者的相关API进行简单处理。但是，对于更复杂的转换，kafka提供了完全集成的Streams API。这允许构建一些应用程序去执行非普通处理任务、计算流的聚合或者将流连接在一起。</p>
<p>它能有效地解决此类应用程序面临的难题：处理无序数据，在代码更改时重新处理输入流，执行有状态计算等。</p>
<p>流式API构建在kafka提供的核心基础功能上：它使用生产者和消费者API进行输入，使用kafka进行有状态存储，并在流处理器实例之间使用相同的组机制来实现容错。</p>
<h3 id="把碎片整合在一起"><a href="#把碎片整合在一起" class="headerlink" title="把碎片整合在一起"></a>把碎片整合在一起</h3><p>将消息传递、存储和流处理组合在一起可能看起来没多大用处，但它对于kafka作为流媒体平台的作用至关重要。</p>
<p>像HDFS这种分布式文件系统允许存储静态文件以进行批处理。kafka系统是高效的，它允许存储和处理过去的历史数据。</p>
<p>传统的企业消息系统允许处理你订阅之后到达的数据。以这种方式构建的应用程序只能处理在它订阅之后到达的未来数据。</p>
<p>kafka结合了这两种功能，这种组合对于kafka作为流媒体应用程序平台以及流数据管道的使用至关重要。</p>
<p>通过组合存储和低延迟订阅，流应用程序可以以相同的方式处理过去和未来的数据。也就是说，单个应用程序也可以处理历史存储的数据，而不是在它处理到达最后一条记录时结束，它可以在未来数据到达时继续处理。这就是流处理包含批处理以及消息驱动应用程序的一般概念。</p>
<p>同样，对于流数据管道，通过组合订阅实时事件，可以将kafka用作极低延迟的管道; 另外，能够可靠地存储数据，也使得可以将其用于必须保证安全的核心数据的传输，或者与仅定期加载数据的离线系统或可能长时间停机以进行扩展和维护集成。它的流处理能力使它可以实时的转换数据。</p>
<p>更多关于kafka提供的保证、API和功能的信息，请参阅其余的<a href="http://kafka.apache.org/documentation.html" target="_blank" rel="noopener">文档</a>。</p>
<p>这里介绍一个很好用的kafka可视化工具kafkatool，下载地址：<br><a href="http://www.kafkatool.com/" target="_blank" rel="noopener">http://www.kafkatool.com/</a></p>
<p>kafkatool连接kafka服务器后，记得要将kafkatool中topic的Message类型设置为String，否则将看到字节码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/11/12/kafka-intro/" data-id="cjz3s88qt001yqc3kkr5e3aeo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-kafka-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/27/kafka-cluster/" class="article-date">
  <time datetime="2018-10-27T05:11:43.000Z" itemprop="datePublished">2018-10-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/27/kafka-cluster/">kafka 集群的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说kafka集群的搭建，如果你只是想简单体验一下kafka，可以直接使用我在上一篇介绍的 <a href="../../../../2018/10/24/kafka-standalone/">kafka 单节点安装</a> 即可。</p>
<p>但是，如果你想在生产环境中使用，那么搭建一个集群可能更适合你。下面将说说kafka集群的安装使用，kafka同样是使用前面例子使用的<code>2.0.0</code>版本，我在一台机器上安装，所以这是伪集群，当修改为真集群的时候，只要将IP地址修改下即可，下面会说明。</p>
<p>首先，你得搭建 zookeeper 集群，因为高版本的kafka中内置了zookeeper组件，所以我们直接使用kafka中内置的zookeeper组件搭建zookeeper集群。但是，你也可以使用zookeeper独立的安装包来搭建zookeeper集群。两者的搭建方法都是一样的，可以参考 <a href="../../../../2017/12/06/zookeeper-install-cluster/">zookeeper集群版安装方法</a></p>
<h3 id="计划在一台Ubuntu-Linux服务器上部署3台kafka服务器，分别为kafka1-kafka2-kafka3"><a href="#计划在一台Ubuntu-Linux服务器上部署3台kafka服务器，分别为kafka1-kafka2-kafka3" class="headerlink" title="计划在一台Ubuntu Linux服务器上部署3台kafka服务器，分别为kafka1, kafka2, kafka3"></a>计划在一台<code>Ubuntu Linux</code>服务器上部署3台<code>kafka</code>服务器，分别为<code>kafka1</code>, <code>kafka2</code>, <code>kafka3</code></h3><p>因为三台<code>kafka</code>服务器的配置都差不多，所以我们先设置好一台<code>kafka1</code>的配置，再将其复制成<code>kafka2</code>, <code>kafka3</code>并修改其中的配置即可。</p>
<p>下面使用kafka内置的zookeeper组件搭建zookeeper集群，我们将kafka的所有服务器都放在同一个目录下：</p>
<h3 id="1-建目录，如下："><a href="#1-建目录，如下：" class="headerlink" title="1.建目录，如下："></a>1.建目录，如下：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD</span><br><span class="line">$ mkdir kafkaCluster</span><br></pre></td></tr></table></figure>
<h3 id="2-将kafka-2-12-2-0-0-tgz放到-home-hewentian-ProjectD-kafkaCluster目录下，并执行如下脚本解压"><a href="#2-将kafka-2-12-2-0-0-tgz放到-home-hewentian-ProjectD-kafkaCluster目录下，并执行如下脚本解压" class="headerlink" title="2.将kafka_2.12-2.0.0.tgz放到/home/hewentian/ProjectD/kafkaCluster目录下，并执行如下脚本解压"></a>2.将<code>kafka_2.12-2.0.0.tgz</code>放到<code>/home/hewentian/ProjectD/kafkaCluster</code>目录下，并执行如下脚本解压</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster</span><br><span class="line">$ tar xzvf kafka_2.12-2.0.0.tgz</span><br><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">kafka_2.12-2.0.0  kafka_2.12-2.0.0.tgz</span><br><span class="line"></span><br><span class="line">$ rm kafka_2.12-2.0.0.tgz</span><br><span class="line">$ mv kafka_2.12-2.0.0/ kafka1  <span class="comment"># 为方便起见，将其命名为 kafka1</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> kafka1/</span><br><span class="line">$ mkdir -p data/zk     <span class="comment"># 存放zookeeper数据的目录</span></span><br><span class="line">$ mkdir -p data/kafka  <span class="comment"># 存放kafka数据的目录</span></span><br><span class="line">$ mkdir logs           <span class="comment"># 新解压的 kafka 没有此目录，需手动创建。因为重定向的日志logs/zookeeper.log需要此目录</span></span><br></pre></td></tr></table></figure>
<h3 id="3-修改-home-hewentian-ProjectD-kafkaCluster-kafka1-config-zookeeper-properties并在其中修改如下内容："><a href="#3-修改-home-hewentian-ProjectD-kafkaCluster-kafka1-config-zookeeper-properties并在其中修改如下内容：" class="headerlink" title="3.修改/home/hewentian/ProjectD/kafkaCluster/kafka1/config/zookeeper.properties并在其中修改如下内容："></a>3.修改<code>/home/hewentian/ProjectD/kafkaCluster/kafka1/config/zookeeper.properties</code>并在其中修改如下内容：</h3><pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk  # 这里必须为绝对路径，否则有可能无法启动
clientPort=2181                                               # 这台服务器的端口为2181这里为默认值
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890
</code></pre><h3 id="4-在-home-hewentian-ProjectD-kafkaCluster-kafka1-data-zk目录下建myid文件并在其中输入1，只输入1，代表server-1"><a href="#4-在-home-hewentian-ProjectD-kafkaCluster-kafka1-data-zk目录下建myid文件并在其中输入1，只输入1，代表server-1" class="headerlink" title="4.在/home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk目录下建myid文件并在其中输入1，只输入1，代表server.1"></a>4.在<code>/home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk</code>目录下建<code>myid</code>文件并在其中输入1，只输入1，代表server.1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk</span><br><span class="line">$ vi myid</span><br></pre></td></tr></table></figure>
<p>这样第一台服务器已经配置完毕。</p>
<h3 id="5-接下来我们将kafka1复制为kafka2-kafka3"><a href="#5-接下来我们将kafka1复制为kafka2-kafka3" class="headerlink" title="5.接下来我们将kafka1复制为kafka2, kafka3"></a>5.接下来我们将<code>kafka1</code>复制为<code>kafka2</code>, <code>kafka3</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster</span><br><span class="line">$ cp -r kafka1 kafka2</span><br><span class="line">$ cp -r kafka1 kafka3</span><br></pre></td></tr></table></figure>
<h3 id="6-将kafka2-data-zk目录下的myid的内容修改为2"><a href="#6-将kafka2-data-zk目录下的myid的内容修改为2" class="headerlink" title="6.将kafka2/data/zk目录下的myid的内容修改为2"></a>6.将<code>kafka2/data/zk</code>目录下的<code>myid</code>的内容修改为2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2/data/zk</span><br><span class="line">$ vi myid</span><br></pre></td></tr></table></figure>
<p>同理，将将<code>kafka3/data/zk</code>目录下的<code>myid</code>的内容修改为3<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3/data/zk</span><br><span class="line">$ vi myid</span><br></pre></td></tr></table></figure></p>
<h3 id="7-修改kafka2的配置文件"><a href="#7-修改kafka2的配置文件" class="headerlink" title="7.修改kafka2的配置文件"></a>7.修改<code>kafka2</code>的配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2/config</span><br><span class="line">$ vi zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>仅修改两处地方即可，要修改的地方如下：</p>
<pre><code>dataDir=/home/hewentian/ProjectD/kafkaCluster/kafka2/data/zk  # 这里是数据保存的位置
clientPort=2182                                               # 这台服务器的端口为2182
</code></pre><p>同理，修改<code>kafka3</code>的配置文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3/config</span><br><span class="line">$ vi zookeeper.properties</span><br></pre></td></tr></table></figure></p>
<p>仅修改两处地方即可，要修改的地方如下：</p>
<pre><code>dataDir=/home/hewentian/ProjectD/kafkaCluster/kafka3/data/zk  # 这里是数据保存的位置
clientPort=2183                                               # 这台服务器的端口为2183
</code></pre><h3 id="8-到目前为此，我们已经将3台zookeeper服务器都配置好了。接下来，我们要将他们都启动"><a href="#8-到目前为此，我们已经将3台zookeeper服务器都配置好了。接下来，我们要将他们都启动" class="headerlink" title="8.到目前为此，我们已经将3台zookeeper服务器都配置好了。接下来，我们要将他们都启动"></a>8.到目前为此，我们已经将3台<code>zookeeper</code>服务器都配置好了。接下来，我们要将他们都启动</h3><p>启动kafka1的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<p>启动kafka2的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ mkdir logs</span><br><span class="line">$ nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<p>启动kafka3的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ mkdir logs</span><br><span class="line">$ nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<h3 id="9-当三台服务器都启动好了，我们分别连到三台zookeeper服务器："><a href="#9-当三台服务器都启动好了，我们分别连到三台zookeeper服务器：" class="headerlink" title="9.当三台服务器都启动好了，我们分别连到三台zookeeper服务器："></a>9.当三台服务器都启动好了，我们分别连到三台zookeeper服务器：</h3><p>连接到kafka1的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/zookeeper-shell.sh 127.0.0.1:2181</span><br></pre></td></tr></table></figure></p>
<p>连接到kafka2的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/zookeeper-shell.sh 127.0.0.1:2182</span><br></pre></td></tr></table></figure></p>
<p>连接到kafka3的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/zookeeper-shell.sh 127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<p>可以通过查看<code>logs/zookeeper.log</code>文件，如果没有报错就说明zookeeper集群启动成功。</p>
<p>这样你在<code>kafka1</code>中的<code>zookeeper</code>所作的修改，都会同步到<code>kafka2</code>, <code>kafka3</code>。<br>例如你在kafka1的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ create /zk_test_cluster my_data_cluster</span><br></pre></td></tr></table></figure></p>
<p>你在kafka2, kafka3的zookeeper客户端用<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls /</span><br></pre></td></tr></table></figure></p>
<p>都会看到节点zk_test_cluster</p>
<p>至此，zookeeper集群部署结束。</p>
<h3 id="10-搭建kafka集群"><a href="#10-搭建kafka集群" class="headerlink" title="10.搭建kafka集群"></a>10.搭建kafka集群</h3><p>配置<code>kafka1</code>服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ vi config/server.properties</span><br><span class="line"></span><br><span class="line">broker.id=1                           <span class="comment"># 这里设置为1，另外两台分别设置为2、3</span></span><br><span class="line"></span><br><span class="line">listeners=PLAINTEXT://127.0.0.1:9092  <span class="comment"># IP地址和端口，这里使用默认的 9092，另外两台分别使用9093、9094</span></span><br><span class="line"></span><br><span class="line">log.dirs=/home/hewentian/ProjectD/kafkaCluster/kafka1/data/kafka</span><br><span class="line"></span><br><span class="line">zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<p>配置<code>kafka2</code>服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ vi config/server.properties</span><br><span class="line"></span><br><span class="line">broker.id=2</span><br><span class="line"></span><br><span class="line">listeners=PLAINTEXT://127.0.0.1:9093</span><br><span class="line"></span><br><span class="line">log.dirs=/home/hewentian/ProjectD/kafkaCluster/kafka2/data/kafka</span><br><span class="line"></span><br><span class="line">zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<p>配置<code>kafka3</code>服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ vi config/server.properties</span><br><span class="line"></span><br><span class="line">broker.id=3</span><br><span class="line"></span><br><span class="line">listeners=PLAINTEXT://127.0.0.1:9094</span><br><span class="line"></span><br><span class="line">log.dirs=/home/hewentian/ProjectD/kafkaCluster/kafka3/data/kafka</span><br><span class="line"></span><br><span class="line">zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<h3 id="11-启动三台kafka服务器"><a href="#11-启动三台kafka服务器" class="headerlink" title="11.启动三台kafka服务器"></a>11.启动三台kafka服务器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/kafka-server-start.sh -daemon /home/hewentian/ProjectD/kafkaCluster/kafka1/config/server.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/kafka-server-start.sh -daemon /home/hewentian/ProjectD/kafkaCluster/kafka2/config/server.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-server-start.sh -daemon /home/hewentian/ProjectD/kafkaCluster/kafka3/config/server.properties</span><br></pre></td></tr></table></figure>
<p>分别从三台kafka服务器中查看启动日志<code>logs/server.log</code>，如果没报错，并且看到如下输出，则启动成功：</p>
<pre><code># kafka1 的输出
[2018-10-27 15:48:54,890] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:48:54,890] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:48:54,895] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)

# kafka2 的输出
[2018-10-27 15:49:22,694] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:22,694] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:22,697] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)

# kafka3 的输出
[2018-10-27 15:49:41,746] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:41,746] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:41,749] INFO [KafkaServer id=3] started (kafka.server.KafkaServer)
</code></pre><p>至此，kafka集群搭建成功。下面，我们简单的试用一下。</p>
<h3 id="12-创建topic"><a href="#12-创建topic" class="headerlink" title="12.创建topic"></a>12.创建topic</h3><p>在任意一台kafka服务器上面创建topic，例如在kafka1上面创建一个名为 my-replicated-topic 的 topic，指定 1 个分区，3 个副本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/kafka-topics.sh --create --zookeeper 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"my-replicated-topic"</span>.</span><br></pre></td></tr></table></figure></p>
<p>上面的参数<code>--zookeeper</code>是集群列表，可以指定所有节点，也可以指定为部分列表。</p>
<p>查看topic的情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</span><br></pre></td></tr></table></figure></p>
<h3 id="13-发送消息"><a href="#13-发送消息" class="headerlink" title="13.发送消息"></a>13.发送消息</h3><p>往我们刚才创建的toipc中发送消息，在任意一台kafka上面都可以的，我们在kafka2上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --topic my-replicated-topic</span><br><span class="line">&gt;</span><br><span class="line">&gt;my <span class="built_in">test</span> message 1</span><br><span class="line">&gt;my <span class="built_in">test</span> message 2</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="14-消费消息"><a href="#14-消费消息" class="headerlink" title="14.消费消息"></a>14.消费消息</h3><p>将我们刚刚发送的消息消费掉，我们从kafka3上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --from-beginning --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">my <span class="built_in">test</span> message 1</span><br><span class="line">my <span class="built_in">test</span> message 2</span><br></pre></td></tr></table></figure></p>
<p>我们在生产者中发送消息，在消费者中就能实时的看到消息。</p>
<h3 id="15-容错测试"><a href="#15-容错测试" class="headerlink" title="15.容错测试"></a>15.容错测试</h3><p>从上面可知my-replicated-topic的leader为3，那我们将broker.id=3的进程杀掉：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ps -ef | grep kafka3/config/server.properties</span><br><span class="line">hewenti+ 22018  1897  5 17:19 pts/23   00:00:16 /usr/<span class="built_in">local</span>/java/jdk1.8.0_102/bin/java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[中间省略部分]</span><br><span class="line"></span><br><span class="line">-0.10.jar:/home/hewentian/ProjectD/kafkaCluster/kafka3/bin/../libs/zookeeper-3.4.13.jar kafka.Kafka /home/hewentian/ProjectD/kafkaCluster/kafka3/config/server.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">kill</span> -9 22018       <span class="comment"># 单机环境下不能通过执行： ./bin/kafka-server-stop.sh 来杀掉当前目录下的kafka，它会杀掉全部kafka</span></span><br></pre></td></tr></table></figure></p>
<p>再查看my-replicated-topic的情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 3,2,1	Isr: 1</span><br></pre></td></tr></table></figure></p>
<p>由上面可见，leader已经变为1。并且，生产消息和消费消息一样可用，不受影响：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --topic my-replicated-topic</span><br><span class="line">&gt;</span><br><span class="line">&gt;my <span class="built_in">test</span> message 1</span><br><span class="line">&gt;my <span class="built_in">test</span> message 2</span><br><span class="line">&gt;</span><br><span class="line">&gt; Tim Ho</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --from-beginning --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">my <span class="built_in">test</span> message 1</span><br><span class="line">my <span class="built_in">test</span> message 2</span><br><span class="line"></span><br><span class="line">Tim Ho</span><br></pre></td></tr></table></figure>
<p>未完，待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/10/27/kafka-cluster/" data-id="cjz3s88qp001nqc3ka3qxaie8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-kafka-standalone" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/24/kafka-standalone/" class="article-date">
  <time datetime="2018-10-24T00:32:40.000Z" itemprop="datePublished">2018-10-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/24/kafka-standalone/">kafka 单节点安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文将说下<code>kafka</code>的单节点安装，我的机器为<code>Ubuntu 16.04 LTS</code>，下面的安装过程参考：<br><a href="http://kafka.apache.org/quickstart" target="_blank" rel="noopener">http://kafka.apache.org/quickstart</a></p>
<h3 id="第一步：我们要将kafka安装包下载回来"><a href="#第一步：我们要将kafka安装包下载回来" class="headerlink" title="第一步：我们要将kafka安装包下载回来"></a>第一步：我们要将<code>kafka</code>安装包下载回来</h3><p>截止本文写时，它的最新版本为<code>2.0.0</code>，可以在它的<a href="https://www.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz" target="_blank" rel="noopener">官网</a>下载。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/</span><br><span class="line">$ wget https://www.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz</span><br><span class="line">$ wget https://www.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz.sha512</span><br><span class="line"></span><br><span class="line">验证下载文件的完整性，在下载的时候要将 SHA512 文件也下载回来</span><br><span class="line">$ sha512sum -c kafka_2.12-2.0.0.tgz.sha512</span><br><span class="line">kafka_2.12-2.0.0.tgz: OK</span><br><span class="line"></span><br><span class="line">$ tar xzf kafka_2.12-2.0.0.tgz</span><br></pre></td></tr></table></figure></p>
<h3 id="第二步：启动服务器"><a href="#第二步：启动服务器" class="headerlink" title="第二步：启动服务器"></a>第二步：启动服务器</h3><p>kafka需要用到zookeeper，所以必须首先启动zookeeper。在高版本的kafka发行包中，已经内置zookeeper，我们直接使用即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure></p>
<p>启动成功后，会看到如下输出：</p>
<pre><code>[2018-10-24 09:14:29,072] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:java.compiler=&lt;NA&gt; (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:os.version=4.13.0-32-generic (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:user.name=hewentian (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,073] INFO Server environment:user.home=/home/hewentian (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,073] INFO Server environment:user.dir=/home/hewentian/ProjectD/kafka_2.12-2.0.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,091] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,091] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,091] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,111] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2018-10-24 09:14:29,121] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
</code></pre><p>接着，打开另外一个终端，启动kafka服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure></p>
<p>启动成功后，会看到如下输出：</p>
<pre><code>[2018-10-24 11:01:45,462] INFO [SocketServer brokerId=0] Started processors for 1 acceptors (kafka.network.SocketServer)
[2018-10-24 11:01:45,494] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-24 11:01:45,494] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-24 11:01:45,497] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
</code></pre><h3 id="第三步：创建topic"><a href="#第三步：创建topic" class="headerlink" title="第三步：创建topic"></a>第三步：创建topic</h3><p>创建一个名字叫<code>test</code>的topic，只有一个分区和一个副本，打开另外一个终端：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span></span><br><span class="line">Created topic <span class="string">"test"</span>.</span><br><span class="line"></span><br><span class="line">查看所有创建的topic</span><br><span class="line">$ ./bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line"><span class="built_in">test</span></span><br></pre></td></tr></table></figure></p>
<h3 id="第四步：往topic发送消息"><a href="#第四步：往topic发送消息" class="headerlink" title="第四步：往topic发送消息"></a>第四步：往topic发送消息</h3><p>kafka自带一个命令行的客户端，用于从文件中或者标准输入中读取消息并且发送到kafka集群，默认每一行会被作为一条消息发送：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic <span class="built_in">test</span></span><br><span class="line">&gt;This is a message</span><br><span class="line">&gt;This is another message</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="第五步：消费topic中的消息"><a href="#第五步：消费topic中的消息" class="headerlink" title="第五步：消费topic中的消息"></a>第五步：消费topic中的消息</h3><p>kafka同样自带一个命令行的消费者，它会将消息输出到标准输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class="built_in">test</span> --from-beginning</span><br><span class="line">This is a message</span><br><span class="line">This is another message</span><br></pre></td></tr></table></figure></p>
<p>这样，一个简单的单节点<code>kafka</code>服务器就搭建完成了，接下来我们将尝试搭建多节点的集群。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/10/24/kafka-standalone/" data-id="cjz3s88qs001tqc3kzv5y65jt" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/">bigdata</a><span class="category-list-count">25</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/container/">container</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/db/">db</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/other/">other</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web-server/">web server</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/26/docker-note/">docker 学习笔记</a>
          </li>
        
          <li>
            <a href="/2019/06/10/nginx-note/">nginx 学习笔记</a>
          </li>
        
          <li>
            <a href="/2019/01/20/hbase-cluster/">hbase 集群的搭建</a>
          </li>
        
          <li>
            <a href="/2019/01/10/hive-note/">hive 学习笔记</a>
          </li>
        
          <li>
            <a href="/2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">link</h3>
    <div class="widget">
      <li><a href="https://github.com/hewentian" title="Tim Ho's Blog">我的github</a></li>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Tim Ho<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/categories/" class="mobile-nav-link">Categories</a>
  
    <a href="/tags/" class="mobile-nav-link">Tags</a>
  
    <a href="/about/" class="mobile-nav-link">About</a>
  
</nav>
    

<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script> -->
<script src="//code.jquery.com/jquery-2.2.4.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>