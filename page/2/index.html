<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tim Ho&#39;s Technology Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="my personal blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Tim Ho&#39;s Technology Blog">
<meta property="og:url" content="https://github.com/hewentian/page/2/index.html">
<meta property="og:site_name" content="Tim Ho&#39;s Technology Blog">
<meta property="og:description" content="my personal blog">
<meta property="og:locale" content="en-US">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tim Ho&#39;s Technology Blog">
<meta name="twitter:description" content="my personal blog">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Tim Ho&#39;s Technology Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">心如止水</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/categories/">Categories</a>
        
          <a class="main-nav-link" href="/tags/">Tags</a>
        
          <a class="main-nav-link" href="/about/">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://github.com/hewentian"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hbase-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/20/hbase-cluster/" class="article-date">
  <time datetime="2019-01-20T04:31:29.000Z" itemprop="datePublished">2019-01-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/20/hbase-cluster/">hbase 集群的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Hbase-介绍"><a href="#Hbase-介绍" class="headerlink" title="Hbase 介绍"></a>Hbase 介绍</h3><p>Hbase的<a href="http://hbase.apache.org/book.html" target="_blank" rel="noopener">官方文档</a>中有对Hbase的详细介绍，这里不再赘述。这里用一句话描述如下：</p>
<p>Apache HBase™ is the Hadoop database, a distributed, scalable, big data store.</p>
<p>Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project’s goal is the hosting of very large tables – billions of rows X millions of columns – atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google’s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.</p>
<h3 id="Hbase-的安装"><a href="#Hbase-的安装" class="headerlink" title="Hbase 的安装"></a>Hbase 的安装</h3><p>安装过程参考这里：<br><a href="http://hbase.apache.org/book.html#quickstart_fully_distributed" target="_blank" rel="noopener">http://hbase.apache.org/book.html#quickstart_fully_distributed</a></p>
<p>Hbase依赖于HADOOP，我们在上一篇<a href="../../../../2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>的基础上安装Hbase。</p>
<p>节点分布如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
slave3:
    ip: 192.168.56.113
    hostname: hadoop-host-slave-3
</code></pre><p>如下图（绿色代表在这些节点上面安装这些程序，与<code>hadoop 集群的搭建HA</code>安装中的图类似，这里多了后面两列）：</p>
<p><img src="/img/hbase-1.png" alt="" title="HBase集群 节点分布"></p>
<h3 id="安装NTP"><a href="#安装NTP" class="headerlink" title="安装NTP"></a>安装NTP</h3><p>可能还要在各节点服务器上面安装NTP服务，实现服务器节点间时间的一致。如果服务器节点间的时间不一致，可能会引发HBase的异常，这一点在HBase官网上有特别强调。在这里，设置我的笔记本电脑为NTP的服务端节点，即是我的电脑从国家授时中心同步时间，然后其它节点（master、slave1、slave2、slave3）作为客户端从我的笔记本同步时间。此篇的安装过程将省略这个步骤，在后续篇章中再介绍，本篇将手动将各节点的时间调成一致。</p>
<h3 id="修改ulimit"><a href="#修改ulimit" class="headerlink" title="修改ulimit"></a>修改ulimit</h3><p>Configuring the maximum number of file descriptors and processes for the user who is running the HBase process is an operating system configuration, rather than an HBase configuration. It is also important to be sure that the settings are changed for the user that actually runs HBase. To see which user started HBase, and that user’s ulimit configuration, look at the first line of the HBase log for that instance.</p>
<p>修改ulimit，以增加linux系统能同时打开文件的数量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line">hadoop  -       nofile  32768</span><br><span class="line">hadoop  -       nproc   32000</span><br></pre></td></tr></table></figure>
<p>修改后需重启系统才能生效。</p>
<h3 id="下载Hbase"><a href="#下载Hbase" class="headerlink" title="下载Hbase"></a>下载Hbase</h3><p>首先下载Hbase，我们下载的时候，要选择适合我们HADOOP版本的Hbase，我们下载的稳定版为<a href="http://archive.apache.org/dist/hbase/1.2.6/" target="_blank" rel="noopener">hbase-1.2.6-bin.tar.gz</a>，将压缩包首先传到<code>master</code>节点的<code>/home/hadoop/</code>目录下，先在<code>master</code>节点配置好，然后同步到其他3个节点。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xzvf hbase-1.2.6-bin.tar.gz</span><br><span class="line">$ </span><br><span class="line">$ <span class="built_in">cd</span> hbase-1.2.6/</span><br><span class="line">$ ls</span><br><span class="line">bin  CHANGES.txt  conf  docs  hbase-webapps  LEGAL  lib  LICENSE.txt  NOTICE.txt  README.txt</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hbase-env-sh，加上JDK绝对路径"><a href="#配置hbase-env-sh，加上JDK绝对路径" class="headerlink" title="配置hbase-env.sh，加上JDK绝对路径"></a>配置hbase-env.sh，加上JDK绝对路径</h3><ol>
<li>JDK的路径就是安装JDK的时候的路径；</li>
<li>Hbase内置有zookeeper，但是为了方便管理，我们单独部署zookeeper，即使用HADOOP中用作ZKFC的zookeeper；</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi hbase-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_102/</span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hbase-site-xml"><a href="#配置hbase-site-xml" class="headerlink" title="配置hbase-site.xml"></a>配置hbase-site.xml</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop-host-master:8020/hbase&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Do not create the dir hbase, the system will create it automatically, and the value is dfs.namenode.rpc-address.hadoop-cluster-ha.nn1&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Property from ZooKeeper<span class="string">'s config zoo.cfg. The port at which the clients will connect.&lt;/description&gt;</span></span><br><span class="line"><span class="string">    &lt;/property&gt;</span></span><br><span class="line"><span class="string">    &lt;property&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span></span><br><span class="line"><span class="string">        &lt;value&gt;hadoop-host-master,hadoop-host-slave-1,hadoop-host-slave-2&lt;/value&gt;</span></span><br><span class="line"><span class="string">    &lt;/property&gt;</span></span><br><span class="line"><span class="string">    &lt;property&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span></span><br><span class="line"><span class="string">        &lt;value&gt;/home/hadoop/zookeeper-3.4.6/data&lt;/value&gt;</span></span><br><span class="line"><span class="string">        &lt;description&gt;Property from ZooKeeper'</span>s config zoo.cfg. The directory <span class="built_in">where</span> the snapshot is stored.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h3 id="配置regionservers"><a href="#配置regionservers" class="headerlink" title="配置regionservers"></a>配置regionservers</h3><p>在将要运行regionservers的节点加入此文件中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi regionservers</span><br><span class="line"></span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br><span class="line">hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<h3 id="配置backup-masters"><a href="#配置backup-masters" class="headerlink" title="配置backup-masters"></a>配置backup-masters</h3><p>我们将<code>hadoop-host-master</code>作为Hbase集群的master，并配置HBase使用<code>hadoop-host-slave-1</code>作为backup master，在conf目录下创建一个文件backup-masters, 并在其中添加作为backup master的主机名<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf</span><br><span class="line">$ vi backup-masters</span><br><span class="line"></span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<h3 id="复制hdfs-site-xml配置文件"><a href="#复制hdfs-site-xml配置文件" class="headerlink" title="复制hdfs-site.xml配置文件"></a>复制hdfs-site.xml配置文件</h3><p>复制<code>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</code>到<code>$HBASE_HOME/conf</code>目录下，这样以保证HDFS与Hbase两边配置一致，这也是官网所推荐的方式。例子，如果HDFS中配置的副本数量为5（默认为3），如果没有将hadoop的<code>hdfs-site.xml</code>复制到<code>$HBASE_HOME/conf</code>目录下，则Hbase将会按3份备份，从而两边不一致，导致出现异常。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/conf/</span><br><span class="line">$ cp /home/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml .</span><br></pre></td></tr></table></figure>
<p>至此，配置完毕，将这些配置同步到其他三个节点，在<code>hadoop-host-master</code>上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ scp -r hbase-1.2.6 hadoop@hadoop-host-slave-1:/home/hadoop/</span><br><span class="line">$ scp -r hbase-1.2.6 hadoop@hadoop-host-slave-2:/home/hadoop/</span><br><span class="line">$ scp -r hbase-1.2.6 hadoop@hadoop-host-slave-3:/home/hadoop/</span><br></pre></td></tr></table></figure></p>
<h3 id="启动Hbase"><a href="#启动Hbase" class="headerlink" title="启动Hbase"></a>启动Hbase</h3><p>可使用<code>$HBASE_HOME/bin/start-hbase.sh</code>指令启动整个集群，如果要使用该命令，则集群的节点间必须实现ssh的免密码登录，这样才能到不同的节点启动服务。</p>
<p>按我们前面的规划，<code>hadoop-host-master</code>将作为Hbase集群的master，其实在哪台机器上面运行<code>start-hbase.sh</code>指令，那么这台机器将成为master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/bin</span><br><span class="line">$ ./start-hbase.sh</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hbase-2.png" alt="" title="HBase集群启动"></p>
<p>当执行<code>jps</code>指令后，可以看到<code>hadoop-host-master</code>上面多了一个<code>HMaster</code>进程，在<code>hadoop-host-slave-1</code>中会同时存在<code>HMaster</code>、<code>HRegionServer</code>进程，而在其他两个节点则只存在<code>HRegionServer</code>进程。</p>
<p>另外，我们可以在其他任何机器通过以下命令启动一个master<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/bin</span><br><span class="line">$ ./hbase-daemon.sh start master</span><br><span class="line"></span><br><span class="line">或者启动作为backup master</span><br><span class="line">$ ./hbase-daemon.sh start master --backup</span><br></pre></td></tr></table></figure></p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:16010/" target="_blank" rel="noopener">http://hadoop-host-master:16010/</a><br><a href="http://hadoop-host-slave-1:16010/" target="_blank" rel="noopener">http://hadoop-host-slave-1:16010/</a></p>
<p>来查看是否启动成功，如无意外的话，你会看到如下结果页面。其中一个是Master，另一个是Back Master：</p>
<p><img src="/img/hbase-3.png" alt="" title="HBase集群管理界面 Master"></p>
<p><img src="/img/hbase-4.png" alt="" title="HBase集群管理界面 Back Master"></p>
<p>同样的它在HDFS中也自动创建了保存数据的目录：</p>
<p><img src="/img/hbase-5.png" alt="" title="HBase集群在HDFS中的目录"></p>
<p>至此，集群搭建成功。</p>
<h3 id="Hbase初体验"><a href="#Hbase初体验" class="headerlink" title="Hbase初体验"></a>Hbase初体验</h3><p>首先我们通过SHELL的方式简单体验一下Hbase：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6/</span><br><span class="line">$ ./bin/hbase shell</span><br><span class="line">2019-01-23 19:16:36,118 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">HBase Shell; enter <span class="string">'help&lt;RETURN&gt;'</span> <span class="keyword">for</span> list of supported commands.</span><br><span class="line">Type <span class="string">"exit&lt;RETURN&gt;"</span> to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.6030 seconds</span><br><span class="line"></span><br><span class="line">=&gt; []</span><br></pre></td></tr></table></figure></p>
<p>由上述结果可知，Hbase中现在没有一张表。我们尝试创建一张表<code>t_student</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create <span class="string">'t_student'</span>, <span class="string">'cf1'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 2.4700 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - t_student</span><br><span class="line">hbase(main):003:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">t_student</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0250 seconds</span><br><span class="line"></span><br><span class="line">=&gt; [<span class="string">"t_student"</span>]</span><br><span class="line">hbase(main):004:0&gt; desc <span class="string">'t_student'</span></span><br><span class="line">Table t_student is ENABLED</span><br><span class="line">t_student</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf1'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, VERSIONS =&gt; <span class="string">'1'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, TTL =&gt; <span class="string">'FOREVER'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, BLO</span><br><span class="line">CKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>&#125;</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.2010 seconds</span><br></pre></td></tr></table></figure></p>
<p>往表中插入2条数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):005:0&gt; put <span class="string">'t_student'</span>, <span class="string">'01'</span>, <span class="string">'cf1:name'</span>, <span class="string">'tim'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.1840 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):006:0&gt; put <span class="string">'t_student'</span>, <span class="string">'02'</span>, <span class="string">'cf1:name'</span>, <span class="string">'timho'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.3630 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):007:0&gt; scan <span class="string">'t_student'</span></span><br><span class="line">ROW                                                  COLUMN+CELL</span><br><span class="line"> 01                                                  column=cf1:name, timestamp=1548242390794, value=tim</span><br><span class="line"> 02                                                  column=cf1:name, timestamp=1548246522887, value=timho</span><br><span class="line">2 row(s) <span class="keyword">in</span> 0.1240 seconds</span><br></pre></td></tr></table></figure></p>
<p>插入数据之后，可能在HDFS中还不能立刻看到，因为数据还在内存中，但可以通过以下命令将数据立刻写到HDFS中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):008:0&gt; flush <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.7290 seconds</span><br></pre></td></tr></table></figure></p>
<p>然后我们可以在HDFS、Hbase的管理界面分别看到表信息：</p>
<p><img src="/img/hbase-6.png" alt="" title="HBase的数据在HDFS中"></p>
<p><img src="/img/hbase-7.png" alt="" title="HBase集群管理界面 用户表信息"></p>
<p>当我们在HDFS中看到表中的某个块的数据，如下：</p>
<p><img src="/img/hbase-8.png" alt="" title="HBase的数据在HDFS中"></p>
<p>我们可以通过Hbase中的命令来查看数据的真实内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hbase-1.2.6</span><br><span class="line">$ ./bin/hbase hfile -p -f /hbase/data/default/t_student/b76cccf6c6a7926bf8f40b4eafc6991e/cf1/2ed0a233411447778982edce04e96fe3</span><br><span class="line">2019-01-23 19:45:33,200 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">2019-01-23 19:45:34,046 INFO  [main] hfile.CacheConfig: Created cacheConfig: CacheConfig:disabled</span><br><span class="line">K: 01/cf1:name/1548242390794/Put/vlen=3/seqid=4 V: tim</span><br><span class="line">Scanned kv count -&gt; 1</span><br></pre></td></tr></table></figure></p>
<p>查看集群状态和节点数量<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):009:0&gt; status</span><br><span class="line">1 active master, 1 backup masters, 3 servers, 0 dead, 1.0000 average load</span><br></pre></td></tr></table></figure></p>
<p>根据条件查询数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):010:0&gt; get <span class="string">'t_student'</span>, <span class="string">'01'</span></span><br><span class="line">COLUMN                                               CELL</span><br><span class="line"> cf1:name                                            timestamp=1548242390794, value=tim</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0590 seconds</span><br></pre></td></tr></table></figure></p>
<p>表失效、表生效、删除表：</p>
<ol>
<li>使用<code>disable</code>命令可将某张表失效，失效后该表将不能使用；</li>
<li>使用<code>enable</code>命令可使表重新生效，表生效后，即可对表进行操作；</li>
<li>使用<code>drop</code>命令可对表进行删除，但只有表在失效的情况下，才能进行删除。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; desc <span class="string">'t_student'</span></span><br><span class="line">Table t_student is ENABLED</span><br><span class="line">t_student</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf1'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, VERSIONS =&gt; <span class="string">'1'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, TTL =&gt; <span class="string">'FOREVER'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, BLO</span><br><span class="line">CKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>&#125;</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0480 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):012:0&gt; <span class="built_in">disable</span> <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 2.4070 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):013:0&gt; desc <span class="string">'t_student'</span></span><br><span class="line">Table t_student is DISABLED</span><br><span class="line">t_student</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf1'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, VERSIONS =&gt; <span class="string">'1'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, TTL =&gt; <span class="string">'FOREVER'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, BLO</span><br><span class="line">CKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>&#125;</span><br><span class="line">1 row(s) <span class="keyword">in</span> 0.0320 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):014:0&gt; <span class="built_in">enable</span> <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 1.3260 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):015:0&gt; <span class="built_in">disable</span> <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 2.2550 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):016:0&gt; drop <span class="string">'t_student'</span></span><br><span class="line">0 row(s) <span class="keyword">in</span> 1.3540 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):017:0&gt; list</span><br><span class="line">TABLE</span><br><span class="line">0 row(s) <span class="keyword">in</span> 0.0060 seconds</span><br><span class="line"></span><br><span class="line">=&gt; []</span><br></pre></td></tr></table></figure>
<p>退出 hbase shell<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; quit</span><br></pre></td></tr></table></figure></p>
<h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><ol>
<li><p>有时候我们重启了hadoop集群后，发现hbase无法使用，有可能是我们在hbase-site.xml中配置的hadoop master节点已经不是active了，解决办法是在hadoop中手动将其设为active状态；</p>
</li>
<li><p>主从节点时间没有同步时，极有可能出现如下错误，同步时间后可以正常启动：</p>
<pre><code>master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 855041 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.

[HBase] ERROR:org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
</code></pre></li>
</ol>
<h3 id="java操作Hbase"><a href="#java操作Hbase" class="headerlink" title="java操作Hbase"></a>java操作Hbase</h3><p>java操作Hbase的例子见这里：<a href="https://github.com/hewentian/bigdata/blob/master/codes/hadoop-demo/src/main/java/com/hewentian/hadoop/utils/HbaseUtil.java">HbaseUtil.java</a>、<a href="https://github.com/hewentian/bigdata/blob/master/codes/hadoop-demo/src/main/java/com/hewentian/hadoop/hbase/HbaseDemo.java">HbaseDemo.java</a></p>
<p>未完，待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/01/20/hbase-cluster/" data-id="clf9thvlj00166h3khnhnm94d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hive-note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/10/hive-note/" class="article-date">
  <time datetime="2019-01-10T14:30:12.000Z" itemprop="datePublished">2019-01-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/10/hive-note/">hive 学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Hive-介绍"><a href="#Hive-介绍" class="headerlink" title="Hive 介绍"></a>Hive 介绍</h3><p>hive的<a href="https://hive.apache.org/" target="_blank" rel="noopener">官方文档</a>中有对hive的详细介绍，这里不再赘述。我们用一句话描述如下：<br>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p>
<h3 id="hive-的安装"><a href="#hive-的安装" class="headerlink" title="hive 的安装"></a>hive 的安装</h3><p>安装过程参考这里：<br><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
<p>hive依赖于HADOOP，我们在上一篇<a href="../../../../2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>的基础上安装hive。</p>
<p>首先下载hive，我们下载的时候，要选择适合我们HADOOP版本的hive，我们下载的稳定版为<a href="http://archive.apache.org/dist/hive/hive-1.2.2/" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a>，我们将在HADOOP集群的namenode上面安装，即在master机器上面安装。将压缩包传到<code>/home/hadoop/</code>目录下。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xzvf apache-hive-1.2.2-bin.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>解压后得到目录<code>apache-hive-1.2.2-bin</code>，我们看下压缩包中的内容：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin</span><br><span class="line">$ ls</span><br><span class="line">bin  conf  examples  hcatalog  lib  LICENSE  NOTICE  README.txt  RELEASE_NOTES.txt  scripts</span><br><span class="line">$</span><br><span class="line">$ ls conf/</span><br><span class="line">beeline-log4j.properties.template  hive-env.sh.template                 hive-log4j.properties.template</span><br><span class="line">hive-default.xml.template          hive-exec-log4j.properties.template  ivysettings.xml</span><br></pre></td></tr></table></figure></p>
<p>配置HADOOP_HOME：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/conf/</span><br><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br><span class="line">$ cp hive-env.sh.template hive-env.sh</span><br><span class="line">$ vi hive-env.sh</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/home/hadoop/hadoop-2.7.3</span><br></pre></td></tr></table></figure></p>
<p>到这里，hive就配置好了，可以运行了。但，不妨看下下面的<code>配置hive元数据的存储位置</code>，因为生产环境一般是要配置的。</p>
<h3 id="配置hive元数据的存储位置（可选配置）"><a href="#配置hive元数据的存储位置（可选配置）" class="headerlink" title="配置hive元数据的存储位置（可选配置）"></a>配置hive元数据的存储位置（可选配置）</h3><p>hive默认将元数据存储在<code>derby</code>数据库中（hive安装包自带），当然我们也可以选择存储在其他数据库，如mysql中。下面演示一下：<br>首先在MYSQL数据库中创建一个数据库，用于存储hive的元数据，我们就将库名创建为hive：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE IF NOT EXISTS hive COLLATE = &apos;utf8_general_ci&apos; CHARACTER SET = &apos;utf8&apos;;</span><br><span class="line">mysql&gt; GRANT ALL ON hive.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line">mysql&gt; GRANT ALL ON hive.* TO &apos;hive&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure></p>
<p>然后配置hive使用mysql存储元数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/conf/</span><br><span class="line">$ vi hive-site.xml</span><br></pre></td></tr></table></figure></p>
<p>修改下面部分，假定我们的数据库地址、用户名和密码如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://mysql.hewentian.com:3306/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      JDBC connect string for a JDBC metastore.</span><br><span class="line">      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class="line">      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>最后，将mysql连接JDBC的jar包<a href="http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.42/mysql-connector-java-5.1.42.jar" target="_blank" rel="noopener">mysql-connector-java-5.1.42.jar</a>放到<code>apache-hive-1.2.2-bin/lib</code>目录下</p>
<p>好了，以上这部分是<strong>可选配置</strong>部分。</p>
<h3 id="启动hive"><a href="#启动hive" class="headerlink" title="启动hive"></a>启动hive</h3><p>初次启动hive，需在HDFS中创建几个目录，用于存储hive的数据，我们在安装hive的master节点执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -mkdir /tmp</span><br><span class="line">$ ./bin/hdfs dfs -mkdir -p /user/hive/warehouse</span><br><span class="line">$</span><br><span class="line">$ ./bin/hdfs dfs -chmod g+w /tmp</span><br><span class="line">$ ./bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></p>
<p>初始化元数据存储相关信息，hive默认使用内置的<code>derby</code>数据库存储元数据。这里使用<code>mysql</code>，如果要使用默认的，则则将下面的<code>mysql</code>修改成<code>derby</code>即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./schematool -dbType mysql -initSchema</span><br><span class="line"></span><br><span class="line">Metastore connection URL:	 jdbc:mysql://mysql.hewentian.com:3306/hive</span><br><span class="line">Metastore Connection Driver :	 com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User:	 hive</span><br><span class="line">Starting metastore schema initialization to 1.2.0</span><br><span class="line">Initialization script hive-schema-1.2.0.mysql.sql</span><br><span class="line">Initialization script completed</span><br><span class="line">schemaTool completed</span><br></pre></td></tr></table></figure></p>
<p>正式启动hive<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./hive</span><br></pre></td></tr></table></figure></p>
<p>启动的时候可能会报如下错误：</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
    at org.apache.hadoop.fs.Path.initialize(Path.java:205)
    at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:171)
    at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:659)
    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:582)
    at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:549)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:750)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
    at java.net.URI.checkPath(URI.java:1823)
    at java.net.URI.&lt;init&gt;(URI.java:745)
    at org.apache.hadoop.fs.Path.initialize(Path.java:202)
    ... 12 more
</code></pre><p>解决方法如下，先建目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/</span><br><span class="line">$ mkdir iotmp</span><br></pre></td></tr></table></figure></p>
<p>将<code>hive-site.xml</code>中</p>
<ol>
<li>包含<code>${system:java.io.tmpdir}</code>的配置项替换为上面的路径<code>/home/hadoop/apache-hive-1.2.2-bin/iotmp</code>，一共有4处；</li>
<li>包含<code>${system:user.name}</code>的配置项替换为<code>hadoop</code>。<br>修改项如下：<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Location of Hive run time structured log file<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apache-hive-1.2.2-bin/iotmp/hadoop/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>重新启动hive：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./hive</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/home/hadoop/apache-hive-1.2.2-bin/lib/hive-common-1.2.2.jar!/hive-log4j.properties</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.821 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; use default;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.043 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; show tables;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.094 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p>
<p>至此，hive安装成功。从上面可知，hive有一个默认的数据库<code>default</code>，并且里面一张表也没有。</p>
<h3 id="hive初体验"><a href="#hive初体验" class="headerlink" title="hive初体验"></a>hive初体验</h3><ul>
<li><p>A longer tutorial that covers more features of HiveQL:<br><a href="https://cwiki.apache.org/confluence/display/Hive/Tutorial" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Tutorial</a></p>
</li>
<li><p>The HiveQL Language Manual:<br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual</a></p>
</li>
</ul>
<h3 id="创建数据库："><a href="#创建数据库：" class="headerlink" title="创建数据库："></a>创建数据库：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE DATABASE IF NOT EXISTS tim;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.323 seconds</span><br><span class="line">hive&gt;</span><br><span class="line">    &gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">tim</span><br><span class="line">Time taken: 0.025 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>
<p>同样，我们可以在HDFS中查看到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -ls /user/hive/warehouse</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwxr-x   - hadoop supergroup          0 2019-01-01 19:32 /user/hive/warehouse/tim.db</span><br></pre></td></tr></table></figure></p>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use tim;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.042 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; CREATE TABLE IF NOT EXISTS t_user (</span><br><span class="line">    &gt; id INT,</span><br><span class="line">    &gt; name STRING COMMENT <span class="string">'user name'</span>,</span><br><span class="line">    &gt; age INT COMMENT <span class="string">'user age'</span>,</span><br><span class="line">    &gt; sex STRING COMMENT <span class="string">'user sex'</span>,</span><br><span class="line">    &gt; birthday DATE COMMENT <span class="string">'user birthday'</span>,</span><br><span class="line">    &gt; address STRING COMMENT <span class="string">'user address'</span></span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; COMMENT <span class="string">'This is the use info table'</span></span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt;  FIELDS TERMINATED BY <span class="string">'\t'</span></span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.521 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; show tables;</span><br><span class="line">OK</span><br><span class="line">t_user</span><br><span class="line">Time taken: 0.035 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<h3 id="查看表结构"><a href="#查看表结构" class="headerlink" title="查看表结构"></a>查看表结构</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc t_user;</span><br><span class="line">OK</span><br><span class="line">id                  	int</span><br><span class="line">name                	string              	user name</span><br><span class="line">age                 	int                 	user age</span><br><span class="line">sex                 	string              	user sex</span><br><span class="line">birthday            	date                	user birthday</span><br><span class="line">address             	string              	user address</span><br><span class="line">Time taken: 0.074 seconds, Fetched: 6 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; INSERT INTO TABLE t_user(id, name, age, sex, birthday, address) VALUES(1, <span class="string">'Tim Ho'</span>, 23, <span class="string">'M'</span>, <span class="string">'1989-05-01'</span>, <span class="string">'Higher Education Mega Center South, Guangzhou city, Guangdong Province'</span>);</span><br><span class="line">Query ID = hadoop_20190102160558_640a90a7-9122-4650-af78-acb436e2643b</span><br><span class="line">Total <span class="built_in">jobs</span> = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is <span class="built_in">set</span> to 0 since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1546186928725_0015, Tracking URL = http://hadoop-host-master:8088/proxy/application_1546186928725_0015/</span></span><br><span class="line"><span class="string">Kill Command = /home/hadoop/hadoop-2.7.3/bin/hadoop job  -kill job_1546186928725_0015</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-01-02 16:06:08,341 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-01-02 16:06:14,565 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 1 seconds 390 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1546186928725_0015</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://hadoop-cluster-ha/user/hive/warehouse/tim.db/t_user/.hive-staging_hive_2019-01-02_16-05-58_785_7094384272339204067-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table tim.t_user</span></span><br><span class="line"><span class="string">Table tim.t_user stats: [numFiles=1, numRows=1, totalSize=96, rawDataSize=95]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 1.39 sec   HDFS Read: 4763 HDFS Write: 162 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 1 seconds 390 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 17.079 seconds</span></span><br><span class="line"><span class="string">hive&gt;</span></span><br></pre></td></tr></table></figure>
<p>执行插入操作它会产生一个mapReduce任务。</p>
<h3 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from t_user;</span><br><span class="line">OK</span><br><span class="line">1	Tim Ho	23	M	1989-05-01	Higher Education Mega Center South, Guangzhou city, Guangdong Province</span><br><span class="line">Time taken: 0.196 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; select * from t_user <span class="built_in">where</span> name=<span class="string">'Tim Ho'</span>;</span><br><span class="line">OK</span><br><span class="line">1	Tim Ho	23	M	1989-05-01	Higher Education Mega Center South, Guangzhou city, Guangdong Province</span><br><span class="line">Time taken: 0.258 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; select count(*) from t_user;</span><br><span class="line">Query ID = hadoop_20190102161100_d60df721-539d-4e5b-a3db-a4951ac884b4</span><br><span class="line">Total <span class="built_in">jobs</span> = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</span><br><span class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</span><br><span class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to <span class="built_in">set</span> a constant number of reducers:</span><br><span class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1546186928725_0016, Tracking URL = http://hadoop-host-master:8088/proxy/application_1546186928725_0016/</span><br><span class="line">Kill Command = /home/hadoop/hadoop-2.7.3/bin/hadoop job  -<span class="built_in">kill</span> job_1546186928725_0016</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2019-01-02 16:11:10,739 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-01-02 16:11:16,997 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.05 sec</span><br><span class="line">2019-01-02 16:11:23,280 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.37 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 2 seconds 370 msec</span><br><span class="line">Ended Job = job_1546186928725_0016</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.37 sec   HDFS Read: 7285 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 2 seconds 370 msec</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">Time taken: 24.444 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<p>由上面可知，执行简单的查询操作不会启动mapReduce，但执行像COUNT这样的统计操作将会产生一个mapReduce。</p>
<h3 id="从文件中导入数据"><a href="#从文件中导入数据" class="headerlink" title="从文件中导入数据"></a>从文件中导入数据</h3><p>语法：</p>
<pre><code>LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
</code></pre><p>我们可以按定义表结构时的使用的字段分隔符(\t)，将数据存放在文本文件里，然后使用LOAD命令来导入。例如我们将数据存放在<code>/home/hadoop/user.txt</code>中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2	scott	25	M	1977-10-21	USA</span><br><span class="line">3	tiger	21	F	1977-08-12	UK</span><br></pre></td></tr></table></figure></p>
<p>然后在hive中执行LOAD命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH <span class="string">'/home/hadoop/user.txt'</span> INTO TABLE t_user;</span><br><span class="line">Loading data to table tim.t_user</span><br><span class="line">Table tim.t_user stats: [numFiles=2, numRows=0, totalSize=151, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.214 seconds</span><br><span class="line">hive&gt; </span><br><span class="line">    &gt; select * from t_user;</span><br><span class="line">OK</span><br><span class="line">1	Tim Ho	23	M	1989-05-01	Higher Education Mega Center South, Guangzhou city, Guangdong Province</span><br><span class="line">2	scott	25	M	1977-10-21	USA</span><br><span class="line">3	tiger	21	F	1977-08-12	UK</span><br><span class="line">Time taken: 0.085 seconds, Fetched: 3 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="通过JAVA代码操作hive"><a href="#通过JAVA代码操作hive" class="headerlink" title="通过JAVA代码操作hive"></a>通过JAVA代码操作hive</h3><p>HQL脚本通常有以下几种方式执行：</p>
<ol>
<li>hive -e “hql”; </li>
<li>hive -f “hql.file”;</li>
<li>hive jdbc code.</li>
</ol>
<p>本节主要讲讲如何通过java来操作hive，首先启动HiveServer2，hiveserver2命令未来可用于替代hive命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./hiveserver2</span><br></pre></td></tr></table></figure></p>
<p>启动后，你可能会发现，啥也没输出。这时我们在另一个SHELL窗口中启动beelie<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/apache-hive-1.2.2-bin/bin</span><br><span class="line">$ ./beeline -u jdbc:hive2://hadoop-host-master:10000 -n hadoop -p hadoop</span><br><span class="line"></span><br><span class="line">Connecting to jdbc:hive2://hadoop-host-master:10000</span><br><span class="line">Connected to: Apache Hive (version 1.2.2)</span><br><span class="line">Driver: Hive JDBC (version 1.2.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; </span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| tim            |</span><br><span class="line">+----------------+--+</span><br><span class="line">2 rows selected (0.217 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; use tim;</span><br><span class="line">No rows affected (0.08 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; show tables;</span><br><span class="line">+-----------+--+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+--+</span><br><span class="line">| t_user    |</span><br><span class="line">+-----------+--+</span><br><span class="line">1 row selected (0.071 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt; select * from t_user;</span><br><span class="line">+------------+--------------+-------------+-------------+------------------+-------------------------------------------------------------------------+--+</span><br><span class="line">| t_user.id  | t_user.name  | t_user.age  | t_user.sex  | t_user.birthday  |                             t_user.address                              |</span><br><span class="line">+------------+--------------+-------------+-------------+------------------+-------------------------------------------------------------------------+--+</span><br><span class="line">| 1          | Tim Ho       | 23          | M           | 1989-05-01       | Higher Education Mega Center South, Guangzhou city, Guangdong Province  |</span><br><span class="line">| 2          | scott        | 25          | M           | 1977-10-21       | USA                                                                     |</span><br><span class="line">| 3          | tiger        | 21          | F           | 1977-08-12       | UK                                                                      |</span><br><span class="line">+------------+--------------+-------------+-------------+------------------+-------------------------------------------------------------------------+--+</span><br><span class="line">3 rows selected (0.219 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop-host-master:10000&gt;</span><br></pre></td></tr></table></figure></p>
<p>由上面可知，和在hive命令下的操作是一样的。上面的命令也可以没有<code>-p hadoop</code>这个参数，这个可以在<code>hive-site.xml</code>中配置。</p>
<p>java代码操作hive的例子在这里：<a href="https://github.com/hewentian/bigdata/blob/master/codes/hadoop-demo/src/main/java/com/hewentian/hadoop/utils/HiveUtil.java">HiveUtil.java</a>、<a href="https://github.com/hewentian/bigdata/blob/master/codes/hadoop-demo/src/main/java/com/hewentian/hadoop/hive/HiveDemo.java">HiveDemo.java</a></p>
<h3 id="后台方式启动hive"><a href="#后台方式启动hive" class="headerlink" title="后台方式启动hive"></a>后台方式启动hive</h3><p>For versions 1.2 and above, <code>hive</code> is deprecated and the <code>hiveserver2</code> command should be used directly.</p>
<p>So the correct way to start hiveserver2 in background is now:</p>
<pre><code>cd /home/hadoop/apache-hive-1.2.2-bin/bin
nohup ./hiveserver2 &amp;
</code></pre><p>Or with output to a log file:</p>
<pre><code>nohup ./hiveserver2 &gt; hive.log &amp;
</code></pre><p>未完待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/01/10/hive-note/" data-id="clf9thvlh00136h3knczphfv8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/">hive</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop-cluster-ha" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/01/hadoop-cluster-ha/" class="article-date">
  <time datetime="2019-01-01T06:13:31.000Z" itemprop="datePublished">2019-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/01/hadoop-cluster-ha/">hadoop 集群的搭建HA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说<code>hadoop</code>集群HA的搭建，如果不想搭建HA，可以参考我之前的笔记：<a href="../../../../2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>，下面HA的搭建很多步骤与此文相同。</p>
<p>为了解决<code>hadoop 1.0.0</code>之前版本的单点故障问题，在<code>hadoop 2.0.0</code>中通过在同一个集群上运行两个<code>NameNode</code>的<code>主动/被动</code>配置热备份，这样集群允许在一个NameNode出现故障时，请求转移到另外一个NameNode来保证集群的正常运行。两个NameNode有相同的职能。在任何时刻，只有一个是<code>active</code>状态的，另一个是<code>standby</code>状态的。当集群运行时，只有<code>active</code>状态的NameNode是正常工作的，<code>standby</code>状态的NameNode是处于待命状态的，时刻同步<code>active</code>状态NameNode的数据。一旦<code>active</code>状态的NameNode不能工作，通过手工或者自动切换，<code>standby</code>状态的NameNode就可以转变为<code>active</code>状态的，就可以继续工作了，这就是高可靠。</p>
<p>安装过程参考官方文档：<br><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></p>
<p>hadoop集群的搭建，我们将搭建如下图所示的集群，HADOOP集群中所有节点的配置文件可以一模一样的。</p>
<p><img src="/img/hadoop-ha-1.png" alt="" title="HADOOP HA集群结构图"></p>
<p>对上图的节点分布，如下图（绿色代表在这些节点上面安装这些程序，一般运行namenode的节点都同时运行ZKFC）：</p>
<p><img src="/img/hadoop-ha-2.png" alt="" title="HADOOP HA集群 节点分布"></p>
<p>在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装四台服务器：master、slave1、slave2、slave3来搭建hadoop集群HA。安装好VirtualBox后，启动它。依次点<code>File -&gt; Host Network Manager -&gt; Create</code>，来创建一个网络和虚拟机中的机器通讯，这个地址是：<code>192.168.56.1</code>，也就是我们外面实体机的地址（仅和虚拟机中的机器通讯使用）。如下图：</p>
<p><img src="/img/hadoop-1.png" alt="" title="虚拟机网络配置"></p>
<p>我们使用<code>ubuntu 18.04</code>来作为我们的服务器，先在虚拟机中安装好一台服务器master，将Jdk、hadoop在上面安装好，然后将master克隆出slave1、slave2、slave3。以master为namenode节点，slave1、slave2、slave3作为datanode节点。slave1同时也作为namenode节点。相关配置如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
slave3:
    ip: 192.168.56.113
    hostname: hadoop-host-slave-3
</code></pre><h3 id="下面开始master的安装"><a href="#下面开始master的安装" class="headerlink" title="下面开始master的安装"></a>下面开始master的安装</h3><p>在虚拟机中安装<code>master</code>的过程中我们会设置一个用户用于登录，我们将用户名、密码都设为<code>hadoop</code>，当然也可以为其他名字，其他安装过程略。安装好之后，使用默认的网关配置NAT，NAT可以访问外网，我们将<code>jdk-8u102-linux-x64.tar.gz</code>和<code>hadoop-2.7.3.tar.gz</code>从它们的官网下载到用户的<code>/home/hadoop/</code>目录下。或在实体机中通过SCP命令传进去。然后将网关设置为<code>Host-only Adapter</code>，如下图所示。</p>
<p><img src="/img/hadoop-2.png" alt="" title="网络配置"></p>
<p>网关设置好了之后，我们接下来配置IP地址。在<code>master</code>中<code>[Settings] -&gt; [Network] -&gt; [Wired 这里打开] -&gt; [IPv4]</code>按如下设置：</p>
<p><img src="/img/hadoop-3.png" alt="" title="网络配置"></p>
<h3 id="管理集群"><a href="#管理集群" class="headerlink" title="管理集群"></a>管理集群</h3><p>在上面的IP等配置好之后，我们选择关闭master，注意不是直接关闭，而是在关闭的时候选择<code>Save the machine state</code>。然后在虚拟机中选中<code>master -&gt; Start 下拉箭头 -&gt; Headless start</code>，然后在我们实体机中通过ssh直接登录到master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@192.168.56.110</span><br></pre></td></tr></table></figure></p>
<p>我们可以在实体机通过配置<code>/etc/hosts</code>，加上如下配置：</p>
<pre><code>192.168.56.110    hadoop-host-master
</code></pre><p>然后就可以通过如下方式登录了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>在实体机中通过下面的配置，就可以无密码登录了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p><strong> 下面的操作，均是在实体机中通过SSH到虚拟机执行的操作。 </strong></p>
<h3 id="安装ssh-openssh-rsync"><a href="#安装ssh-openssh-rsync" class="headerlink" title="安装ssh openssh rsync"></a>安装ssh openssh rsync</h3><p>如系统已安装，则勿略下面的安装操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh openssh-server rsync</span><br></pre></td></tr></table></figure></p>
<p>如果上述命令无法执行，请先执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>JDK的安装请参考我之前的笔记：<a href="../../../../2017/12/08/jdk-install/">安装 JDK</a>，这里不再赘述。安装到此目录<code>/usr/local/jdk1.8.0_102/</code>下，记住此路径，下面会用到。下在进行hadoop的安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xf hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压后得到hadoop-2.7.3目录，hadoop的程序和相关配置就在此目录中。</p>
<h3 id="建保存数据的目录"><a href="#建保存数据的目录" class="headerlink" title="建保存数据的目录"></a>建保存数据的目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p hdfs/tmp</span><br><span class="line">$ mkdir -p hdfs/name</span><br><span class="line">$ mkdir -p hdfs/data</span><br><span class="line">$ mkdir -p journal/data</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 hdfs/</span><br><span class="line">$ chmod -R 777 journal/</span><br></pre></td></tr></table></figure>
<h3 id="配置文件浏览"><a href="#配置文件浏览" class="headerlink" title="配置文件浏览"></a>配置文件浏览</h3><p>hadoop的配置文件都位于下面的目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop</span><br><span class="line">$ ls </span><br><span class="line">capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh</span><br><span class="line">configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template</span><br><span class="line">container-executor.cfg      httpfs-signature.secret  mapred-site.xml.template</span><br><span class="line">core-site.xml               httpfs-site.xml          slaves</span><br><span class="line">hadoop-env.cmd              kms-acls.xml             ssl-client.xml.example</span><br><span class="line">hadoop-env.sh               kms-env.sh               ssl-server.xml.example</span><br><span class="line">hadoop-metrics2.properties  kms-log4j.properties     yarn-env.cmd</span><br><span class="line">hadoop-metrics.properties   kms-site.xml             yarn-env.sh</span><br><span class="line">hadoop-policy.xml           log4j.properties         yarn-site.xml</span><br><span class="line">hdfs-site.xml               mapred-env.cmd</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hadoop-env-sh，加上JDK绝对路径"><a href="#配置hadoop-env-sh，加上JDK绝对路径" class="headerlink" title="配置hadoop-env.sh，加上JDK绝对路径"></a>配置hadoop-env.sh，加上JDK绝对路径</h3><p>JDK的路径就是上面安装JDK的时候的路径：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_102/</span><br></pre></td></tr></table></figure></p>
<h3 id="配置core-site-xml，在该文件中加入如下内容"><a href="#配置core-site-xml，在该文件中加入如下内容" class="headerlink" title="配置core-site.xml，在该文件中加入如下内容"></a>配置core-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-cluster-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-master:2181,hadoop-host-slave-1:2181,hadoop-host-slave-2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hdfs-site-xml，在该文件中加入如下内容"><a href="#配置hdfs-site-xml，在该文件中加入如下内容" class="headerlink" title="配置hdfs-site.xml，在该文件中加入如下内容"></a>配置hdfs-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-cluster-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.hadoop-cluster-ha<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.hadoop-cluster-ha.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.hadoop-cluster-ha.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-slave-1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.hadoop-cluster-ha.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.hadoop-cluster-ha.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-host-slave-1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop-host-slave-1:8485;hadoop-host-slave-2:8485;hadoop-host-slave-3:8485/hadoop-cluster-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.hadoop-cluster-ha<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop-2.7.3/journal/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>至此，master中要安装的通用环境配置完成。在虚拟机中将master复制出slave1、slave2、slave3。并参考上面配置IP地址的方法将slave1的ip配置为:<code>192.168.56.111</code>，slave2的ip配置为：<code>192.168.56.112</code>，slave3的ip配置为：<code>192.168.56.113</code>。</p>
<h3 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h3><p>配置master的主机名为<code>hadoop-host-master</code>，在master节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>配置slave1的主机名为<code>hadoop-host-slave-1</code>，在slave1节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<p>配置slave2的主机名为<code>hadoop-host-slave-2</code>，在slave2节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>配置slave3的主机名为<code>hadoop-host-slave-3</code>，在slave3节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p><strong> 注意：各个节点的主机名一定要不同，否则相同主机名的节点，只会有一个连得上namenode节点，并且集群会报错，修改主机名后，要重启才生效。 </strong></p>
<h3 id="配置域名解析"><a href="#配置域名解析" class="headerlink" title="配置域名解析"></a>配置域名解析</h3><p>分别对master、slave1、slave2、slave3都执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">127.0.0.1	localhost</span><br><span class="line">192.168.56.110	hadoop-host-master</span><br><span class="line">192.168.56.111	hadoop-host-slave-1</span><br><span class="line">192.168.56.112	hadoop-host-slave-2</span><br><span class="line">192.168.56.113	hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<h3 id="集中式管理集群"><a href="#集中式管理集群" class="headerlink" title="集中式管理集群"></a>集中式管理集群</h3><p>配置SSH无密码登陆，分别在master、slave1、slave2和slave3上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>在master、slave1上面执行如下脚本（master和slave1都作为namenode）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop-host-master</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-1</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-2</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>每执行一条命令的时候，都先输入yes，然后再输入目标机器的登录密码。</p>
<p>如果能成功运行如下命令，则配置免密登录其他机器成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop-host-master</span><br><span class="line">$ ssh hadoop-host-slave-1</span><br><span class="line">$ ssh hadoop-host-slave-2</span><br><span class="line">$ ssh hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>在master、slave1上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi slaves    <span class="comment"># 加入如下内容</span></span><br><span class="line">$</span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br><span class="line">hadoop-host-slave-3</span><br></pre></td></tr></table></figure></p>
<p>当执行<code>start-dfs.sh</code>时，它会去slaves文件中找从节点。</p>
<h3 id="安装zookeeper"><a href="#安装zookeeper" class="headerlink" title="安装zookeeper"></a>安装zookeeper</h3><p>我们在master、slave1和slave2上面安装zookeeper集群，安装过程可以参考：<a href="../../../../2017/12/06/zookeeper-cluster/">zookeeper 集群版安装方法</a>，这里不再赘述。</p>
<p>至此，集群配置完成，下面将启动集群。</p>
<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><p>首次启动的时候，先启动<code>journalnode</code>，分别在三台<code>journalnode</code>机器上面启动，因为接下来格式化<code>namenode</code>的时候，数据会写到这些节点中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh start journalnode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 JournalNode</span><br></pre></td></tr></table></figure></p>
<p>接下来在任意一台namenode执行如下命令，我们在master中执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -format    <span class="comment"># 再次启动的时候不需要执行此操作</span></span><br><span class="line">$ ./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 NameNode</span><br></pre></td></tr></table></figure></p>
<p>然后在另一台未格式化的namenode节点，即slave1执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure></p>
<p>然后停掉所有服务，在master下执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">Stopping namenodes on [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: no namenode to stop</span><br><span class="line">hadoop-host-master: stopping namenode</span><br><span class="line">hadoop-host-slave-1: no datanode to stop</span><br><span class="line">hadoop-host-slave-2: no datanode to stop</span><br><span class="line">hadoop-host-slave-3: no datanode to stop</span><br><span class="line">Stopping journal nodes [hadoop-host-slave-1 hadoop-host-slave-2 hadoop-host-slave-3]</span><br><span class="line">hadoop-host-slave-2: stopping journalnode</span><br><span class="line">hadoop-host-slave-1: stopping journalnode</span><br><span class="line">hadoop-host-slave-3: stopping journalnode</span><br><span class="line">Stopping ZK Failover Controllers on NN hosts [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: no zkfc to stop</span><br><span class="line">hadoop-host-master: no zkfc to stop</span><br></pre></td></tr></table></figure></p>
<p>在其中一个namenode上执行格式化ZKFC，我们在master中执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs zkfc -formatZK</span><br><span class="line">$</span><br><span class="line"></span><br><span class="line">18/12/30 12:54:52 INFO ha.ActiveStandbyElector: Session connected.</span><br><span class="line">18/12/30 12:54:52 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/hadoop-cluster-ha <span class="keyword">in</span> ZK.</span><br><span class="line">18/12/30 12:54:52 INFO zookeeper.ClientCnxn: EventThread shut down</span><br><span class="line">18/12/30 12:54:52 INFO zookeeper.ZooKeeper: Session: 0x167fd5512250000 closed</span><br></pre></td></tr></table></figure></p>
<p>再次启动集群的时候，不需执行上面的操作，直接执行如下命令即可，我们在master上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br><span class="line">$</span><br><span class="line"></span><br><span class="line">Starting namenodes on [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: starting namenode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-namenode-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-master: starting namenode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-namenode-hadoop-host-master.out</span><br><span class="line">hadoop-host-slave-2: starting datanode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-datanode-hadoop-host-slave-2.out</span><br><span class="line">hadoop-host-slave-1: starting datanode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-datanode-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-slave-3: starting datanode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-datanode-hadoop-host-slave-3.out</span><br><span class="line">Starting journal nodes [hadoop-host-slave-1 hadoop-host-slave-2 hadoop-host-slave-3]</span><br><span class="line">hadoop-host-slave-2: starting journalnode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-journalnode-hadoop-host-slave-2.out</span><br><span class="line">hadoop-host-slave-1: starting journalnode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-journalnode-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-slave-3: starting journalnode, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-journalnode-hadoop-host-slave-3.out</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [hadoop-host-master hadoop-host-slave-1]</span><br><span class="line">hadoop-host-slave-1: starting zkfc, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-zkfc-hadoop-host-slave-1.out</span><br><span class="line">hadoop-host-master: starting zkfc, logging to /home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-zkfc-hadoop-host-master.out</span><br></pre></td></tr></table></figure></p>
<p>它会自动启动namenode、datanode、journalnode和zkfc，在启动的过程中观看日志，是个好习惯。</p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:50070/" target="_blank" rel="noopener">http://hadoop-host-master:50070/</a><br><a href="http://hadoop-host-slave-1:50070/" target="_blank" rel="noopener">http://hadoop-host-slave-1:50070/</a><br>来查看是否启动成功，如无意外的话，你会看到如下结果页面。其中一个是active，另一个是standby：</p>
<p><img src="/img/hadoop-ha-3.png" alt="" title="hadoop管理界面standby：Overview"></p>
<p><img src="/img/hadoop-ha-4.png" alt="" title="hadoop管理界面active：Overview"></p>
<p>我们在active节点的页面上切换tab到Datanodes可以看到有3个datanode节点，如下图所示：</p>
<p><img src="/img/hadoop-ha-5.png" alt="" title="hadoop管理界面:Datanodes"></p>
<p>切换到<code>Utilities -&gt; Browse the file system</code>，如下图所示（只能在active节点的页面中查看，standby节点对HDFS没有READ权限）：</p>
<p><img src="/img/hadoop-ha-6.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>从上面的界面可以看到，目前HDFS中没有任何文件。我们尝试往其中放一个文件，就将我们的hadoop的压缩包放进去，在<code>active</code>的namenode节点中执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/Downloads/hadoop-2.7.3.tar.gz /</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hadoop supergroup  214092195 2018-12-29 22:07 /hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>我们在图形界面中查看，如下图：</p>
<p><img src="/img/hadoop-ha-7.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>我们点击列表中的文件，将会显示它的数据具体分布在哪些节点上，如下图：</p>
<p><img src="/img/hadoop-ha-8.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p><strong> 注意：在主节点执行<code>start-dfs.sh</code>，主节点的用户名必须和所有从节点的用户名相同。因为主节点服务器以这个用户名去远程登录到其他从节点的服务器中，所以在所有的生产环境中控制同一类集群的用户一定要相同。 </strong></p>
<h3 id="验证failover，即验证两个namenode是否可以自动切换"><a href="#验证failover，即验证两个namenode是否可以自动切换" class="headerlink" title="验证failover，即验证两个namenode是否可以自动切换"></a>验证failover，即验证两个namenode是否可以自动切换</h3><p>我们将<code>active</code>的namenode kill掉，在<code>active</code>的namenode节点上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">2593 QuorumPeerMain</span><br><span class="line">31444 Jps</span><br><span class="line">30613 NameNode</span><br><span class="line">30965 DFSZKFailoverController</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">kill</span> -9 30613</span><br></pre></td></tr></table></figure></p>
<p>我们kill掉之后发现standby无法自动切换到active。我们查看日志，发现：<br>/home/hadoop/hadoop-2.7.3/logs/hadoop-hadoop-zkfc-hadoop-host-slave-1.log<br>有如下内容：</p>
<p><img src="/img/hadoop-ha-9.png" alt=""></p>
<p>结论：两个namenode节点无法自动切换，的原因是操作系统安装的<code>openssh</code>版本和<code>hadoop</code>内部使用的版本不匹配造成的。</p>
<p>解决方案：将<code>$HADOOP_HOME/share</code>目录下的<code>jsch-0.1.42.jar</code>升级到<code>jsch-0.1.54.jar</code>，重启集群，问题解决。</p>
<p>我们首先到maven中央仓库下载<code>jsch-0.1.54.jar</code>：</p>
<pre><code>https://mvnrepository.com/artifact/com.jcraft/jsch/0.1.54
</code></pre><p>我们只需将两个namenode中的<code>jsch-0.1.42.jar</code>升级到<code>jsch-0.1.54.jar</code>即可：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ find ./ -name <span class="string">"*jsch*"</span></span><br><span class="line">$ </span><br><span class="line">./share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/jsch-0.1.42.jar</span><br><span class="line">./share/hadoop/common/lib/jsch-0.1.42.jar</span><br><span class="line">./share/hadoop/tools/lib/jsch-0.1.42.jar</span><br><span class="line">./share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/jsch-0.1.42.jar</span><br></pre></td></tr></table></figure></p>
<p>从查询结果看，只有4个JAR包需要升级，我们只要将两个namenode节点中的JAR包替换即可。重启集群，再次验证<code>failover</code>，我们可以看到两个namenode已经可以自动切换。大功告成。</p>
<h3 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h3><p>YARN的启动步骤和<a href="../../../../2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>一样，这里不再赘述。</p>
<h3 id="active和standby之间的手动切换"><a href="#active和standby之间的手动切换" class="headerlink" title="active和standby之间的手动切换"></a>active和standby之间的手动切换</h3><p>有时候，我们需要手动将某个namenode设置为active，可以通过<code>haadmin</code>命令，相关用法如下（<strong>我一般的做法是将原来active的namenode断网，从而让standby的节点成为active，然后再将之前断网的机器连回网络</strong>）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ ./bin/hdfs haadmin --<span class="built_in">help</span></span><br><span class="line">-<span class="built_in">help</span>: Unknown <span class="built_in">command</span></span><br><span class="line">Usage: haadmin</span><br><span class="line">    [-transitionToActive [--forceactive] &lt;serviceId&gt;]</span><br><span class="line">    [-transitionToStandby &lt;serviceId&gt;]</span><br><span class="line">    [-failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;]</span><br><span class="line">    [-getServiceState &lt;serviceId&gt;]</span><br><span class="line">    [-checkHealth &lt;serviceId&gt;]</span><br><span class="line">    [-<span class="built_in">help</span> &lt;<span class="built_in">command</span>&gt;]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line">-fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include <span class="keyword">in</span> the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is</span><br><span class="line">bin/hadoop <span class="built_in">command</span> [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs haadmin -getServiceState nn1</span><br><span class="line">standby</span><br><span class="line">$ ./bin/hdfs haadmin -getServiceState nn2</span><br><span class="line">active</span><br><span class="line">$ ./bin/hdfs haadmin -transitionToActive --forcemanual nn1</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2019/01/01/hadoop-cluster-ha/" data-id="clf9thvl5000i6h3kyk4mnkub" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop-mapreduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/19/hadoop-mapreduce/" class="article-date">
  <time datetime="2018-12-19T12:06:47.000Z" itemprop="datePublished">2018-12-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/19/hadoop-mapreduce/">hadoop mapreduce 示例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>关于hadoop集群的搭建，请参考我的上一篇 <a href="../../../../2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>，这里将说说如何写一个简单的统计单词个数的<code>mapReduce</code>示例程序，并部署在<code>YARN</code>上面运行。</p>
<p>代码托管在：<a href="https://github.com/hewentian/bigdata">https://github.com/hewentian/bigdata</a></p>
<p>下面详细说明。</p>
<h3 id="第一步：将要统计单词个数的文件放到HDFS中"><a href="#第一步：将要统计单词个数的文件放到HDFS中" class="headerlink" title="第一步：将要统计单词个数的文件放到HDFS中"></a>第一步：将要统计单词个数的文件放到HDFS中</h3><p>例如我们将hadoop安装目录下的<code>README.txt</code>文件放到HDFS中的<code>/</code>目录下，在<code>master</code>节点上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/hadoop-2.7.3/README.txt /</span><br></pre></td></tr></table></figure></p>
<p><code>README.txt</code>文件的内容如下：</p>
<pre><code>For the latest information about Hadoop, please visit our website at:

   http://hadoop.apache.org/core/

and our wiki, at:

   http://wiki.apache.org/hadoop/

This distribution includes cryptographic software.  The country in 
which you currently reside may have restrictions on the import, 
possession, use, and/or re-export to another country, of 
encryption software.  BEFORE using any encryption software, please 
check your country&apos;s laws, regulations and policies concerning the
import, possession, or use, and re-export of encryption software, to 
see if this is permitted.  See &lt;http://www.wassenaar.org/&gt; for more
information.

The U.S. Government Department of Commerce, Bureau of Industry and
Security (BIS), has classified this software as Export Commodity 
Control Number (ECCN) 5D002.C.1, which includes information security
software using or performing cryptographic functions with asymmetric
algorithms.  The form and manner of this Apache Software Foundation
distribution makes it eligible for export under the License Exception
ENC Technology Software Unrestricted (TSU) exception (see the BIS 
Export Administration Regulations, Section 740.13) for both object 
code and source code.

The following provides more details on the included cryptographic
software:
  Hadoop Core uses the SSL libraries from the Jetty project written 
by mortbay.org.
</code></pre><h3 id="第二步：建立一个maven工程"><a href="#第二步：建立一个maven工程" class="headerlink" title="第二步：建立一个maven工程"></a>第二步：建立一个maven工程</h3><p>新建一个maven工程，目录结构如下：</p>
<p><img src="/img/hadoop-mapreduce-1.png" alt="" title="mapreduce工程项目结构"></p>
<p>其中，pom.xml内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hewentian<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop/<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.apache.org<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="第三步：编写mapper程序"><a href="#第三步：编写mapper程序" class="headerlink" title="第三步：编写mapper程序"></a>第三步：编写mapper程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.hadoop.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.MapReduceBase;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Reporter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;WordCountMapper&lt;/b&gt; 是</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018-12-18 23:06:02</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每次调用map方法会传入split中的一行数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key             该行数据在文件中的位置下标</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value           这行数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> outputCollector</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> reporter</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; outputCollector, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="keyword">if</span> (StringUtils.isNotBlank(line)) &#123;</span><br><span class="line">            StringTokenizer st = <span class="keyword">new</span> StringTokenizer(line);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (st.hasMoreTokens()) &#123;</span><br><span class="line">                String word = st.nextToken();</span><br><span class="line">                outputCollector.collect(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>)); <span class="comment">// map 的输出</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="第四步：编写reducer程序"><a href="#第四步：编写reducer程序" class="headerlink" title="第四步：编写reducer程序"></a>第四步：编写reducer程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.hadoop.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.MapReduceBase;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.Reporter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;WordCountReducer&lt;/b&gt; 是</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018-12-18 23:47:12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">MapReduceBase</span> <span class="keyword">implements</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; outputCollector, Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (values.hasNext()) &#123;</span><br><span class="line">            sum += values.next().get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outputCollector.collect(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="第五步：编写job程序"><a href="#第五步：编写job程序" class="headerlink" title="第五步：编写job程序"></a>第五步：编写job程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hewentian.hadoop.mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.JobClient;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.JobConf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;WordCountJob&lt;/b&gt; 是</span></span><br><span class="line"><span class="comment"> * &lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> &lt;a href="mailto:wentian.he@qq.com"&gt;hewentian&lt;/a&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018-12-19 09:05:18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountJob</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"need: input file and output dir."</span>);</span><br><span class="line">            System.out.println(<span class="string">"eg: &#123;HADOOP_HOME&#125;/bin/hadoop jar /home/hadoop/wordCount.jar /README.txt /output/wc/"</span>);</span><br><span class="line">            System.exit(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        JobConf jobConf = <span class="keyword">new</span> JobConf(WordCountJob.class);</span><br><span class="line">        jobConf.setJobName(<span class="string">"word count mapreduce demo"</span>);</span><br><span class="line"></span><br><span class="line">        jobConf.setMapperClass(WordCountMapper.class);</span><br><span class="line">        jobConf.setReducerClass(WordCountReducer.class);</span><br><span class="line">        jobConf.setOutputKeyClass(Text.class);</span><br><span class="line">        jobConf.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// mapreduce 输入数据所在的目录或文件</span></span><br><span class="line">        FileInputFormat.addInputPath(jobConf, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">// mr执行之后的输出数据的目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(jobConf, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        JobClient.runJob(jobConf);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="第六步：将程序打包成JAR文件"><a href="#第六步：将程序打包成JAR文件" class="headerlink" title="第六步：将程序打包成JAR文件"></a>第六步：将程序打包成JAR文件</h3><p>将上述工程打包成JAR文件，并设置默认运行的类为<code>WordCountJob</code>，打包后得文件<code>wordCount.jar</code>，我们将它上传到<code>master</code>节点的<code>home</code>目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scp wordCount.jar hadoop@hadoop-host-master:~/</span><br></pre></td></tr></table></figure></p>
<h3 id="第七步：登录master节点执行JAR文件"><a href="#第七步：登录master节点执行JAR文件" class="headerlink" title="第七步：登录master节点执行JAR文件"></a>第七步：登录master节点执行JAR文件</h3><p>登录master节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>执行JAR文件，若指定的输出目录不存在，HDFS会自动创建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hadoop jar /home/hadoop/wordCount.jar /README.txt /output/wc/</span><br></pre></td></tr></table></figure></p>
<p>执行过程中部分输出如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">18/12/07 02:48:57 INFO client.RMProxy: Connecting to ResourceManager at hadoop-host-master/192.168.56.110:8032</span><br><span class="line">18/12/07 02:48:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop-host-master/192.168.56.110:8032</span><br><span class="line">18/12/07 02:48:58 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.</span><br><span class="line">18/12/07 02:49:00 INFO mapred.FileInputFormat: Total input paths to process : 1</span><br><span class="line"></span><br><span class="line">18/12/07 02:49:00 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">18/12/07 02:49:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544066791145_0005</span><br><span class="line">18/12/07 02:49:00 INFO impl.YarnClientImpl: Submitted application application_1544066791145_0005</span><br><span class="line">18/12/07 02:49:01 INFO mapreduce.Job: The url to track the job: http://hadoop-host-master:8088/proxy/application_1544066791145_0005/</span><br><span class="line">18/12/07 02:49:01 INFO mapreduce.Job: Running job: job_1544066791145_0005</span><br><span class="line">18/12/07 02:49:10 INFO mapreduce.Job: Job job_1544066791145_0005 running in uber mode : false</span><br><span class="line">18/12/07 02:49:10 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/12/07 02:49:20 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/12/07 02:49:27 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">18/12/07 02:49:28 INFO mapreduce.Job: Job job_1544066791145_0005 completed successfully</span><br><span class="line">18/12/07 02:49:28 INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=2419</span><br><span class="line">		FILE: Number of bytes written=360364</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=2235</span><br><span class="line">		HDFS: Number of bytes written=1306</span><br><span class="line">		HDFS: Number of read operations=9</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=2</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=2</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=16581</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=4407</span><br><span class="line">		Total time spent by all map tasks (ms)=16581</span><br><span class="line">		Total time spent by all reduce tasks (ms)=4407</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=16581</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=4407</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=16978944</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=4512768</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=31</span><br><span class="line">		Map output records=179</span><br><span class="line">		Map output bytes=2055</span><br><span class="line">		Map output materialized bytes=2425</span><br><span class="line">		Input split bytes=186</span><br><span class="line">		Combine input records=0</span><br><span class="line">		Combine output records=0</span><br><span class="line">		Reduce input groups=131</span><br><span class="line">		Reduce shuffle bytes=2425</span><br><span class="line">		Reduce input records=179</span><br><span class="line">		Reduce output records=131</span><br><span class="line">		Spilled Records=358</span><br><span class="line">		Shuffled Maps =2</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=2</span><br><span class="line">		GC time elapsed (ms)=364</span><br><span class="line">		CPU time spent (ms)=1510</span><br><span class="line">		Physical memory (bytes) snapshot=480575488</span><br><span class="line">		Virtual memory (bytes) snapshot=5843423232</span><br><span class="line">		Total committed heap usage (bytes)=262725632</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=2049</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=1306</span><br></pre></td></tr></table></figure></p>
<p>等执行成功后，在<code>master</code>节点上查看结果（部分）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -cat /output/wc/*</span><br><span class="line">(BIS),	1</span><br><span class="line">(ECCN)	1</span><br><span class="line">(TSU)	1</span><br><span class="line">(see	1</span><br><span class="line">5D002.C.1,	1</span><br><span class="line">740.13)	1</span><br><span class="line">&lt;http://www.wassenaar.org/&gt;	1</span><br><span class="line">Administration	1</span><br><span class="line">Apache	1</span><br><span class="line">BEFORE	1</span><br><span class="line">BIS	1</span><br><span class="line">Bureau	1</span><br><span class="line">Commerce,	1</span><br><span class="line">Commodity	1</span><br><span class="line">Control	1</span><br><span class="line">Core	1</span><br><span class="line">Department	1</span><br><span class="line">ENC	1</span><br><span class="line">Exception	1</span><br><span class="line">Export	2</span><br><span class="line">For	1</span><br><span class="line">Foundation	1</span><br></pre></td></tr></table></figure></p>
<h3 id="我们在浏览器中查看HDFS和YARN中的数据"><a href="#我们在浏览器中查看HDFS和YARN中的数据" class="headerlink" title="我们在浏览器中查看HDFS和YARN中的数据"></a>我们在浏览器中查看HDFS和YARN中的数据</h3><p>在HDFS管理器中查看：</p>
<p><img src="/img/hadoop-mapreduce-2.png" alt="" title="mapreduce的结果在HDFS中"></p>
<p>在YARN管理器中查看：</p>
<p><img src="/img/hadoop-mapreduce-3.png" alt="" title="mapreduce在YARN中的记录"></p>
<p>大功告成！！！ <strong> （hadoop集群中的时间与我本机的时间不一致，毕竟，很久没启动集群了） </strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/12/19/hadoop-mapreduce/" data-id="clf9thvla000p6h3k01g4ro4i" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hadoop-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/hadoop-cluster/" class="article-date">
  <time datetime="2018-12-04T01:12:43.000Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/04/hadoop-cluster/">hadoop 集群的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说hadoop集群的搭建，这里说的集群是真集群，不是伪集群。不过，这里的真集群是在虚拟机环境中搭建的。</p>
<p>在我的笔记本电脑中，安装虚拟机VirtualBox，在虚拟机中安装三台服务器：master、slave1、slave2来搭建hadoop集群。安装好VirtualBox后，启动它。依次点<code>File -&gt; Host Network Manager -&gt; Create</code>，来创建一个网络和虚拟机中的机器通讯，这个地址是：<code>192.168.56.1</code>，也就是我们外面实体机的地址（仅和虚拟机中的机器通讯使用）。如下图：</p>
<p><img src="/img/hadoop-1.png" alt="" title="虚拟机网络配置"></p>
<p>我们使用<code>ubuntu 18.04</code>来作为我们的服务器，先在虚拟机中安装好一台服务器master，将Jdk、hadoop在上面安装好，然后将master克隆出slave1、slave2。以master为namenode节点，slave1、slave2作为datanode节点。相关配置如下：</p>
<pre><code>master：
    ip: 192.168.56.110
    hostname: hadoop-host-master
slave1:
    ip: 192.168.56.111
    hostname: hadoop-host-slave-1
slave2:
    ip: 192.168.56.112
    hostname: hadoop-host-slave-2
</code></pre><h3 id="下面开始master的安装"><a href="#下面开始master的安装" class="headerlink" title="下面开始master的安装"></a>下面开始master的安装</h3><p>在虚拟机中安装<code>master</code>的过程中我们会设置一个用户用于登录，我们将用户名、密码都设为<code>hadoop</code>，当然也可以为其他名字，其他安装过程略。安装好之后，使用默认的网关配置NAT，NAT可以访问外网，我们将<code>jdk-8u102-linux-x64.tar.gz</code>和<a href="http://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/" target="_blank" rel="noopener">hadoop-2.7.3.tar.gz</a>从它们的官网下载到用户的<code>/home/hadoop/</code>目录下。或在实体机中通过SCP命令传进去。然后将网关设置为<code>Host-only Adapter</code>，如下图所示。</p>
<p><img src="/img/hadoop-2.png" alt="" title="网络配置"></p>
<p>网关设置好了之后，我们接下来配置IP地址。在<code>master</code>中<code>[Settings] -&gt; [Network] -&gt; [Wired 这里打开] -&gt; [IPv4]</code>按如下设置：</p>
<p><img src="/img/hadoop-3.png" alt="" title="网络配置"></p>
<h3 id="管理集群"><a href="#管理集群" class="headerlink" title="管理集群"></a>管理集群</h3><p>在上面的IP等配置好之后，我们选择关闭master，注意不是直接关闭，而是在关闭的时候选择<code>Save the machine state</code>。然后在虚拟机中选中<code>master -&gt; Start 下拉箭头 -&gt; Headless start</code>，然后在我们实体机中通过ssh直接登录到master。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@192.168.56.110</span><br></pre></td></tr></table></figure></p>
<p>我们可以在实体机通过配置<code>/etc/hosts</code>，加上如下配置：</p>
<pre><code>192.168.56.110    hadoop-host-master
</code></pre><p>然后就可以通过如下方式登录了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>在实体机中通过下面的配置，就可以无密码登录了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop@hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p><strong> 下面的操作，均是在实体机中通过SSH到虚拟机执行的操作。 </strong></p>
<h3 id="安装ssh-openssh-rsync"><a href="#安装ssh-openssh-rsync" class="headerlink" title="安装ssh openssh rsync"></a>安装ssh openssh rsync</h3><p>如系统已安装，则勿略下面的安装操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh openssh-server rsync</span><br></pre></td></tr></table></figure></p>
<p>如果上述命令无法执行，请先执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>JDK的安装请参考我之前的笔记：<a href="../../../../2017/12/08/jdk-install/">安装 JDK</a>，这里不再赘述。安装到此目录<code>/usr/local/jdk1.8.0_102/</code>下，记住此路径，下面会用到。下在进行hadoop的安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/</span><br><span class="line">$ tar xf hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压后得到hadoop-2.7.3目录，hadoop的程序和相关配置就在此目录中。</p>
<h3 id="建保存数据的目录"><a href="#建保存数据的目录" class="headerlink" title="建保存数据的目录"></a>建保存数据的目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p hdfs/tmp</span><br><span class="line">$ mkdir -p hdfs/name</span><br><span class="line">$ mkdir -p hdfs/data</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 hdfs/</span><br></pre></td></tr></table></figure>
<h3 id="配置文件浏览"><a href="#配置文件浏览" class="headerlink" title="配置文件浏览"></a>配置文件浏览</h3><p>hadoop的配置文件都位于下面的目录下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop</span><br><span class="line">$ ls </span><br><span class="line">capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh</span><br><span class="line">configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template</span><br><span class="line">container-executor.cfg      httpfs-signature.secret  mapred-site.xml.template</span><br><span class="line">core-site.xml               httpfs-site.xml          slaves</span><br><span class="line">hadoop-env.cmd              kms-acls.xml             ssl-client.xml.example</span><br><span class="line">hadoop-env.sh               kms-env.sh               ssl-server.xml.example</span><br><span class="line">hadoop-metrics2.properties  kms-log4j.properties     yarn-env.cmd</span><br><span class="line">hadoop-metrics.properties   kms-site.xml             yarn-env.sh</span><br><span class="line">hadoop-policy.xml           log4j.properties         yarn-site.xml</span><br><span class="line">hdfs-site.xml               mapred-env.cmd</span><br></pre></td></tr></table></figure></p>
<h3 id="配置hadoop-env-sh，加上JDK绝对路径"><a href="#配置hadoop-env-sh，加上JDK绝对路径" class="headerlink" title="配置hadoop-env.sh，加上JDK绝对路径"></a>配置hadoop-env.sh，加上JDK绝对路径</h3><p>JDK的路径就是上面安装JDK的时候的路径：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_102/</span><br></pre></td></tr></table></figure></p>
<h3 id="配置core-site-xml，在该文件中加入如下内容"><a href="#配置core-site-xml，在该文件中加入如下内容" class="headerlink" title="配置core-site.xml，在该文件中加入如下内容"></a>配置core-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-host-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="配置hdfs-site-xml，在该文件中加入如下内容"><a href="#配置hdfs-site-xml，在该文件中加入如下内容" class="headerlink" title="配置hdfs-site.xml，在该文件中加入如下内容"></a>配置hdfs-site.xml，在该文件中加入如下内容</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/hadoop-2.7.3/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>至此，master中要安装的通用环境配置完成。在虚拟机中将master复制出slave1、slave2。并参考上面配置IP地址的方法将slave1的ip配置为:<code>192.168.56.111</code>，slave2的ip配置为：<code>192.168.56.112</code>。</p>
<h3 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h3><p>配置master的主机名为<code>hadoop-host-master</code>，在master节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-master</span><br></pre></td></tr></table></figure></p>
<p>配置slave1的主机名为<code>hadoop-host-slave-1</code>，在slave1节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-1</span><br></pre></td></tr></table></figure></p>
<p>配置slave2的主机名为<code>hadoop-host-slave-2</code>，在slave2节点执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p><strong> 注意：各个节点的主机名一定要不同，否则相同主机名的节点，只会有一个连得上namenode节点，并且集群会报错，修改主机名后，要重启才生效。 </strong></p>
<h3 id="配置域名解析"><a href="#配置域名解析" class="headerlink" title="配置域名解析"></a>配置域名解析</h3><p>分别对master、slave1和slave2都执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/hosts</span><br><span class="line"></span><br><span class="line">修改为如下内容</span><br><span class="line">127.0.0.1	localhost</span><br><span class="line">192.168.56.110	hadoop-host-master</span><br><span class="line">192.168.56.111	hadoop-host-slave-1</span><br><span class="line">192.168.56.112	hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>至此，集群配置完成，下面将启动集群。</p>
<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><p>首先启动namenode节点，也就是master，首次启动的时候，要格式化namenode。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs namenode -format    <span class="comment"># 再次启动的时候不需要执行此操作</span></span><br><span class="line">$ ./sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">4016 Jps</span><br><span class="line">2556 NameNode</span><br></pre></td></tr></table></figure></p>
<p>接下来启动datanode节点，也就是slave1、slave2，在这两台服务器上都执行如下启动脚本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh start datanode</span><br><span class="line">$</span><br><span class="line">$ jps    <span class="comment"># 查看是否启动成功</span></span><br><span class="line">2451 Jps</span><br><span class="line">2162 DataNode</span><br></pre></td></tr></table></figure></p>
<p>可以在实体机的浏览器中输入：<br><a href="http://hadoop-host-master:50070/" target="_blank" rel="noopener">http://hadoop-host-master:50070/</a><br>来查看是否启动成功。</p>
<p><img src="/img/hadoop-4.png" alt="" title="hadoop管理界面：Overview"></p>
<p>切换tab到Datanodes可以看到有2个datanode节点，如下图所示：</p>
<p><img src="/img/hadoop-5.png" alt="" title="hadoop管理界面:Datanodes"></p>
<p>切换到<code>Utilities -&gt; Browse the file system</code>，如下图所示：</p>
<p><img src="/img/hadoop-6.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>从上面的界面可以，目前HDFS中没有任何文件。我们尝试往其中放一个文件，就将我们的hadoop压缩包放进去，在namenode节点中执行如下操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hdfs dfs -put /home/hadoop/hadoop-2.7.3.tar.gz /</span><br><span class="line"></span><br><span class="line">$ ./bin/hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup  214092195 2018-12-06 12:20 /hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>我们在图形界面中查看，如下图：</p>
<p><img src="/img/hadoop-7.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<p>我们点击列表中的文件，将会显示它的数据具体分布在哪些节点上，如下图：</p>
<p><img src="/img/hadoop-8.png" alt="" title="hadoop管理界面:Browse the file system"></p>
<h3 id="停掉集群"><a href="#停掉集群" class="headerlink" title="停掉集群"></a>停掉集群</h3><p>先停掉datanode节点，在slave1、slave2上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure></p>
<p>然后停掉namenode节点，在master上面执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/hadoop-daemon.sh stop namenode</span><br></pre></td></tr></table></figure></p>
<h3 id="集中式管理集群"><a href="#集中式管理集群" class="headerlink" title="集中式管理集群"></a>集中式管理集群</h3><p>如果我们的集群里面有成千上万台机器，在每一台机器上面都这样来启动，肯定是不行的。下面我们将通过配置，只在一台机器上面执行一个脚本，就将整个集群启动。</p>
<p>配置SSH无密码登陆，分别在master、slave1和slave2上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>在master上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id hadoop-host-master</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-1</span><br><span class="line">$ ssh-copy-id hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>每执行一条命令的时候，都先输入yes，然后再输入目标机器的登录密码。</p>
<p>如果能成功运行如下命令，则配置免密登录其他机器成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh hadoop-host-master</span><br><span class="line">$ ssh hadoop-host-slave-1</span><br><span class="line">$ ssh hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>在master上面执行如下脚本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi slaves    <span class="comment"># 加入如下内容</span></span><br><span class="line">$</span><br><span class="line">hadoop-host-slave-1</span><br><span class="line">hadoop-host-slave-2</span><br></pre></td></tr></table></figure></p>
<p>当执行<code>start-dfs.sh</code>时，它会去slaves文件中找从节点。</p>
<p>在master上面启动整个集群：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面通过<code>jps</code>命令可以看到整个集群已经成功启动。同样的，停掉整个集群的命令，如下，同样是在master上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></p>
<p>相关操作，如下图所示：</p>
<p><img src="/img/hadoop-9.png" alt="" title="hadoop集中式管理"></p>
<p><strong> 注意：在主节点执行<code>start-dfs.sh</code>，主节点的用户名必须和所有从节点的用户名相同。因为主节点服务器以这个用户名去远程登录到其他从节点的服务器中，所以在所有的生产环境中控制同一类集群的用户一定要相同。 </strong></p>
<h3 id="启动YARN"><a href="#启动YARN" class="headerlink" title="启动YARN"></a>启动YARN</h3><p>分别在master、slave1和slave2上面都建如下目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3</span><br><span class="line">$ mkdir -p yarn/nm</span><br><span class="line">$</span><br><span class="line">$ chmod -R 777 yarn/</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面都按如下方式配置mapred-site.xml，刚解压的hadoop是没有mapred-site.xml的，但是有mapred-site.xml.template，我们修改文件名，并作如下配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ mv mapred-site.xml.template mapred-site.xml</span><br><span class="line">$ vi mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>分别在master、slave1和slave2上面都按如下方式配置yarn-site.xml<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/etc/hadoop/</span><br><span class="line">$ vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定ResourceManager的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop-host-master&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定reducer获取数据的方式 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/home/hadoop/hadoop-2.7.3/yarn/nm&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>配置好了，下面开始启动：<br>在master上面启动resourcemanager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure></p>
<p>在slave1、slave2上面分别启动nodemanager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure></p>
<p>我们可以通过浏览器，查看资源管理器：<br><a href="http://hadoop-host-master:8088/" target="_blank" rel="noopener">http://hadoop-host-master:8088/</a></p>
<p><img src="/img/hadoop-10.png" alt="" title="yarn资源管理"></p>
<p>点击图中的<code>Active Nodes</code>可以看到下图的详情，（如果<code>Unhealthy Nodes</code>有节点，则可能是由于虚拟机中主机的磁盘空间不足所致）。</p>
<p><img src="/img/hadoop-11.png" alt="" title="yarn资源管理"></p>
<p>当然相应的停止命令如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/yarn-daemon.sh stop resourcemanager</span><br><span class="line">$ ./sbin/yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure></p>
<p>如果有配置集中式管理，我们也可以通过在master上面通过一个命令启动、停止YARN<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-yarn.sh    <span class="comment"># 启动yarn</span></span><br><span class="line">$ ./sbin/stop-yarn.sh    <span class="comment"># 停止yarn</span></span><br></pre></td></tr></table></figure></p>
<p>或者在master上面，通过一个命令启动hadoop和yarn<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-all.sh</span><br><span class="line">或者按顺序执行如下两个命令</span><br><span class="line">$ ./sbin/start-dfs.sh</span><br><span class="line">$ ./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hadoop-12.png" alt="" title="yarn集中启动管理"></p>
<h3 id="启动MR作业日志管理器"><a href="#启动MR作业日志管理器" class="headerlink" title="启动MR作业日志管理器"></a>启动MR作业日志管理器</h3><p>在namenode节点，也就是master，启动MR作业日志管理器。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></p>
<p>它同样有自已的图形界面：<br><a href="http://hadoop-host-master:19888/" target="_blank" rel="noopener">http://hadoop-host-master:19888/</a></p>
<p><img src="/img/hadoop-13.png" alt="" title="MR作业日志管理器"></p>
<h3 id="尝试向集群中提交一个mapReduce任务"><a href="#尝试向集群中提交一个mapReduce任务" class="headerlink" title="尝试向集群中提交一个mapReduce任务"></a>尝试向集群中提交一个mapReduce任务</h3><p>我们在namenode节点中向集群提交一个计算圆周率的mapReduce任务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hadoop/hadoop-2.7.3/</span><br><span class="line">$ ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 4 10</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/hadoop-14.png" alt="" title="计算圆周率"></p>
<p><img src="/img/hadoop-15.png" alt="" title="计算圆周率"></p>
<p>从上图可以看出，圆周率已经被计算出来：<code>3.40</code>。另外，在yarn中也可以看到任务的执行情况：<br><img src="/img/hadoop-16.png" alt="" title="计算圆周率"></p>
<p>至此， 集群搭建完毕。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/12/04/hadoop-cluster/" data-id="clf9thvl8000l6h3kwj2blvf4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-kafka-intro" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/12/kafka-intro/" class="article-date">
  <time datetime="2018-11-12T01:04:49.000Z" itemprop="datePublished">2018-11-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/12/kafka-intro/">kafka 介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这是一篇译文，因英文水平有限，翻译未免有不足之处。如果想看原文，请访问这里：<br><a href="http://kafka.apache.org/intro" target="_blank" rel="noopener">http://kafka.apache.org/intro</a></p>
<p><strong> 如果想简单体验一下kafka，可以阅读我上两篇介绍的 <a href="../../../../2018/10/24/kafka-standalone/">kafka 单节点安装</a>、<a href="../../../../2018/10/27/kafka-cluster/">kafka 集群的搭建</a> </strong></p>
<h3 id="Apache-Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？"><a href="#Apache-Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？" class="headerlink" title="Apache Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？"></a>Apache Kafka是一个分布式的流媒体平台，那么，它到底指的是什么呢？</h3><p>流媒体平台有3个主要的性能指标：</p>
<ol>
<li>发布和订阅消息流，类似于消息队列或者企业消息系统；</li>
<li>以容错方式、持久化存储流数据；</li>
<li>实时处理流数据。</li>
</ol>
<p>kafka通常应用于两种广泛的场景：</p>
<ol>
<li>在系统或应用程序之间构建可靠的用于传输实时数据的管道; </li>
<li>构建实时的流数据处理程序，来转换或处理数据流。</li>
</ol>
<h3 id="为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能"><a href="#为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能" class="headerlink" title="为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能"></a>为了弄清楚kafka到底是怎样完成这些功能的，从下面开始我们钻研和探究一下kafka的功能</h3><p>首先，了解一下几个概念：</p>
<ol>
<li>kafka可以以集群方式运行于一台或者多台服务器，这些服务器可以分布在不同的数据中心；</li>
<li>kafka集群将流式数据分类存储，这种类别通常被称为主题；</li>
<li>每一条消息由键、值和时间戳组成。</li>
</ol>
<p><img src="/img/kafka-1.png" alt="" title="来源：http://kafka.apache.org/20/images/kafka-apis.png"></p>
<p>kafka有4个核心API：</p>
<ol>
<li><a href="http://kafka.apache.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a>允许应用程序将一条消息发布到一个或者多个kafka主题中；</li>
<li><a href="http://kafka.apache.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>允许应用程序订阅一个或者多个主题，并且处理被发往其中的数据流；</li>
<li><a href="http://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>允许一个应用充当流式处理器：从作为输入流的一个或多个主题中消费消息，然后将处理过的消息输出到另一个或多个主题中。高效地将输入流的数据转换、传输到输出流中；</li>
<li><a href="http://kafka.apache.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>允许建立和重用已有的生产者或消费者，它们连接着某个kafka主题，而这些主题是和已存在的应用和数据系统连接着的。例如，关系数据库的连接器将会捕获对表的每一个修改。</li>
</ol>
<p>在kafka中，客户端和服务器端的通讯是通过一个简单、高效、语言无关的<a href="https://kafka.apache.org/protocol.html" target="_blank" rel="noopener">TCP协议</a>完成的。此协议是有版本代差的，但新版本向后兼容旧版本。我们提供一个JAVA客户端连接kafka，但是其他语言的客户端也提供。消费者和服务端建立的是长连接。</p>
<h3 id="主题和日志（存储策略）"><a href="#主题和日志（存储策略）" class="headerlink" title="主题和日志（存储策略）"></a>主题和日志（存储策略）</h3><p>我们首先钻研一下kafka中为处理流记录而提供的核心抽象概念–主题。</p>
<p>主题就是一个分类，或者说是专为发布消息而命名的。在kafka中主题通常有多个订阅者，也就是说一个主题可以有零个、一个或者多个消费者，这些消费者都订阅写往其中的消息。</p>
<p>对每一个主题，kafka集群都使用分区存储，像下面这样：<br><img src="/img/kafka-2.png" alt="" title="来源：http://kafka.apache.org/20/images/log_anatomy.png"></p>
<p>每一个分区中的消息都是按顺序存储的，持续往该分区中存放的数据的顺序都是不可改变的，结构化存储。分区中的每一条消息都会被分配一个有序的ID号，被称为偏移量，用于唯一标示该分区中的每一条消息。</p>
<p>kafka集群会持久化发布到它的每一条消息，无论它们是否已经被消费过，可以通过配置文件配置该消息存放多久。例如，如果保存策略被设置为2天，那么当一条消息发布2天之内，它都是可以被消费的，只是一旦被消费之后，它就会被删掉以释放空间。kafka是持续高性能的，这与存储于它的数据大小关系不大，因此长期保存数据，都是没问题的。</p>
<p><img src="/img/kafka-3.png" alt="" title="来源：http://kafka.apache.org/20/images/log_consumer.png"></p>
<p>实际上，唯一存储于每一个消费者中的元数据是偏移量或者该消费者在这个分区中访问存储数据的位置。偏移量由消费者控制：通常，消费者中保存的偏移量随着它消费消息，将呈线性增长。但是，实际上，由于这个偏移量是由消费者控制的，所以它可以指定它消费的任何位置上的消息。例如：一个消费者可以重置到一个旧的偏移量来处理旧的数据或者跳过大部分记录，然后从当前位置开始消费消息。每个消费者的偏移量在kafka服务器中也是有存储的。</p>
<p>这个组合功能意味着kafka消费者是轻量级的，它们的连接和断开对集群和其他消费者影响极小。例如，你可以使用我们的命令行工具去不停地显示最新添加到某个主题的内容，而这，不会对任何订阅这个主题的消费者产生影响。</p>
<p>分区在存储中扮演着不同的目的：首先，它允许存储的数据量超过单台服务器允许的规模。每个单独的分区能储存的数据量取决于它所在的服务器磁盘的大小等因素，但是一个主题可以有多个分区，因此它能储存任意数量的数据。其次，分区充当并行处理的单元–同时能处理的并发数。</p>
<h3 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h3><p>一个主题的分区分布于kafka集群中的多台服务器中，每一台服务器都可以处理数据和向共享分区发送请求。为了容错，每一个分区都可以配置一定的副本数。</p>
<p>每一个分区都有一台服务器担当主服务器，可以有零个或者多个从服务器。主服务器处理对分区的所有读和写请求，从服务器由主服务器调度。如果主服务器挂掉了，从服务器中会自动产生一个新的主服务器。集群中的每一台服务器既充当某个分区的主服务器，又充当其他分区的从服务器，所以整个集群是负载均衡的。</p>
<h3 id="地域复制"><a href="#地域复制" class="headerlink" title="地域复制"></a>地域复制</h3><p>kafka的MirrorMaker为你的集群提供跨地域复制支持。使用MirrorMaker，消息可以跨越多个数据中心或者不同的云区进行同步。你可以在主从模式下用于备份和恢复，或者在主主模式下使数据更靠近你的用户，或者支持数据本地请求。</p>
<h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>生产者往它们选定的主题中发送消息的时候，应该为每一条消息指定它要发送到的分区。我们可以使用环形策略，简单地使数据平均分配于所有分区中，也可以根据消息中的语义来自动地选择分区。接下来将会说下分区的使用。</p>
<h3 id="消息者"><a href="#消息者" class="headerlink" title="消息者"></a>消息者</h3><p>我们可以对消费者分组，每个组有一个组名。每一条发送到指定主题中的消息，都会被订阅了这个主题的同一个组中的一个消费者消费。同一个组中的消费者可以在同一台机器或者多台机器中。</p>
<p>如果所有消费者实例都在同一个组中，那么所有消息都会高效地平均发送到所有消费者实例。<br>如果所有消费者实例分布在不同的组中，那么每条消息都会被广播到所有组中的一个消费者。</p>
<p><img src="/img/kafka-4.png" alt="" title="来源：http://kafka.apache.org/20/images/consumer-groups.png"></p>
<p>上图所示：该kafka集群有2台服务器，4个分区（P0-P3），有2个消费者组。消费者组A有2个消费者实例，而消费者组B有4个。</p>
<p>通常，主题都会有少量的消费者组，在逻辑上看，一个消费者组就是一个订阅者。每个组包含多个消费者，这能很好的实现扩展和容错。在订阅的语义上：订阅者只不过是一群消费者，而不是一个。</p>
<p>消费的方式在kafka中的实现是通过将分区分配给所有消费者实例，因此在任何时刻，每一个实例都是一个”公平共享“分区的唯一消费者。维护组中成员关系的方式在kafak中是通过kafka协议自动实现的：如果新的实例加进组，那么它将从其他组员中获取一个分区（如果这个组员处理两个以上分区）；如果一个实例挂掉了，那么它所处理的分区将被分配给组中剩下的成员们。</p>
<p>kafka对每一个分区中的消息都只提供一个总的顺序，同一个主题中不同分区中的顺序各不相同。每个分区排序组织该分区中数据的能力能满足大部分应用的需求。但是，如果你想要一个所有消息的总顺序，可以通过为这个主题设置一个分区来实现，不过，这意味着一个消费者组中只能有一个消费者来处理该主题的消息。</p>
<h3 id="多租户架构"><a href="#多租户架构" class="headerlink" title="多租户架构"></a>多租户架构</h3><p>你可以以多租户架构方式部署kafka。多租户架构可以通过配置，来指定哪个主题可以生产和消费数据，并且支持设置操作指标。管理员可以定义和限制所有请求的指标，以控制客户端能使用的服务器资源数。</p>
<p>更多相关信息，请访问<a href="https://kafka.apache.org/documentation/#security" target="_blank" rel="noopener">这里</a>。</p>
<h3 id="保证"><a href="#保证" class="headerlink" title="保证"></a>保证</h3><p>在高层次看kafka提供以下保证：</p>
<ol>
<li>一个生产者发往指定主题中指定分区中的消息，将会按照它们发送的顺序出现。例如：如果一个生产者发送消息M1、M2，如果M1先发送，那么M1将会首先出现在那个分区中，并且M1的顺序号要比M2的小；</li>
<li>消费者按顺序读取存储在主题中的消息；</li>
<li>如果一个主题有 N 个副本，那么我们能承受高达 N-1 台服务器同时挂掉，而不会丢失任何消息。</li>
</ol>
<p>更多关于这些保证的描述将在相关章节中详细说明。</p>
<h3 id="kafka作为一个消息系统"><a href="#kafka作为一个消息系统" class="headerlink" title="kafka作为一个消息系统"></a>kafka作为一个消息系统</h3><p>kafka的流媒体概念与传统的企业消息系统相比有什么不同？</p>
<p>消息传输在传统上有2种模型：队列和发布-订阅。在队列模型中，一组消费者从一台服务器读取消息，每个消息只会发送到其中一个消费者；在发布-订阅模型中，每条消息都会广播到所有消费者。这两种模型各自都有优势和不足。队列的优势是允许你将消息平均分配给所有消费者处理，这能扩展系统的处理能力。不幸的是，队列不能有多个订阅者，消息一旦被其中一个消费者读取就会被删掉。发布-订阅模型允许你将消息广播到所有消费者，这种方式不能扩展处理能力，因为每条消息都会被发送到所有订阅者。</p>
<p>消费者组的概念在kafka中通常包含上述两种概念。对队列来说，消费者组允许你将数据平均分配给所有消费者组来处理；对发布-订阅来说，kafka允许你将消息广播到所有消费者组。</p>
<p>kafka模型的优势是每个主题都有这两种属性：它能扩展处理能力，同时也支持多个订阅者。我们不需要选择其中一个，或者另外一个。</p>
<p>kafka相比于传统的消息系统，它更能保证消息的顺序。</p>
<p>传统队列会将消息按顺序保存在服务器上，如果多个消费者同时消费这个队列，那么服务器将按消息的存储顺序来分发给消费者。然而，尽管服务器按顺序分发消息，但是消息是异步的发送到每个消费者的，所以不同的消费者接收到消息的顺序可能不同。这意味着，在并行处理的情况下，消息的顺序将不能保证。在消息系统中通常有一个概念：唯一消费者，它允许只有一个消费者消费一个队列，当然这也意味着这种情况下不存在并行处理。</p>
<p>kafka在这方面做得比较好。在主题中，它有一个并行的概念–分区。kafka既能保证消息的顺序，又能在多个消费者之间保持负载均衡。通过将主题的分区分配给指定的消费者组，每个分区只能被消费者组中的一个消费者消费，来实现的。这样，我们能确保这个消费者是这个分区的唯一消费者和按顺序消费这个分区中的消息。尽管主题有很多个分区，我们仍能在多个消费者实例之间保持负载均衡。值得注意的是，消费者组中消费者的数量不能多于分区数。</p>
<h3 id="kafka作为一个存储系统"><a href="#kafka作为一个存储系统" class="headerlink" title="kafka作为一个存储系统"></a>kafka作为一个存储系统</h3><p>任何允许发布与消费消息分离的消息队列，实际上充当了目前使用的消息存储系统。kafka的不同之处在于它还是一个非常优秀的存储系统。</p>
<p>写入kafka的数据将会写入磁盘，并且进行副本复制以实现容错。kafka允许生产者等待确认，在收到回复之后才会认为写成功，并且即使写入的服务器失败了，也能保证这条消息是存在的。</p>
<p>kafka能很好地使用磁盘结构来扩容：无论服务器上有 50KB 还是 50TB 的持久化数据，kafka的性能都是一样的，不会随着数据的增多而出现性能下降。</p>
<p>由于kafka可以大规模的存储数据，并且允许客户控制其读取位置，您可以将kafka作为一种专用于高性能、低延迟提交日志存储，并且能复制和传播的分布式文件系统。</p>
<p>更多关于kafka的提交日志存储和副本复制的设计，请访问<a href="https://kafka.apache.org/documentation/#design" target="_blank" rel="noopener">这里</a>。</p>
<h3 id="kafka作为一个流媒体处理系统"><a href="#kafka作为一个流媒体处理系统" class="headerlink" title="kafka作为一个流媒体处理系统"></a>kafka作为一个流媒体处理系统</h3><p>kafka仅仅提供读取、写入和存储数据流是不够的，最终目的是实现流的实时处理。</p>
<p>在kafka中，流处理器是指持续地从输入主题获取数据流，对获取到的数据流执行某种处理，并将处理过的数据，持续地输出到输出主题中。</p>
<p>例如，零售店的应用程序可能会将销售额和货物作为输入流，通过相关计算，然后输出重新排序和根据此数据计算的价格调整的流。</p>
<p>可以直接使用生产者和消费者的相关API进行简单处理。但是，对于更复杂的转换，kafka提供了完全集成的Streams API。这允许构建一些应用程序去执行非普通处理任务、计算流的聚合或者将流连接在一起。</p>
<p>它能有效地解决此类应用程序面临的难题：处理无序数据，在代码更改时重新处理输入流，执行有状态计算等。</p>
<p>流式API构建在kafka提供的核心基础功能上：它使用生产者和消费者API进行输入，使用kafka进行有状态存储，并在流处理器实例之间使用相同的组机制来实现容错。</p>
<h3 id="把碎片整合在一起"><a href="#把碎片整合在一起" class="headerlink" title="把碎片整合在一起"></a>把碎片整合在一起</h3><p>将消息传递、存储和流处理组合在一起可能看起来没多大用处，但它对于kafka作为流媒体平台的作用至关重要。</p>
<p>像HDFS这种分布式文件系统允许存储静态文件以进行批处理。kafka系统是高效的，它允许存储和处理过去的历史数据。</p>
<p>传统的企业消息系统允许处理你订阅之后到达的数据。以这种方式构建的应用程序只能处理在它订阅之后到达的未来数据。</p>
<p>kafka结合了这两种功能，这种组合对于kafka作为流媒体应用程序平台以及流数据管道的使用至关重要。</p>
<p>通过组合存储和低延迟订阅，流应用程序可以以相同的方式处理过去和未来的数据。也就是说，单个应用程序也可以处理历史存储的数据，而不是在它处理到达最后一条记录时结束，它可以在未来数据到达时继续处理。这就是流处理包含批处理以及消息驱动应用程序的一般概念。</p>
<p>同样，对于流数据管道，通过组合订阅实时事件，可以将kafka用作极低延迟的管道; 另外，能够可靠地存储数据，也使得可以将其用于必须保证安全的核心数据的传输，或者与仅定期加载数据的离线系统或可能长时间停机以进行扩展和维护集成。它的流处理能力使它可以实时的转换数据。</p>
<p>更多关于kafka提供的保证、API和功能的信息，请参阅其余的<a href="http://kafka.apache.org/documentation.html" target="_blank" rel="noopener">文档</a>。</p>
<p>这里介绍一个很好用的kafka可视化工具kafkatool，下载地址：<br><a href="http://www.kafkatool.com/" target="_blank" rel="noopener">http://www.kafkatool.com/</a></p>
<p>kafkatool连接kafka服务器后，记得要将kafkatool中topic的Message类型设置为String，否则将看到字节码。</p>
<h3 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --bootstrap-server broker_host:port --create --topic my-topic --partitions 1 \</span><br><span class="line">    --replication-factor 1 --config max.message.bytes=64000 --config flush.messages=1</span><br></pre></td></tr></table></figure>
<h3 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h3><p>增加分区数，分区只能增加不能减少<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic my_topic_name \</span><br><span class="line">    --partitions 40</span><br></pre></td></tr></table></figure></p>
<h3 id="删除主题"><a href="#删除主题" class="headerlink" title="删除主题"></a>删除主题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --bootstrap-server broker_host:port --delete --topic my_topic_name</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/11/12/kafka-intro/" data-id="clf9thvmd00216h3k9k2l2lcg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-kafka-cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/27/kafka-cluster/" class="article-date">
  <time datetime="2018-10-27T05:11:43.000Z" itemprop="datePublished">2018-10-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/27/kafka-cluster/">kafka 集群的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说kafka集群的搭建，如果你只是想简单体验一下kafka，可以直接使用我在上一篇介绍的 <a href="../../../../2018/10/24/kafka-standalone/">kafka 单节点安装</a> 即可。</p>
<p>但是，如果你想在生产环境中使用，那么搭建一个集群可能更适合你。下面将说说kafka集群的安装使用，kafka同样是使用前面例子使用的<code>2.0.0</code>版本，我在一台机器上安装，所以这是伪集群，当修改为真集群的时候，只要将IP地址修改下即可，下面会说明。</p>
<p>首先，你得搭建 zookeeper 集群，因为高版本的kafka中内置了zookeeper组件，所以我们直接使用kafka中内置的zookeeper组件搭建zookeeper集群。但是，你也可以使用zookeeper独立的安装包来搭建zookeeper集群。两者的搭建方法都是一样的，可以参考 <a href="../../../../2017/12/06/zookeeper-cluster/">zookeeper集群版安装方法</a></p>
<h3 id="计划在一台Ubuntu-Linux服务器上部署3台kafka服务器，分别为kafka1-kafka2-kafka3"><a href="#计划在一台Ubuntu-Linux服务器上部署3台kafka服务器，分别为kafka1-kafka2-kafka3" class="headerlink" title="计划在一台Ubuntu Linux服务器上部署3台kafka服务器，分别为kafka1, kafka2, kafka3"></a>计划在一台<code>Ubuntu Linux</code>服务器上部署3台<code>kafka</code>服务器，分别为<code>kafka1</code>, <code>kafka2</code>, <code>kafka3</code></h3><p>因为三台<code>kafka</code>服务器的配置都差不多，所以我们先设置好一台<code>kafka1</code>的配置，再将其复制成<code>kafka2</code>, <code>kafka3</code>并修改其中的配置即可。</p>
<p>下面使用kafka内置的zookeeper组件搭建zookeeper集群，我们将kafka的所有服务器都放在同一个目录下：</p>
<h3 id="1-建目录，如下："><a href="#1-建目录，如下：" class="headerlink" title="1.建目录，如下："></a>1.建目录，如下：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD</span><br><span class="line">$ mkdir kafkaCluster</span><br></pre></td></tr></table></figure>
<h3 id="2-将kafka-2-12-2-0-0-tgz放到-home-hewentian-ProjectD-kafkaCluster目录下，并执行如下脚本解压"><a href="#2-将kafka-2-12-2-0-0-tgz放到-home-hewentian-ProjectD-kafkaCluster目录下，并执行如下脚本解压" class="headerlink" title="2.将kafka_2.12-2.0.0.tgz放到/home/hewentian/ProjectD/kafkaCluster目录下，并执行如下脚本解压"></a>2.将<code>kafka_2.12-2.0.0.tgz</code>放到<code>/home/hewentian/ProjectD/kafkaCluster</code>目录下，并执行如下脚本解压</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster</span><br><span class="line">$ tar xzvf kafka_2.12-2.0.0.tgz</span><br><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">kafka_2.12-2.0.0  kafka_2.12-2.0.0.tgz</span><br><span class="line"></span><br><span class="line">$ rm kafka_2.12-2.0.0.tgz</span><br><span class="line">$ mv kafka_2.12-2.0.0/ kafka1  <span class="comment"># 为方便起见，将其命名为 kafka1</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> kafka1/</span><br><span class="line">$ mkdir -p data/zk     <span class="comment"># 存放zookeeper数据的目录</span></span><br><span class="line">$ mkdir -p data/kafka  <span class="comment"># 存放kafka数据的目录</span></span><br><span class="line">$ mkdir logs           <span class="comment"># 新解压的 kafka 没有此目录，需手动创建。因为重定向的日志logs/zookeeper.log需要此目录</span></span><br></pre></td></tr></table></figure>
<h3 id="3-修改-home-hewentian-ProjectD-kafkaCluster-kafka1-config-zookeeper-properties并在其中修改如下内容："><a href="#3-修改-home-hewentian-ProjectD-kafkaCluster-kafka1-config-zookeeper-properties并在其中修改如下内容：" class="headerlink" title="3.修改/home/hewentian/ProjectD/kafkaCluster/kafka1/config/zookeeper.properties并在其中修改如下内容："></a>3.修改<code>/home/hewentian/ProjectD/kafkaCluster/kafka1/config/zookeeper.properties</code>并在其中修改如下内容：</h3><pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk  # 这里必须为绝对路径，否则有可能无法启动
clientPort=2181                                               # 这台服务器的端口为2181这里为默认值
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890
</code></pre><h3 id="4-在-home-hewentian-ProjectD-kafkaCluster-kafka1-data-zk目录下建myid文件并在其中输入1，只输入1，代表server-1"><a href="#4-在-home-hewentian-ProjectD-kafkaCluster-kafka1-data-zk目录下建myid文件并在其中输入1，只输入1，代表server-1" class="headerlink" title="4.在/home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk目录下建myid文件并在其中输入1，只输入1，代表server.1"></a>4.在<code>/home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk</code>目录下建<code>myid</code>文件并在其中输入1，只输入1，代表server.1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1/data/zk</span><br><span class="line">$ vi myid</span><br></pre></td></tr></table></figure>
<p>这样第一台服务器已经配置完毕。</p>
<h3 id="5-接下来我们将kafka1复制为kafka2-kafka3"><a href="#5-接下来我们将kafka1复制为kafka2-kafka3" class="headerlink" title="5.接下来我们将kafka1复制为kafka2, kafka3"></a>5.接下来我们将<code>kafka1</code>复制为<code>kafka2</code>, <code>kafka3</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster</span><br><span class="line">$ cp -r kafka1 kafka2</span><br><span class="line">$ cp -r kafka1 kafka3</span><br></pre></td></tr></table></figure>
<h3 id="6-将kafka2-data-zk目录下的myid的内容修改为2"><a href="#6-将kafka2-data-zk目录下的myid的内容修改为2" class="headerlink" title="6.将kafka2/data/zk目录下的myid的内容修改为2"></a>6.将<code>kafka2/data/zk</code>目录下的<code>myid</code>的内容修改为2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2/data/zk</span><br><span class="line">$ vi myid</span><br></pre></td></tr></table></figure>
<p>同理，将将<code>kafka3/data/zk</code>目录下的<code>myid</code>的内容修改为3<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3/data/zk</span><br><span class="line">$ vi myid</span><br></pre></td></tr></table></figure></p>
<h3 id="7-修改kafka2的配置文件"><a href="#7-修改kafka2的配置文件" class="headerlink" title="7.修改kafka2的配置文件"></a>7.修改<code>kafka2</code>的配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2/config</span><br><span class="line">$ vi zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>仅修改两处地方即可，要修改的地方如下：</p>
<pre><code>dataDir=/home/hewentian/ProjectD/kafkaCluster/kafka2/data/zk  # 这里是数据保存的位置
clientPort=2182                                               # 这台服务器的端口为2182
</code></pre><p>同理，修改<code>kafka3</code>的配置文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3/config</span><br><span class="line">$ vi zookeeper.properties</span><br></pre></td></tr></table></figure></p>
<p>仅修改两处地方即可，要修改的地方如下：</p>
<pre><code>dataDir=/home/hewentian/ProjectD/kafkaCluster/kafka3/data/zk  # 这里是数据保存的位置
clientPort=2183                                               # 这台服务器的端口为2183
</code></pre><h3 id="8-到目前为此，我们已经将3台zookeeper服务器都配置好了。接下来，我们要将他们都启动"><a href="#8-到目前为此，我们已经将3台zookeeper服务器都配置好了。接下来，我们要将他们都启动" class="headerlink" title="8.到目前为此，我们已经将3台zookeeper服务器都配置好了。接下来，我们要将他们都启动"></a>8.到目前为此，我们已经将3台<code>zookeeper</code>服务器都配置好了。接下来，我们要将他们都启动</h3><p>启动kafka1的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<p>启动kafka2的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ mkdir logs</span><br><span class="line">$ nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<p>启动kafka3的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ mkdir logs</span><br><span class="line">$ nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<h3 id="9-当三台服务器都启动好了，我们分别连到三台zookeeper服务器："><a href="#9-当三台服务器都启动好了，我们分别连到三台zookeeper服务器：" class="headerlink" title="9.当三台服务器都启动好了，我们分别连到三台zookeeper服务器："></a>9.当三台服务器都启动好了，我们分别连到三台zookeeper服务器：</h3><p>连接到kafka1的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/zookeeper-shell.sh 127.0.0.1:2181</span><br></pre></td></tr></table></figure></p>
<p>连接到kafka2的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/zookeeper-shell.sh 127.0.0.1:2182</span><br></pre></td></tr></table></figure></p>
<p>连接到kafka3的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/zookeeper-shell.sh 127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<p>可以通过查看<code>logs/zookeeper.log</code>文件，如果没有报错就说明zookeeper集群启动成功。</p>
<p>这样你在<code>kafka1</code>中的<code>zookeeper</code>所作的修改，都会同步到<code>kafka2</code>, <code>kafka3</code>。<br>例如你在kafka1的zookeeper服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ create /zk_test_cluster my_data_cluster</span><br></pre></td></tr></table></figure></p>
<p>你在kafka2, kafka3的zookeeper客户端用<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls /</span><br></pre></td></tr></table></figure></p>
<p>都会看到节点zk_test_cluster</p>
<p>至此，zookeeper集群部署结束。</p>
<h3 id="10-搭建kafka集群"><a href="#10-搭建kafka集群" class="headerlink" title="10.搭建kafka集群"></a>10.搭建kafka集群</h3><p>配置<code>kafka1</code>服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ vi config/server.properties</span><br><span class="line"></span><br><span class="line">broker.id=1                           <span class="comment"># 这里设置为1，另外两台分别设置为2、3</span></span><br><span class="line"></span><br><span class="line">listeners=PLAINTEXT://127.0.0.1:9092  <span class="comment"># IP地址和端口，这里使用默认的 9092，另外两台分别使用9093、9094</span></span><br><span class="line"></span><br><span class="line">log.dirs=/home/hewentian/ProjectD/kafkaCluster/kafka1/data/kafka</span><br><span class="line"></span><br><span class="line">zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<p>配置<code>kafka2</code>服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ vi config/server.properties</span><br><span class="line"></span><br><span class="line">broker.id=2</span><br><span class="line"></span><br><span class="line">listeners=PLAINTEXT://127.0.0.1:9093</span><br><span class="line"></span><br><span class="line">log.dirs=/home/hewentian/ProjectD/kafkaCluster/kafka2/data/kafka</span><br><span class="line"></span><br><span class="line">zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<p>配置<code>kafka3</code>服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ vi config/server.properties</span><br><span class="line"></span><br><span class="line">broker.id=3</span><br><span class="line"></span><br><span class="line">listeners=PLAINTEXT://127.0.0.1:9094</span><br><span class="line"></span><br><span class="line">log.dirs=/home/hewentian/ProjectD/kafkaCluster/kafka3/data/kafka</span><br><span class="line"></span><br><span class="line">zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</span><br></pre></td></tr></table></figure></p>
<h3 id="11-启动三台kafka服务器"><a href="#11-启动三台kafka服务器" class="headerlink" title="11.启动三台kafka服务器"></a>11.启动三台kafka服务器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/kafka-server-start.sh -daemon /home/hewentian/ProjectD/kafkaCluster/kafka1/config/server.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/kafka-server-start.sh -daemon /home/hewentian/ProjectD/kafkaCluster/kafka2/config/server.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-server-start.sh -daemon /home/hewentian/ProjectD/kafkaCluster/kafka3/config/server.properties</span><br></pre></td></tr></table></figure>
<p>分别从三台kafka服务器中查看启动日志<code>logs/server.log</code>，如果没报错，并且看到如下输出，则启动成功：</p>
<pre><code># kafka1 的输出
[2018-10-27 15:48:54,890] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:48:54,890] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:48:54,895] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)

# kafka2 的输出
[2018-10-27 15:49:22,694] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:22,694] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:22,697] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)

# kafka3 的输出
[2018-10-27 15:49:41,746] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:41,746] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-27 15:49:41,749] INFO [KafkaServer id=3] started (kafka.server.KafkaServer)
</code></pre><p>至此，kafka集群搭建成功。下面，我们简单的试用一下。</p>
<h3 id="12-创建topic"><a href="#12-创建topic" class="headerlink" title="12.创建topic"></a>12.创建topic</h3><p>在任意一台kafka服务器上面创建topic，例如在kafka1上面创建一个名为 my-replicated-topic 的 topic，指定 1 个分区，3 个副本：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/kafka-topics.sh --create --zookeeper 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"my-replicated-topic"</span>.</span><br></pre></td></tr></table></figure></p>
<p>上面的参数<code>--zookeeper</code>是集群列表，可以指定所有节点，也可以指定为部分列表。</p>
<p>查看topic的情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka1</span><br><span class="line">$ ./bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</span><br></pre></td></tr></table></figure></p>
<h3 id="13-发送消息"><a href="#13-发送消息" class="headerlink" title="13.发送消息"></a>13.发送消息</h3><p>往我们刚才创建的toipc中发送消息，在任意一台kafka上面都可以的，我们在kafka2上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --topic my-replicated-topic</span><br><span class="line">&gt;</span><br><span class="line">&gt;my <span class="built_in">test</span> message 1</span><br><span class="line">&gt;my <span class="built_in">test</span> message 2</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="14-消费消息"><a href="#14-消费消息" class="headerlink" title="14.消费消息"></a>14.消费消息</h3><p>将我们刚刚发送的消息消费掉，我们从kafka3上面执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --from-beginning --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">my <span class="built_in">test</span> message 1</span><br><span class="line">my <span class="built_in">test</span> message 2</span><br></pre></td></tr></table></figure></p>
<p>我们在生产者中发送消息，在消费者中就能实时的看到消息。</p>
<h3 id="15-容错测试"><a href="#15-容错测试" class="headerlink" title="15.容错测试"></a>15.容错测试</h3><p>从上面可知my-replicated-topic的leader为3，那我们将broker.id=3的进程杀掉：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ps -ef | grep kafka3/config/server.properties</span><br><span class="line">hewenti+ 22018  1897  5 17:19 pts/23   00:00:16 /usr/<span class="built_in">local</span>/java/jdk1.8.0_102/bin/java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[中间省略部分]</span><br><span class="line"></span><br><span class="line">-0.10.jar:/home/hewentian/ProjectD/kafkaCluster/kafka3/bin/../libs/zookeeper-3.4.13.jar kafka.Kafka /home/hewentian/ProjectD/kafkaCluster/kafka3/config/server.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">kill</span> -9 22018       <span class="comment"># 单机环境下不能通过执行： ./bin/kafka-server-stop.sh 来杀掉当前目录下的kafka，它会杀掉全部kafka</span></span><br></pre></td></tr></table></figure></p>
<p>再查看my-replicated-topic的情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 3,2,1	Isr: 1</span><br></pre></td></tr></table></figure></p>
<p>由上面可见，leader已经变为1。并且，生产消息和消费消息一样可用，不受影响：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka2</span><br><span class="line">$ ./bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --topic my-replicated-topic</span><br><span class="line">&gt;</span><br><span class="line">&gt;my <span class="built_in">test</span> message 1</span><br><span class="line">&gt;my <span class="built_in">test</span> message 2</span><br><span class="line">&gt;</span><br><span class="line">&gt; Tim Ho</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafkaCluster/kafka3</span><br><span class="line">$ ./bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 --from-beginning --topic my-replicated-topic</span><br><span class="line"></span><br><span class="line">my <span class="built_in">test</span> message 1</span><br><span class="line">my <span class="built_in">test</span> message 2</span><br><span class="line"></span><br><span class="line">Tim Ho</span><br></pre></td></tr></table></figure>
<p>未完，待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/10/27/kafka-cluster/" data-id="clf9thvlo001h6h3kjh85e451" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-kafka-standalone" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/24/kafka-standalone/" class="article-date">
  <time datetime="2018-10-24T00:32:40.000Z" itemprop="datePublished">2018-10-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/24/kafka-standalone/">kafka 单节点安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文将说下<code>kafka</code>的单节点安装，我的机器为<code>Ubuntu 16.04 LTS</code>，下面的安装过程参考：<br><a href="http://kafka.apache.org/quickstart" target="_blank" rel="noopener">http://kafka.apache.org/quickstart</a></p>
<h3 id="第一步：我们要将kafka安装包下载回来"><a href="#第一步：我们要将kafka安装包下载回来" class="headerlink" title="第一步：我们要将kafka安装包下载回来"></a>第一步：我们要将<code>kafka</code>安装包下载回来</h3><p>截止本文写时，它的最新版本为<code>2.0.0</code>，可以在它的<a href="https://www.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz" target="_blank" rel="noopener">官网</a>下载。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/</span><br><span class="line">$ wget https://www.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz</span><br><span class="line">$ wget https://www.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz.sha512</span><br><span class="line"></span><br><span class="line">验证下载文件的完整性，在下载的时候要将 SHA512 文件也下载回来</span><br><span class="line">$ sha512sum -c kafka_2.12-2.0.0.tgz.sha512</span><br><span class="line">kafka_2.12-2.0.0.tgz: OK</span><br><span class="line"></span><br><span class="line">$ tar xzf kafka_2.12-2.0.0.tgz</span><br></pre></td></tr></table></figure></p>
<h3 id="第二步：启动服务器"><a href="#第二步：启动服务器" class="headerlink" title="第二步：启动服务器"></a>第二步：启动服务器</h3><p>kafka需要用到zookeeper，所以必须首先启动zookeeper。在高版本的kafka发行包中，已经内置zookeeper，我们直接使用即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure></p>
<p>启动成功后，会看到如下输出：</p>
<pre><code>[2018-10-24 09:14:29,072] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:java.compiler=&lt;NA&gt; (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:os.version=4.13.0-32-generic (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,072] INFO Server environment:user.name=hewentian (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,073] INFO Server environment:user.home=/home/hewentian (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,073] INFO Server environment:user.dir=/home/hewentian/ProjectD/kafka_2.12-2.0.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,091] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,091] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,091] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-10-24 09:14:29,111] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2018-10-24 09:14:29,121] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
</code></pre><p>接着，打开另外一个终端，启动kafka服务器：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure></p>
<p>启动成功后，会看到如下输出：</p>
<pre><code>[2018-10-24 11:01:45,462] INFO [SocketServer brokerId=0] Started processors for 1 acceptors (kafka.network.SocketServer)
[2018-10-24 11:01:45,494] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-24 11:01:45,494] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-10-24 11:01:45,497] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
</code></pre><h3 id="第三步：创建topic"><a href="#第三步：创建topic" class="headerlink" title="第三步：创建topic"></a>第三步：创建topic</h3><p>创建一个名字叫<code>test</code>的topic，只有一个分区和一个副本，打开另外一个终端：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span></span><br><span class="line">Created topic <span class="string">"test"</span>.</span><br><span class="line"></span><br><span class="line">查看所有创建的topic</span><br><span class="line">$ ./bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line"><span class="built_in">test</span></span><br></pre></td></tr></table></figure></p>
<h3 id="第四步：往topic发送消息"><a href="#第四步：往topic发送消息" class="headerlink" title="第四步：往topic发送消息"></a>第四步：往topic发送消息</h3><p>kafka自带一个命令行的客户端，用于从文件中或者标准输入中读取消息并且发送到kafka集群，默认每一行会被作为一条消息发送：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic <span class="built_in">test</span></span><br><span class="line">&gt;This is a message</span><br><span class="line">&gt;This is another message</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="第五步：消费topic中的消息"><a href="#第五步：消费topic中的消息" class="headerlink" title="第五步：消费topic中的消息"></a>第五步：消费topic中的消息</h3><p>kafka同样自带一个命令行的消费者，它会将消息输出到标准输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kafka_2.12-2.0.0/</span><br><span class="line">$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class="built_in">test</span> --from-beginning</span><br><span class="line">This is a message</span><br><span class="line">This is another message</span><br></pre></td></tr></table></figure></p>
<p>这样，一个简单的单节点<code>kafka</code>服务器就搭建完成了，接下来我们将尝试搭建多节点的集群。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/10/24/kafka-standalone/" data-id="clf9thvlw001q6h3kxi6p5x5d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-jenkins-note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/05/jenkins-note/" class="article-date">
  <time datetime="2018-10-05T04:02:47.000Z" itemprop="datePublished">2018-10-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/other/">other</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/05/jenkins-note/">jenkins 学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将说说jenkins的使用，通过阅读本POST，你将拥有一台属于自已的jenkins服务器。</p>
<p>首先，我们要将<code>jenkins</code>的安装包下载回来，可以在它的<a href="http://mirrors.jenkins.io/war-stable/latest/" target="_blank" rel="noopener">官网</a>下载最新稳定版：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/</span><br><span class="line">$ wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war</span><br><span class="line">$ wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war.sha256</span><br><span class="line"></span><br><span class="line">验证下载文件的完整性</span><br><span class="line">$ sha256sum -c jenkins.war.sha256 </span><br><span class="line">jenkins.war: OK</span><br></pre></td></tr></table></figure>
<p>我们将它安装在当前目录(<code>/home/hewentian/ProjectD</code>)下，在当前目录下创建一个jenkins目录，用作<code>JENKINS_HOME</code>目录，我们将相关命令放到一个脚本<code>start_jenkins.sh</code>中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD</span><br><span class="line">$ touch start_jenkins.sh</span><br><span class="line">$ vi start_jenkins.sh</span><br></pre></td></tr></table></figure></p>
<p>其中<code>start_jenkins.sh</code>脚本的内容如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line">JENKINS_HOME=/home/hewentian/ProjectD/jenkins</span><br><span class="line">JENKINS_WAR=/home/hewentian/ProjectD/jenkins.war</span><br><span class="line">LOG_ROOT=<span class="variable">$JENKINS_HOME</span>/logs</span><br><span class="line">LOG_FILE=<span class="variable">$LOG_ROOT</span>/jenkins.log</span><br><span class="line">WEB_ROOT=<span class="variable">$JENKINS_HOME</span>/war</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Starting Jenkins ..."</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"JENKINS_HOME: <span class="variable">$JENKINS_HOME</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"JENKINS_WAR: <span class="variable">$JENKINS_WAR</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"LOG_FILE: <span class="variable">$LOG_FILE</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"WEB_ROOT: <span class="variable">$WEB_ROOT</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$JENKINS_HOME</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"creating: <span class="variable">$JENKINS_HOME</span>"</span></span><br><span class="line">    mkdir <span class="variable">$JENKINS_HOME</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$LOG_ROOT</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"creating: <span class="variable">$LOG_ROOT</span>"</span></span><br><span class="line">    mkdir <span class="variable">$LOG_ROOT</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -e <span class="variable">$LOG_FILE</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"creating: <span class="variable">$LOG_FILE</span>"</span></span><br><span class="line">    touch <span class="variable">$LOG_FILE</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$WEB_ROOT</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"creating: <span class="variable">$WEB_ROOT</span>"</span></span><br><span class="line">    mkdir <span class="variable">$WEB_ROOT</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">java -Xms1024m -Xmx1024m -Djava.awt.headless=<span class="literal">true</span> -DJENKINS_HOME=<span class="variable">$JENKINS_HOME</span> -jar <span class="variable">$JENKINS_WAR</span> --logfile=<span class="variable">$LOG_FILE</span> --webroot=<span class="variable">$WEB_ROOT</span> --httpPort=8080 --daemon &gt;&gt; <span class="variable">$LOG_FILE</span></span><br><span class="line"></span><br><span class="line">tail -f <span class="variable">$LOG_FILE</span></span><br></pre></td></tr></table></figure></p>
<p>启动jenkins：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ chmod +x start_jenkins.sh</span><br><span class="line">$ . start_jenkins.sh</span><br></pre></td></tr></table></figure></p>
<p>如果你看到如下输出：</p>
<pre><code>Starting Jenkins ...
JENKINS_HOME: /home/hewentian/ProjectD/jenkins
JENKINS_WAR: /home/hewentian/ProjectD/jenkins.war
LOG_FILE: /home/hewentian/ProjectD/jenkins/logs/jenkins.log
WEB_ROOT: /home/hewentian/ProjectD/jenkins/war
creating: /home/hewentian/ProjectD/jenkins
creating: /home/hewentian/ProjectD/jenkins/logs
creating: /home/hewentian/ProjectD/jenkins/logs/jenkins.log
creating: /home/hewentian/ProjectD/jenkins/war
Forking into background to run as a daemon.
Running from: /home/hewentian/ProjectD/jenkins.war
Oct 06, 2018 10:48:18 AM org.eclipse.jetty.util.log.Log initialized
INFO: Logging initialized @780ms to org.eclipse.jetty.util.log.JavaUtilLog
Oct 06, 2018 10:48:18 AM winstone.Logger logInternal
.
.
. 中间省略部分日志
.
Oct 06, 2018 10:48:29 AM jenkins.install.SetupWizard init
INFO: 

*************************************************************
*************************************************************
*************************************************************

Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:

02b24053bc4844f4a348fdbbbf65c347

This may also be found at: /home/hewentian/ProjectD/jenkins/secrets/initialAdminPassword

*************************************************************
*************************************************************
*************************************************************

Oct 06, 2018 10:48:36 AM hudson.model.UpdateSite updateData
INFO: Obtained the latest update center data file for UpdateSource default
Oct 06, 2018 10:48:37 AM hudson.model.UpdateSite updateData
INFO: Obtained the latest update center data file for UpdateSource default
Oct 06, 2018 10:48:37 AM jenkins.InitReactorRunner$1 onAttained
INFO: Completed initialization
Oct 06, 2018 10:48:37 AM hudson.WebAppMain$3 run
INFO: Jenkins is fully up and running
Oct 06, 2018 10:48:37 AM hudson.model.DownloadService$Downloadable load
INFO: Obtained the updated data file for hudson.tasks.Maven.MavenInstaller
Oct 06, 2018 10:48:37 AM hudson.model.AsyncPeriodicWork$1 run
INFO: Finished Download metadata. 10,126 ms
</code></pre><p>则证明启动成功，我们按上面的提示打开浏览器，输入：<br><a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a></p>
<p>你将会见到如下界面：<br><img src="/img/jenkins-1.png" alt="" title="jenkins初次启动界面"></p>
<p>将上面日志中的密码输入到上述界面，并点击<code>[Continue]</code>按钮，将出现下图界面：<br><img src="/img/jenkins-2.png" alt="" title="jenkins安装插件界面"></p>
<p>为简单起见，选择<code>Install suggested plugins</code>安装即可，安装进度如下：<br><img src="/img/jenkins-3.png" alt="" title="jenkins安装插件界面"></p>
<p>接下来是设置admin用户和密码：<br>Username: hewentian<br>Password: abc123</p>
<p><img src="/img/jenkins-4.png" alt="" title="jenkins设置admin用户"></p>
<p>点击<code>[Save and Continue]</code>，并在接下来的界面点击<code>[Save and Finish]</code>完成设置。<br><img src="/img/jenkins-5.png" alt="" title="jenkins最终界面"></p>
<h3 id="下面进行简单的配置"><a href="#下面进行简单的配置" class="headerlink" title="下面进行简单的配置"></a>下面进行简单的配置</h3><p>按下图所示设置JDK、Maven：<code>[Manage Jenkins]-&gt;[Global Tool Configuration]</code>：<br><img src="/img/jenkins-6.png" alt="" title="设置"><br><img src="/img/jenkins-7.png" alt="" title="设置"></p>
<h3 id="下面安装插件"><a href="#下面安装插件" class="headerlink" title="下面安装插件"></a>下面安装插件</h3><p><code>[Manage Jenkins]-&gt;[Manage Plugins]</code><br>安装<code>Maven Integration</code>插件，如下图，直接点击<code>Install without restart</code>，该插件是用于建立maven job<br><img src="/img/jenkins-8.png" alt="" title="安装maven插件"></p>
<p>安装<code>Deploy to container</code>插件，用于将构建好的应用部署到容器中：<br><img src="/img/jenkins-9.png" alt="" title="安装Deploy to container插件"></p>
<h3 id="下面演示构建项目"><a href="#下面演示构建项目" class="headerlink" title="下面演示构建项目"></a>下面演示构建项目</h3><h4 id="示例A、构建一个从gitHub中拉取原码的maven项目"><a href="#示例A、构建一个从gitHub中拉取原码的maven项目" class="headerlink" title="示例A、构建一个从gitHub中拉取原码的maven项目"></a>示例A、构建一个从gitHub中拉取原码的maven项目</h4><p><code>[New Item]</code>-&gt;选择<code>[Maven project]</code>，并在<code>[Enter an item name]</code>中输入mvn-test，然后点击<code>[ok]</code>，如下图：<br><img src="/img/jenkins-10.png" alt="" title="构建一个从gitHub中拉取原码的maven项目"></p>
<p>在弹出的界面中选中<code>[Discard old builds]</code>并将<code>Max of builds to keep</code>设为10，然后设置源码仓库，如下所示：<br><img src="/img/jenkins-11.png" alt=""></p>
<p><strong> 注意： </strong> 如果我们的仓库中包含有多个项目，而我们此处要构建的只是其中一个，则我们需要指定构建哪一个：<code>Additional Behaviours -&gt; Add -&gt; Sparse Checkout paths</code>，在<code>Path</code>处填入: <code>/{repository_name}/{need_to_build_project}/**</code></p>
<p>例如：<br>Repository URL: <code>https://github.com/jenkins-docs/simple-java-maven-app.git</code><br>Path: <code>/simple-java-maven-app/my-app/**</code><br>如果是这种方式，则下面的Root POM也要修改成对应的项目:<br>Root POM: <code>simple-java-maven-app/my-app/pom.xml</code><br>上面的Path开头是有<code>/</code>的，而Root POM开头是没有<code>/</code>的。</p>
<p>设置Build的<code>Goals and options</code>为<code>clean install</code>，如下：<br><img src="/img/jenkins-12.png" alt=""></p>
<p>其他设置保持默认，点击<code>[Save]</code>，在弹出的界面点击<code>[Build Now]</code>，然后再点击下方构建历史中正在构建的任务的<code>[Console Output]</code>。<br><img src="/img/jenkins-13.png" alt=""></p>
<p><img src="/img/jenkins-14.png" alt=""><br><img src="/img/jenkins-15.png" alt=""></p>
<p>如上图所示，构建成功了。切换到上图中的目录中查看目标文件，并运行它：<br><img src="/img/jenkins-16.png" alt=""></p>
<p>这样，一个简单的maven项目就构建完成了。</p>
<h4 id="示例B、构建一个从gitHub中拉取原码的maven-web项目，并部署到运行中的tomcat"><a href="#示例B、构建一个从gitHub中拉取原码的maven-web项目，并部署到运行中的tomcat" class="headerlink" title="示例B、构建一个从gitHub中拉取原码的maven web项目，并部署到运行中的tomcat"></a>示例B、构建一个从gitHub中拉取原码的maven web项目，并部署到运行中的tomcat</h4><p>首先，我们创建一个最简单的maven web项目，并推到：<br><a href="https://github.com/hewentian/web-test">https://github.com/hewentian/web-test</a><br>web-test项目只有三个文件：</p>
<pre><code>pom.xml
src/main/webapp/WEB-INF/web.xml
src/main/webapp/index.jsp
</code></pre><p><code>pom.xml</code>文件内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">	<span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hewentian<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>web-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">packaging</span>&gt;</span>war<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>web-test Maven Webapp<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.apache.org<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">finalName</span>&gt;</span>web-test<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><code>src/main/webapp/WEB-INF/web.xml</code>文件内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE web-app PUBLIC</span></span><br><span class="line"><span class="meta"> "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"</span></span><br><span class="line"><span class="meta"> "http://java.sun.com/dtd/web-app_2_3.dtd" &gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">web-app</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">display-name</span>&gt;</span>Archetype Created Web Application<span class="tag">&lt;/<span class="name">display-name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">web-app</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><code>src/main/webapp/index.jsp</code>文件内容如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">	&lt;body&gt;</span><br><span class="line">		&lt;h2&gt;Hello World!&lt;/h2&gt;</span><br><span class="line">	&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>
<p>接着，我们准备一台tomcat，我已经准备好了一台，位于：</p>
<pre><code>/home/hewentian/ProjectD/apache-tomcat-8.0.47
</code></pre><p>因为jenkins使用了<code>8080</code>端口，所以tomcat不能使用默认的<code>8080</code>端口，我们将其修改为<code>8867</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/apache-tomcat-8.0.47/conf</span><br><span class="line">$ vi server.xml</span><br><span class="line"></span><br><span class="line">只修改此处即可</span><br><span class="line">&lt;Connector port=<span class="string">"8867"</span> protocol=<span class="string">"HTTP/1.1"</span> connectionTimeout=<span class="string">"20000"</span> redirectPort=<span class="string">"8443"</span> /&gt;</span><br></pre></td></tr></table></figure></p>
<p>配置tomcat的管理员帐号：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/apache-tomcat-8.0.47/conf</span><br><span class="line">$ vi tomcat-users.xml</span><br><span class="line"></span><br><span class="line">在&lt;tomcat-users&gt;节点里添加如下内容：</span><br><span class="line"></span><br><span class="line">&lt;role rolename=<span class="string">"manager-gui"</span>/&gt;</span><br><span class="line">&lt;role rolename=<span class="string">"manager-script"</span>/&gt;</span><br><span class="line">&lt;role rolename=<span class="string">"manager-jmx"</span>/&gt;</span><br><span class="line">&lt;role rolename=<span class="string">"manager-status"</span>/&gt;</span><br><span class="line"></span><br><span class="line">&lt;user username=<span class="string">"hwt"</span> password=<span class="string">"pwd123"</span> roles=<span class="string">"manager-gui,manager-script,manager-jmx,manager-status"</span>/&gt;</span><br></pre></td></tr></table></figure></p>
<p>其中的<code>username=&quot;hwt&quot; password=&quot;pwd123&quot;</code>是用于登录Tomcat用的，下面会用到，重启tomcat。</p>
<p>回到jenkins，我们新建一个Item，命名为web-app-test：<br><img src="/img/jenkins-17.png" alt=""><br><img src="/img/jenkins-18.png" alt=""></p>
<p>配置代码仓库，如下图。点击<code>Credentials</code>右边的<code>Add-&gt;jenkins</code><br><img src="/img/jenkins-19.png" alt=""></p>
<p>在弹出的对话框中，选择<code>SSH Username with private key</code>，将<code>~/.ssh/id_rsa</code>文件的内容复制到Key中，点<code>Add</code>：<br><img src="/img/jenkins-20.png" alt=""></p>
<p>在配置代码仓库中，选择刚才创建的<code>Credentials</code>：<br><img src="/img/jenkins-21.png" alt=""></p>
<p>配置构建触发器：<br><img src="/img/jenkins-22.png" alt=""></p>
<p>说明：</p>
<ol>
<li>Build whenever a SNAPSHOT dependency is built：在构建的时候，会根据pom.xml文件的继承关系构建发生一个构建引起其他构建的；</li>
<li>Poll SCM：这是CI系统中常见的选项。当您选择此选项，您可以指定一个定时作业表达式来定义Jenkins每隔多久检查一下您源代码仓库的变化。如果发现变化，就执行一次构建。例如，表达式中填写0,15,30,45 <em> </em> <em> </em>将使Jenkins每隔15分钟就检查一次您源码仓库的变化；</li>
<li>Build periodically：此选项仅仅通知Jenkins按指定的频率对项目进行构建，而不管SCM是否有变化。如果想在这个Job中运行一些测试用例的话，它就很有帮助。</li>
</ol>
<p>配置构建设置：<br><img src="/img/jenkins-23.png" alt=""></p>
<p>接着我们试着点击<code>Build Now</code>试下能否成功构建：<br><img src="/img/jenkins-24.png" alt=""></p>
<p>当你看到如下输出时，证明构建成功：<br><img src="/img/jenkins-25.png" alt=""></p>
<p>接着我们配置部署到tomcat，回到web-app-test的jenkins配置，在<code>Add post-build action</code>中选择<code>Deploy war/ear to a container</code>，如下图：<br><img src="/img/jenkins-26.png" alt=""></p>
<p>在<code>Credentials</code>右则点击<code>Add-&gt;Jenkins</code>，并在弹出的对话框中输入上面在tomcat中配置的用户名：<br><img src="/img/jenkins-27.png" alt=""></p>
<p>说明：</p>
<ol>
<li>首先tomcat是启动的，并且Tomcat中没有部署web-test.war；</li>
<li>WAR/EAR files：war文件的存放位置，如：target/web-test.war 注意：相对路径，target前是没有/的；</li>
<li>Context path：访问时需要输入的内容，如wt访问时如下：<a href="http://127.0.0.1:8867/wt/" target="_blank" rel="noopener">http://127.0.0.1:8867/wt/</a>，如果为空，默认是war包的名字；</li>
<li>Container：选择你的web容器，如tomca 8.x；</li>
<li>Credentials: 在右边的下拉页面中选择访问Tomcat的用户名、密码，如果没有，则点【Add】；</li>
<li>Tomcat URL：填入你Tomcat的访问地址，如：<a href="http://127.0.0.1:8867/；" target="_blank" rel="noopener">http://127.0.0.1:8867/；</a></li>
<li>svn、git、tomcat的用户名和密码设置了是没有办法在web界面修改的。如果要修改则先去Jenkins目录删除hudson.scm.SubversionSCM.xml文件，或者在jenkins用户页中删掉该用户，虽然jenkins页面提供修改方法，但是，无效。</li>
</ol>
<p>接着我们点击<code>Build Now</code>开始构建：<br><img src="/img/jenkins-28.png" alt=""></p>
<p>如果你看到上面输出，则证明构建和部署成功，可以打开浏览器查看：<br><img src="/img/jenkins-29.png" alt=""></p>
<p>到此，大功告成。</p>
<h4 id="示例C、将示例A产生的JAR包部署到远程机器上面运行"><a href="#示例C、将示例A产生的JAR包部署到远程机器上面运行" class="headerlink" title="示例C、将示例A产生的JAR包部署到远程机器上面运行"></a>示例C、将示例A产生的JAR包部署到远程机器上面运行</h4><p>我们回到示例A的jenkins配置，在Post Steps下选择<code>Run only if build succeeds</code>，点<code>Add post-build step</code>并选择<code>Execute shell</code>，在Command中填入如下脚本，此脚本由我同事<code>严忠思</code>编写，我稍作修改：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.定义变量</span></span><br><span class="line"><span class="comment"># SSH 端口</span></span><br><span class="line"><span class="built_in">export</span> SSH_PORT=<span class="string">"12022"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行 jar 包的机器,多个IP以空格分隔，如: 192.168.30.241 192.168.30.242</span></span><br><span class="line"><span class="built_in">export</span> SSH_IP_LIST=<span class="string">"192.168.30.241"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行 jar 的用户</span></span><br><span class="line"><span class="built_in">export</span> USERNAME=<span class="string">"root"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境 dev,test,gray,prod</span></span><br><span class="line"><span class="built_in">export</span> RUN_SERVER=<span class="string">"dev"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 远程存放 jar 包文件路径,注这个路径要先手动创建, mkdir -p /www/web/my-app &amp;&amp; chown root.root /www/web/my-app</span></span><br><span class="line"><span class="built_in">export</span> REMOTE_JAR_DIR=<span class="string">"/www/web/my-app/<span class="variable">$&#123;RUN_SERVER&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Jenkins (-DJENKINS_HOME)用 maven 编译打包程序的路径与文件</span></span><br><span class="line"><span class="built_in">export</span> JENKINS_JAR_FILE=<span class="string">"/home/hewentian/ProjectD/jenkins/workspace/mvn-test/target/my-app-1.0-SNAPSHOT.jar"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#jar 打包文件名</span></span><br><span class="line"><span class="built_in">export</span> JAR_FILE=<span class="string">"my-app-1.0-SNAPSHOT.jar"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行 JAR 的端口，我这里并不使用这个端口号，故可不填</span></span><br><span class="line"><span class="built_in">export</span> JAR_PORT=<span class="string">"8802"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志路径</span></span><br><span class="line"><span class="built_in">export</span> LOG_PATH=<span class="string">"/www/logs/my-app"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#jvm参数</span></span><br><span class="line"><span class="built_in">export</span> JAR_JAVA_OPTS=<span class="string">"-XX:-UseGCOverheadLimit -Xmx1024m"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># jar 运行命令</span></span><br><span class="line"><span class="built_in">export</span> JAR_COMMOND=<span class="string">"nohup java <span class="variable">$&#123;JAR_JAVA_OPTS&#125;</span> \</span></span><br><span class="line"><span class="string">-jar <span class="variable">$&#123;REMOTE_JAR_DIR&#125;</span>/<span class="variable">$&#123;JAR_FILE&#125;</span> \</span></span><br><span class="line"><span class="string">--spring.cloud.config.profile=<span class="variable">$&#123;RUN_SERVER&#125;</span> \</span></span><br><span class="line"><span class="string">--server.port=<span class="variable">$&#123;JAR_PORT&#125;</span> \</span></span><br><span class="line"><span class="string">--logging.path=<span class="variable">$&#123;LOG_PATH&#125;</span> \</span></span><br><span class="line"><span class="string">&gt; <span class="variable">$&#123;LOG_PATH&#125;</span>/my-app.log &amp;"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待时间，如果不配置，则脚本默认为 40 秒</span></span><br><span class="line"><span class="built_in">export</span> SLEEP_SEC=20</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.主程序</span></span><br><span class="line">/bin/bash -x /home/hewentian/ProjectD/jenkins/script/jar.sh</span><br></pre></td></tr></table></figure></p>
<p>在我的本机执行如下命令，创建存放脚本的目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/jenkins</span><br><span class="line">$ mkdir script</span><br><span class="line">$ <span class="built_in">cd</span> script</span><br><span class="line">$ touch jar.sh</span><br><span class="line">$ chmod +x jar.sh</span><br></pre></td></tr></table></figure></p>
<p>在jar.sh中输入如下脚本，此脚本同样由我的同事<code>严忠思</code>编写：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/env sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"-------------------- start print env var --------------------"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"SSH_PORT: <span class="variable">$SSH_PORT</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"SSH_IP_LIST: <span class="variable">$SSH_IP_LIST</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"USERNAME: <span class="variable">$USERNAME</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"RUN_SERVER: <span class="variable">$RUN_SERVER</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"REMOTE_JAR_DIR: <span class="variable">$REMOTE_JAR_DIR</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"JENKINS_JAR_FILE: <span class="variable">$JENKINS_JAR_FILE</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"JAR_COMMOND: <span class="variable">$JAR_COMMOND</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"-------------------- end print env var --------------------"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### IP 数量</span></span><br><span class="line">IP_LIST=1</span><br><span class="line">HOST_COUNT=$(<span class="built_in">echo</span> <span class="variable">$&#123;SSH_IP_LIST&#125;</span> | wc -w)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 默认定义时间为40秒</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;SLEEP_SEC&#125;</span>"</span> == <span class="string">""</span> ];<span class="keyword">then</span></span><br><span class="line">    SLEEP_SEC=40</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### 检查是否添加公钥</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">SSH_CHECK</span></span>()&#123;</span><br><span class="line">    ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"uname -n"</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"$?"</span> -ne 0 ];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> -e <span class="string">"Jenkins 登录失败, <span class="variable">$&#123;SSH_HOST&#125;</span> 没有添加 SSH 公钥，请把 Jenkins 公钥添加到 <span class="variable">$&#123;SSH_HOST&#125;</span> \n"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"或检查 <span class="variable">$&#123;SSH_HOST&#125;</span>  ~/.ssh 目录与 ~/.ssh/authorized_keys 文件权限(chmod 700 ~/.ssh &amp;&amp; chmod 600 ~/.ssh/authorized_keys)"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 构建目录，如果失败可以验证客户端没权限</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">RUN_DIR</span></span>()&#123;  </span><br><span class="line">    ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"uname -n;/bin/mkdir -p <span class="variable">$&#123;REMOTE_JAR_DIR&#125;</span>"</span></span><br><span class="line">    RUN_ID=`<span class="built_in">echo</span> $?`</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;RUN_ID&#125;</span>"</span> -ne 0 ];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;SSH_HOST&#125;</span> 的 <span class="variable">$&#123;USERNAME&#125;</span> 用户创建目录失败，请检查 <span class="variable">$&#123;USERNAME&#125;</span> 用户是否有权限"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 存放日志目录</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">LOG_PATH_DIR</span></span>()&#123;  </span><br><span class="line">    ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"uname -n;/bin/mkdir -p <span class="variable">$&#123;LOG_PATH&#125;</span>"</span></span><br><span class="line">    RUN_ID=`<span class="built_in">echo</span> $?`</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;RUN_ID&#125;</span>"</span> -ne 0 ];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;SSH_HOST&#125;</span> 的 <span class="variable">$&#123;USERNAME&#125;</span> 用户创建目录失败，请检查 <span class="variable">$&#123;USERNAME&#125;</span> 用户是否有权限"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">RSYNC_JAR</span></span>()&#123;  </span><br><span class="line">    rsync -azP --delete -e <span class="string">"ssh -p <span class="variable">$SSH_PORT</span> -o 'StrictHostKeyChecking=no'"</span> <span class="variable">$&#123;JENKINS_JAR_FILE&#125;</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span>:<span class="variable">$&#123;REMOTE_JAR_DIR&#125;</span> &gt; /dev/null</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">JAR_PID</span></span>()&#123;  </span><br><span class="line">    PID=$(ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"/usr/sbin/lsof -i:<span class="variable">$&#123;JAR_PORT&#125;</span> | grep -vi PID | awk '&#123;print \$2&#125;'"</span>)</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$PID</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">STOP_JAR</span></span>()&#123;  </span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$&#123;PID&#125;</span>"</span> ] || [ -n <span class="string">"<span class="variable">$&#123;PID2&#125;</span>"</span> ];<span class="keyword">then</span>    </span><br><span class="line">        ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"kill -9 <span class="variable">$PID</span> &gt; /dev/null 2&gt;&amp;1"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"INFO: <span class="variable">$&#123;JAR_FILE&#125;</span> 进程已杀"</span></span><br><span class="line">    <span class="keyword">else</span>    </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"INFO: <span class="variable">$&#123;JAR_FILE&#125;</span> is Down"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    PID=<span class="string">""</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">START_JAR</span></span>()&#123;  </span><br><span class="line">    ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"source /etc/profile &gt; /dev/null; cd <span class="variable">$&#123;REMOTE_JAR_DIR&#125;</span>; <span class="variable">$&#123;JAR_COMMOND&#125;</span> "</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">CHECK_JAR</span></span>()&#123; </span><br><span class="line">    PID=$(ssh -p <span class="variable">$&#123;SSH_PORT&#125;</span> -o <span class="string">"StrictHostKeyChecking=no"</span> <span class="variable">$&#123;USERNAME&#125;</span>@<span class="variable">$&#123;SSH_HOST&#125;</span> <span class="string">"/usr/sbin/lsof -i:<span class="variable">$&#123;JAR_PORT&#125;</span> | grep -vi PID | awk '&#123;print \$2&#125;'"</span>)</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$PID</span>"</span> != <span class="string">""</span> ];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;SSH_HOST&#125;</span> 的 <span class="variable">$&#123;JAR_FILE&#125;</span> 启动成功"</span> </span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;SSH_HOST&#125;</span> 的 <span class="variable">$&#123;JAR_FILE&#125;</span> 启动失败,请运维登录服务器查看进程或相关启动日志"</span> </span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> SSH_HOST <span class="keyword">in</span> <span class="variable">$SSH_IP_LIST</span></span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line">    SSH_CHECK </span><br><span class="line">    RUN_DIR    </span><br><span class="line">    LOG_PATH_DIR</span><br><span class="line">    RSYNC_JAR </span><br><span class="line">    JAR_PID </span><br><span class="line">    <span class="comment">#STOP_JAR</span></span><br><span class="line">    START_JAR</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;IP_LIST&#125;</span>"</span> -le <span class="string">"<span class="variable">$&#123;HOST_COUNT&#125;</span>"</span> ];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"正在检测 <span class="variable">$&#123;SSH_HOST&#125;</span> 的 <span class="variable">$&#123;JAR_FILE&#125;</span> 程序是否成功启动，请等待 <span class="variable">$&#123;SLEEP_SEC&#125;</span> 秒!"</span></span><br><span class="line">        sleep <span class="variable">$&#123;SLEEP_SEC&#125;</span></span><br><span class="line">        <span class="comment">#CHECK_JAR</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;IP_LIST&#125;</span>"</span> -gt <span class="string">"<span class="variable">$&#123;HOST_COUNT&#125;</span>"</span> ];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"INFO: &lt;<span class="variable">$&#123;IP_LIST&#125;</span>&gt; 更新下一台机..."</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">let</span> IP_LIST=IP_LIST+1</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>回到jenkins去点击<code>[Build Now]</code>，在<code>[Console Output]</code>观看它的构建情况。等构建成功后，我们登录<code>192.168.30.241</code>查看情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/</span><br><span class="line">$ ssh -p 12022 root@192.168.30.241</span><br><span class="line"></span><br><span class="line">Last login: Thu Oct 11 11:18:50 2018 from 10.1.23.231</span><br><span class="line">[root@192.168.30.241 ~]<span class="comment"># ls /www/</span></span><br><span class="line">logs  web</span><br><span class="line">[root@192.168.30.241 ~]<span class="comment"># ls /www/web/my-app/dev/</span></span><br><span class="line">my-app-1.0-SNAPSHOT.jar</span><br><span class="line">[root@192.168.30.241 ~]<span class="comment"># more /www/logs/my-app/my-app.log </span></span><br><span class="line">Hello World!</span><br></pre></td></tr></table></figure></p>
<p>从上述输出可知，我们的构建已经成功！！！</p>
<p><strong>将脚本存放在<code>jar.sh</code>中的好处是此脚本可以供多个项目共同使用，只要在<code>Execute shell</code>中根据不同项目定义不同的变量值即可。</strong></p>
<h4 id="示例D、构建指定的git分支"><a href="#示例D、构建指定的git分支" class="headerlink" title="示例D、构建指定的git分支"></a>示例D、构建指定的git分支</h4><p>要实现这个功能，我们要在jenkins安装一个插件<code>Git Parameter</code>：<br><img src="/img/jenkins-30.png" alt=""></p>
<p>我们还是以示例A的为例，去到它的配置中，选中<code>This project is parameterized</code>，点<code>Add parameter</code>-&gt;<code>Git Parameter</code>，设置如下：<br><img src="/img/jenkins-31.png" alt=""></p>
<p>并在<code>Branches to build</code>按下图所示填：<br><img src="/img/jenkins-32.png" alt=""></p>
<p>回到mvn-test这个job，你会发现原先的<code>Build Now</code>已经变成了<code>Build with Parameters</code>，我们点它：<br><img src="/img/jenkins-33.png" alt=""></p>
<p>至此，就可以构建我们想构建的分支了。</p>
<h4 id="示例E、当构建出错的时候，如何回滚-rollback-到上一个版本"><a href="#示例E、当构建出错的时候，如何回滚-rollback-到上一个版本" class="headerlink" title="示例E、当构建出错的时候，如何回滚(rollback)到上一个版本"></a>示例E、当构建出错的时候，如何回滚(rollback)到上一个版本</h4><p>要实现这个功能，我们要在jenkins安装一个插件<code>Copy Artifact</code>：<br><img src="/img/jenkins-34.png" alt=""></p>
<p>未完待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/10/05/jenkins-note/" data-id="clf9thvlp001k6h3kv9fnux99" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/jenkins/">jenkins</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-elk-note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/02/elk-note/" class="article-date">
  <time datetime="2018-10-02T03:09:56.000Z" itemprop="datePublished">2018-10-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata/">bigdata</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/02/elk-note/">ELK 日志系统的搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇将介绍 ELK 日志系统的搭建，我们将在一台机器上面搭建，系统配置如下：<br><img src="/img/system-property.png" alt="" title="系统配置"></p>
<p><code>logstash</code>的整体结构图如下：<br><img src="/img/elk-structure.png" alt="" title="来源：https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png"></p>
<p>我们将使用<code>redis</code>作为上图中的<code>INPUTS</code>，而<code>elasticsearch</code>作为上图中的<code>OUTPUTS</code>，这也是<code>logstash</code>官方的推荐。而它们的安装可以参考以下例子：<br><code>redis</code>的安装请参考：<a href="../../../../2018/08/07/redis-standalone/">redis 的安装使用</a><br><code>elasticsearch</code>的安装请参考：<a href="../../../../2018/09/16/elasticsearch-standalone/">elasticsearch 单节点安装</a></p>
<p><strong> 注意：elasticsearch、logstash、kibana它们的版本最好保持一致，这里都是使用6.4.0版本。 </strong></p>
<h3 id="kibana的安装将在本篇的稍后介绍，下面先介绍下logstash的安装"><a href="#kibana的安装将在本篇的稍后介绍，下面先介绍下logstash的安装" class="headerlink" title="kibana的安装将在本篇的稍后介绍，下面先介绍下logstash的安装"></a><code>kibana</code>的安装将在本篇的稍后介绍，下面先介绍下<code>logstash</code>的安装</h3><p>首先，我们要将<code>logstash</code>安装包下载回来，可以在它的<a href="https://artifacts.elastic.co/downloads/logstash/logstash-6.4.0.tar.gz" target="_blank" rel="noopener">官网</a>下载，当然，我们也可以从这里下载 <a href="https://pan.baidu.com/s/10p4YqzwSk1ixLqvSuv2sAA" title="百度网盘" target="_blank" rel="noopener">logstash-6.4.0.tar.gz</a>，推荐从<code>logstash</code>官网下载对应版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/</span><br><span class="line">$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.0.tar.gz</span><br><span class="line">$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.0.tar.gz.sha512</span><br><span class="line"></span><br><span class="line">验证下载文件的完整性，在下载的时候要将 SHA512 文件也下载回来</span><br><span class="line">$ sha512sum -c logstash-6.4.0.tar.gz.sha512 </span><br><span class="line">logstash-6.4.0.tar.gz: OK</span><br><span class="line"></span><br><span class="line">$ tar xzf logstash-6.4.0.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压后，得到目录<code>logstash-6.4.0</code>，可以查看下它包含有哪些文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/logstash-6.4.0</span><br><span class="line">$ ls</span><br><span class="line"></span><br><span class="line">bin           data          lib          logstash-core             NOTICE.TXT  x-pack</span><br><span class="line">config        Gemfile       LICENSE.txt  logstash-core-plugin-api  tools</span><br><span class="line">CONTRIBUTORS  Gemfile.lock  logs         modules                   vendor</span><br></pre></td></tr></table></figure></p>
<h4 id="测试安装是否成功：以标准输入、标准输出作为input-output"><a href="#测试安装是否成功：以标准输入、标准输出作为input-output" class="headerlink" title="测试安装是否成功：以标准输入、标准输出作为input, output"></a>测试安装是否成功：以标准输入、标准输出作为input, output</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/logstash-6.4.0/bin</span><br><span class="line">$ ./logstash -e <span class="string">'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123; &#125; &#125;'</span></span><br><span class="line"></span><br><span class="line">Sending Logstash logs to /home/hewentian/ProjectD/logstash-6.4.0/logs <span class="built_in">which</span> is now configured via log4j2.properties</span><br><span class="line">[2018-10-02T14:25:37,017][WARN ][logstash.config.source.multilocal] Ignoring the <span class="string">'pipelines.yml'</span> file because modules or <span class="built_in">command</span> line options are specified</span><br><span class="line">[2018-10-02T14:25:38,201][INFO ][logstash.runner          ] Starting Logstash &#123;<span class="string">"logstash.version"</span>=&gt;<span class="string">"6.4.0"</span>&#125;</span><br><span class="line">[2018-10-02T14:25:41,748][INFO ][logstash.pipeline        ] Starting pipeline &#123;:pipeline_id=&gt;<span class="string">"main"</span>, <span class="string">"pipeline.workers"</span>=&gt;4, <span class="string">"pipeline.batch.size"</span>=&gt;125, <span class="string">"pipeline.batch.delay"</span>=&gt;50&#125;</span><br><span class="line">[2018-10-02T14:25:41,919][INFO ][logstash.pipeline        ] Pipeline started successfully &#123;:pipeline_id=&gt;<span class="string">"main"</span>, :thread=&gt;<span class="string">"#&lt;Thread:0x4c1685e4 run&gt;"</span>&#125;</span><br><span class="line">The stdin plugin is now waiting <span class="keyword">for</span> input:</span><br><span class="line">[2018-10-02T14:25:41,990][INFO ][logstash.agent           ] Pipelines running &#123;:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]&#125;</span><br><span class="line">[2018-10-02T14:25:42,396][INFO ][logstash.agent           ] Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#此时窗口在等待输入</span></span><br><span class="line"></span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面是logstash的输出结果</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">      <span class="string">"@version"</span> =&gt; <span class="string">"1"</span>,</span><br><span class="line">    <span class="string">"@timestamp"</span> =&gt; 2018-10-02T06:25:59.608Z,</span><br><span class="line">       <span class="string">"message"</span> =&gt; <span class="string">"hello world"</span>,</span><br><span class="line">          <span class="string">"host"</span> =&gt; <span class="string">"hewentian-Lenovo-IdeaPad-Y470"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的测试结果可知，软件安装正确，下面开始我们的定制配置。</p>
<p>配置文件放在config目录下，此目录下已经有一个示例配置，因为我们要将redis作为我们的INPUTS，所以我们要建立它的配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/logstash-6.4.0/config</span><br><span class="line">$ cp logstash-sample.conf logstash-redis.conf</span><br><span class="line">$ </span><br><span class="line">$ vi logstash-redis.conf</span><br></pre></td></tr></table></figure></p>
<p>在<code>logstash-redis.conf</code>中配置如下，这里暂未配置FILTERS（后面会讲到如何配置）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample Logstash configuration for creating a simple</span></span><br><span class="line"><span class="comment"># Redis -&gt; Logstash -&gt; Elasticsearch pipeline.</span></span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">  redis &#123;</span><br><span class="line">    <span class="built_in">type</span> =&gt; <span class="string">"systemlog"</span></span><br><span class="line">    host =&gt; <span class="string">"127.0.0.1"</span></span><br><span class="line">    port =&gt; 6379</span><br><span class="line">    password =&gt; <span class="string">"abc123"</span></span><br><span class="line">    db =&gt; 0</span><br><span class="line">    data_type =&gt; <span class="string">"list"</span></span><br><span class="line">    key =&gt; <span class="string">"systemlog"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">  <span class="keyword">if</span> [<span class="built_in">type</span>] == <span class="string">"systemlog"</span> &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">      hosts =&gt; [<span class="string">"http://127.0.0.1:9200"</span>]</span><br><span class="line">      index =&gt; <span class="string">"redis-systemlog-%&#123;+YYYY.MM.dd&#125;"</span></span><br><span class="line">      <span class="comment">#user =&gt; "elastic"</span></span><br><span class="line">      <span class="comment">#password =&gt; "changeme"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在启动<code>logstash</code>前，验证一下配置文件是否正确，这是一个好习惯：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/logstash-6.4.0/bin</span><br><span class="line">$ ./logstash -f ../config/logstash-redis.conf -t</span><br></pre></td></tr></table></figure></p>
<p>如果你见到如下输出，则配置正确：</p>
<pre><code>Sending Logstash logs to /home/hewentian/ProjectD/logstash-6.4.0/logs which is now configured via log4j2.properties
[2018-09-30T16:32:45,043][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=&gt;&quot;path.queue&quot;, :path=&gt;&quot;/home/hewentian/ProjectD/logstash-6.4.0/data/queue&quot;}
[2018-09-30T16:32:45,064][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=&gt;&quot;path.dead_letter_queue&quot;, :path=&gt;&quot;/home/hewentian/ProjectD/logstash-6.4.0/data/dead_letter_queue&quot;}
[2018-09-30T16:32:46,030][WARN ][logstash.config.source.multilocal] Ignoring the &apos;pipelines.yml&apos; file because modules or command line options are specified
Configuration OK
[2018-09-30T16:32:50,630][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash
</code></pre><p>接下来，就可以启动logstash了：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/logstash-6.4.0/bin</span><br><span class="line">$ ./logstash -f ../config/logstash-redis.conf</span><br></pre></td></tr></table></figure></p>
<p>如果见到如下输出，则启动成功：</p>
<pre><code>[2018-09-30T16:34:44,175][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}
</code></pre><h4 id="下面进行简单的测试"><a href="#下面进行简单的测试" class="headerlink" title="下面进行简单的测试"></a>下面进行简单的测试</h4><p>我们首先，往redis中推入3条记录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/redis-4.0.11_master/src</span><br><span class="line">$ ./redis-cli -h 127.0.0.1</span><br><span class="line">127.0.0.1:6379&gt; AUTH abc123</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; lpush systemlog hello world</span><br><span class="line">(<span class="built_in">integer</span>) 2</span><br><span class="line">127.0.0.1:6379&gt; lpush systemlog <span class="string">'&#123;"name":"Tim Ho","age":23,"student":true&#125;'</span></span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br></pre></td></tr></table></figure></p>
<p>启动elastchsearch-head可以看到数据已经进入到es中了：<br><img src="/img/elk-head-1.png" alt="" title="elk-head-1"></p>
<p>你会发现上面推到<code>systemlog</code>中的信息如果是JSON格式，则在elasticsearch中会自动解析到相应的field中，否则会放到默认的field：<code>message</code>中。</p>
<h3 id="kibana的安装"><a href="#kibana的安装" class="headerlink" title="kibana的安装"></a>kibana的安装</h3><p><code>kibana</code>的安装很简单，将<code>kibana</code>安装包下载回来，可以在它的<a href="https://artifacts.elastic.co/downloads/kibana/kibana-6.4.0-linux-x86_64.tar.gz" target="_blank" rel="noopener">官网</a>下载，当然，我们也可以从这里下载 <a href="https://pan.baidu.com/s/1-h0z7DR2uhuwhCn0_vNc4w" title="百度网盘" target="_blank" rel="noopener">kibana-6.4.0-linux-x86_64.tar.gz</a>，推荐从<code>kibana</code>官网下载对应版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/</span><br><span class="line">$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.4.0-linux-x86_64.tar.gz</span><br><span class="line">$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.4.0-linux-x86_64.tar.gz.sha512</span><br><span class="line"></span><br><span class="line">验证下载文件的完整性，在下载的时候要将 SHA512 文件也下载回来</span><br><span class="line">$ sha512sum -c kibana-6.4.0-linux-x86_64.tar.gz.sha512 </span><br><span class="line">kibana-6.4.0-linux-x86_64.tar.gz: OK</span><br><span class="line"></span><br><span class="line">$ tar xzf kibana-6.4.0-linux-x86_64.tar.gz</span><br></pre></td></tr></table></figure>
<p>对<code>kibana</code>配置要查看的<code>elasticsearch</code>，只需修改如下配置项即可，如果是在本机安装<code>elasticsearch</code>，并且使用默认的9200端口，则无需配置。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kibana-6.4.0-linux-x86_64/config</span><br><span class="line">$ vi kibana.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改如下配置项，如果使用默认的，则无需修改</span></span><br><span class="line"><span class="comment">#server.port: 5601</span></span><br><span class="line"><span class="comment">#elasticsearch.url: "http://localhost:9200"</span></span><br><span class="line"><span class="comment">#elasticsearch.username: "user"</span></span><br><span class="line"><span class="comment">#elasticsearch.password: "pass"</span></span><br></pre></td></tr></table></figure></p>
<p>接着启动<code>kibana</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/hewentian/ProjectD/kibana-6.4.0-linux-x86_64/bin</span><br><span class="line">$ ./kibana <span class="comment"># 或者以后台方式运行 nohup ./kibana &amp;</span></span><br></pre></td></tr></table></figure></p>
<p>打开浏览器，并输入下面的地址：<br><a href="http://localhost:5601" target="_blank" rel="noopener">http://localhost:5601</a></p>
<p>你将看到如下界面：<br><img src="/img/elk-kibana-1.png" alt="" title="kibana初始界面"></p>
<p>点击上图中的<code>[Management]-&gt;[Index Patterns]-&gt;[Create index pattern]</code>，输入<code>index name：redis-systemlog-*</code>，如下图<br><img src="/img/elk-kibana-2.png" alt="" title="kibana配置index name界面"></p>
<p>点击<code>[Next step]</code>按钮，并在接下来的界面中的<code>Time Filter field name</code>中选择<code>I don&#39;t want to user the Time Filter</code>，最后点击<code>Create index pattern</code>完成创建。接着点击左则的<code>[Discover]</code>并在左则的界面中选择中<code>redis-systemlog-*</code>，你将看到如下结果：<br><img src="/img/elk-kibana-3.png" alt="" title="kibana查询界面"></p>
<p>至此，简单的 ELK 基本搭建完毕。下面展示一个简单的配置示例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample Logstash configuration for creating a simple</span></span><br><span class="line"><span class="comment"># Redis -&gt; Logstash -&gt; Elasticsearch pipeline.</span></span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">  <span class="comment"># system log</span></span><br><span class="line">  redis &#123;</span><br><span class="line">    <span class="built_in">type</span> =&gt; <span class="string">"systemlog"</span></span><br><span class="line">    host =&gt; <span class="string">"127.0.0.1"</span></span><br><span class="line">    port =&gt; 6379</span><br><span class="line">    password =&gt; <span class="string">"abc123"</span></span><br><span class="line">    db =&gt; 0</span><br><span class="line">    data_type =&gt; <span class="string">"list"</span></span><br><span class="line">    key =&gt; <span class="string">"systemlog"</span></span><br><span class="line">    codec  =&gt; <span class="string">"json"</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># user log</span></span><br><span class="line">  redis &#123;</span><br><span class="line">    <span class="built_in">type</span> =&gt; <span class="string">"userlog"</span></span><br><span class="line">    host =&gt; <span class="string">"127.0.0.1"</span></span><br><span class="line">    port =&gt; 6379</span><br><span class="line">    password =&gt; <span class="string">"abc123"</span></span><br><span class="line">    db =&gt; 0</span><br><span class="line">    data_type =&gt; <span class="string">"list"</span></span><br><span class="line">    key =&gt; <span class="string">"userlog"</span></span><br><span class="line">    codec  =&gt; <span class="string">"json"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [<span class="string">"http://127.0.0.1:9200"</span>]</span><br><span class="line">    index =&gt; <span class="string">"%&#123;type&#125;-%&#123;+YYYY.MM&#125;"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="下面我们将继续探索它的高级功能。"><a href="#下面我们将继续探索它的高级功能。" class="headerlink" title="下面我们将继续探索它的高级功能。"></a>下面我们将继续探索它的高级功能。</h3><p>很多时候，对于<code>systemlog</code>中的某条信息（不一定是JSON格式），如果我们只需要某些信息，那我们又怎样做呢？这里就需要使用FILTERS了。</p>
<p>在FILTERS中使用grok正则表达式，关于grok，可以参见这里的说明：<br><a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html</a></p>
<p>未完，待续……</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/hewentian/2018/10/02/elk-note/" data-id="clf9thvkh00096h3krpbplmle" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/">&laquo; __('prev')</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/">bigdata</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/container/">container</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/db/">db</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/other/">other</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web-server/">web server</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/03/07/xxl-job-note/">xxl-job 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/11/28/seata-note/">seata 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/09/25/spring-note/">spring 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/04/01/mybatis-note/">mybatis 学习笔记</a>
          </li>
        
          <li>
            <a href="/2021/01/22/canal-note/">canal 学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">link</h3>
    <div class="widget">
      <li><a href="https://github.com/hewentian" title="Tim Ho's Blog">我的github</a></li>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Tim Ho<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/categories/" class="mobile-nav-link">Categories</a>
  
    <a href="/tags/" class="mobile-nav-link">Tags</a>
  
    <a href="/about/" class="mobile-nav-link">About</a>
  
</nav>
    

<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script> -->
<!-- <script src="//code.jquery.com/jquery-2.2.4.min.js"></script> -->
<script src="/js/jquery-3.5.1.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>